{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1Fjia_1ixvstJFpwsNQ9pqC-NK2QERcwE",
      "authorship_tag": "ABX9TyOBXMv2MQwXZJ7HlXFVZD0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Bioinformatics-Code/blob/main/Copy_of_Nat_2024_Regression_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDnewNfrdnBi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load the data into a pandas DataFrame\n",
        "file_path = \"/content/drive/MyDrive/Natesh_scrna/liver_TF_normalized_week7_17 (1).csv\"\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "NFvEVFjZefUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the first column has the DNA sequence identifiers\n",
        "data['Unnamed'] = data.iloc[:, 0].str.split(\"_\").str[0]"
      ],
      "metadata": {
        "id": "6NB_n2nfe_-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "aZ37K76gfPZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem: the strings associated with the transcription factors are not straightforward like I thought. I am missing some key concept or formulation for labelling the transcription factors, will address later.**"
      ],
      "metadata": {
        "id": "mCPGs3XYfqsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns.tolist())\n",
        "\n",
        "# or a helper function:\n",
        "def check_gene_presence(gene_name, df):\n",
        "    variants = [gene_name.upper(), gene_name.capitalize(), gene_name.lower()]\n",
        "    found = []\n",
        "    for variant in variants:\n",
        "        if variant in df.columns:\n",
        "            found.append(variant)\n",
        "    return found if found else None\n",
        "\n",
        "print(\"Alb columns found:\", check_gene_presence(\"Alb\", data))\n",
        "print(\"Afp columns found:\", check_gene_presence(\"Afp\", data))\n",
        "print(\"Hnf4a columns found:\", check_gene_presence(\"Hnf4a\", data))\n"
      ],
      "metadata": {
        "id": "ykJmvVzafR5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Gene Presence\n",
        "\n",
        "Below is the code snippet used to verify whether our dataset (`data`) contains columns for **Alb**, **Afp**, and **Hnf4a** (in various case variants). We also print out the entire list of columns in the DataFrame for reference.\n",
        "\n",
        "```python\n",
        "# Print all columns in the dataset\n",
        "print(data.columns.tolist())\n",
        "\n",
        "# Define a helper function to check for possible variations of a gene name\n",
        "def check_gene_presence(gene_name, df):\n",
        "    variants = [gene_name.upper(), gene_name.capitalize(), gene_name.lower()]\n",
        "    found = []\n",
        "    for variant in variants:\n",
        "        if variant in df.columns:\n",
        "            found.append(variant)\n",
        "    return found if found else None\n",
        "\n",
        "# Check for variations of 'Alb', 'Afp', and 'Hnf4a'\n",
        "print(\"Alb columns found:\", check_gene_presence(\"Alb\", data))\n",
        "print(\"Afp columns found:\", check_gene_presence(\"Afp\", data))\n",
        "print(\"Hnf4a columns found:\", check_gene_presence(\"Hnf4a\", data))\n"
      ],
      "metadata": {
        "id": "QGOKKlVngaq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[  'Unnamed: 0', 'ALB', 'AFP', 'HNF4A', 'FOXA3', 'APOA1', 'HHEX', 'PROX1',   'TBX3', 'ONECUT1', 'GATA4', 'HNF1A', 'CEBPA', 'TTR', 'ATF5', 'CEBPB',   'CREB3L3', 'HLF', 'KLF15', 'MLXIPL', 'NR0B2', 'NR1H4', 'NR1I3', 'NR5A2',   'PPARA', 'RORC', 'NFIB', 'NFIC', 'PPARG', 'RARA', 'RORA', 'PARP10',   'PARP12', 'MAFB', 'CRY1', 'CUX2', 'ARNTL', 'KAT2B', 'STAT5A', 'ABTB2',   'ATOH8', 'CRIP2', 'MAOA', 'HLX', 'SIRT5', 'PAX8', 'ARID5A', 'L3MBTL4',   'TBX15', 'AR', 'NFIX', 'NFIA', 'RXRA', 'FOXA1', 'ESR1', 'FOXA2',   'HNF1B', 'cell_state', 'Unnamed']\n",
        "\n",
        "Alb columns found: ['ALB']\n",
        "\n",
        "Afp columns found: ['AFP']\n",
        "\n",
        "Hnf4a columns found: ['HNF4A']\n"
      ],
      "metadata": {
        "id": "yhS_vnJjhOTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "myUsRgxcgJgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### so I can have the notebook organized like I did last time I will move the 'cell_state' column to the zeroth column"
      ],
      "metadata": {
        "id": "XnObD5dDiDP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'cell_state' column from its current position\n",
        "cell_state_series = data.pop('cell_state')\n",
        "\n",
        "# Insert 'cell_state' back as the first column\n",
        "data.insert(0, 'cell_state', cell_state_series)\n",
        "\n",
        "# Verify\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "3Avlx_OShcNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're certain 'Unnamed: 0' is the column name:\n",
        "data.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "# Verify removal:\n",
        "print(data.columns)\n"
      ],
      "metadata": {
        "id": "mB9kU7cuiRDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "oxVsqevgjHqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the first column has the DNA sequence identifiers\n",
        "data['cell_state'] = data.iloc[:, 0].str.split(\"_\").str[0]"
      ],
      "metadata": {
        "id": "Et2u_btBjK4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "LnklozcYjjmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "`(cell)(\\d+)`:\n",
        "\n",
        "Captures two groups:\n",
        "\n",
        "* Group 1: The word \"cell\"\n",
        "* Group 2: One or more digits `(\\d+)`\n",
        "\n",
        "    * `r'\\1_\\2'`: Replaces with Group 1, then an underscore\n",
        "    * then Group 2 `regex=True`: Ensures we’re using a regular-expression replacement (necessary in recent versions of pandas)"
      ],
      "metadata": {
        "id": "WnlIzXNmlKlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "data['cell_state'] = data['cell_state'].str.replace(\n",
        "    r'(cell)(\\d+)',\n",
        "    r'\\1_\\2',\n",
        "    regex=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "RmMdnCKPjloU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "qmyk-MsXkxFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Want to see if their is an approximate linear pattern in the numbers\n",
        "\n",
        "example workflow to extract just the integer portion from \"cell_XXX\" (e.g., \"cell_183\" → 183), store it in a new column, and plot it in the order of the rows.\n",
        "\n",
        "**Explanation**\n",
        "\n",
        "* Regex Extraction:\n",
        "    * `str.extract(r'cell_(\\d+)')` captures only the digits after \"cell_\".\n",
        "* The regular expression `r'cell_(\\d+)'` means:\n",
        "`\"cell_\":` literally the string `“cell_”`\n",
        "* `(\\d+)`: capture one or more digits into a group\n",
        "\n",
        "**Converting to Integer:**\n",
        "* `.astype(int)` ensures we treat the extracted string digits as numerical values, making it easier to plot or do numerical operations.\n",
        "\n",
        "**Plotting:**\n",
        "* `plt.plot(data['cell_number'])` creates a line plot of the values in the order they appear.\n",
        "* We could also use a scatter plot if you prefer: `plt.scatter(datas.index, datas['cell_number'])`.\n",
        "\n",
        "This way, we can quickly see if the cell numbers follow any pattern across rows (e.g., if they increase steadily, jump around, etc.).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GUNmzDGwm9vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Extract the numeric part (digits) from the 'cell_state' column\n",
        "#    For example, \"cell_183\" → 183\n",
        "data['cell_number'] = data['cell_state'].str.extract(r'cell_(\\d+)', expand=False)\n",
        "\n",
        "# 2. Convert that numeric part to an integer (if desired)\n",
        "data['cell_number'] = data['cell_number'].astype(int)\n",
        "\n",
        "# 3. Plot these numbers in the order they appear (row-wise)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(data['cell_number'], marker='o')\n",
        "plt.title(\"Cell Number in Row Order\")\n",
        "plt.xlabel(\"Row Index\")\n",
        "plt.ylabel(\"Cell Number\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u-0towFpk26u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which TFs Should We Include?\n",
        "### A. Check for the “classic” TFs I used last year\n",
        "Last year, I tried:\n",
        "\n",
        "`['Foxa2', 'Foxa3', 'Hnf1a', 'Cebpa', 'Tbx3', 'Prox1', 'Hnf4a']\n",
        "`\n",
        "\n",
        "In the new dataset, you see columns like:\n",
        "\n",
        "`'FOXA1', 'FOXA2', 'FOXA3', 'HNF1A', 'HNF1B', 'HNF4A', 'ONECUT1' (≅ HNF6), 'GATA4', 'HHEX' (≅ HEX), 'CEBPA', 'PROX1', 'TBX3', ...`\n",
        "\n",
        "So all of those seven from last year are indeed present in some form (watching out for uppercase). Specifically:\n",
        "\n",
        "Foxa2 → `'FOXA2'`\n",
        "Foxa3 → `'FOXA3'`\n",
        "Hnf1a → `'HNF1A'`\n",
        "Cebpa → `'CEBPA'`\n",
        "Tbx3 → `'TBX3'`\n",
        "Prox1 → `'PROX1'`\n",
        "Hnf4a → `'HNF4A'`\n",
        "\n",
        "### B. Consider adding other related TFs in this dataset\n",
        "* FOXA1: sometimes interesting when analyzing hepatocyte or endoderm lineage development.\n",
        "* HNF1B: the new data has `'HNF1B'`1.\n",
        "* GATA4: in the list of columns. Often relevant for liver development.\n",
        "* HHEX: in the data as `'HHEX'` (a known alias for “Hex”).\n",
        "* ONECUT1 (alias for HNF6) if we want to replicate the idea of HNF6.\n",
        "* CEBPB if we’re curious about other members of the CEBP family.\n",
        "\n",
        "If Natesh/Dan specifically want to see all TFs that have positive correlation with ALB or negative correlation with AFP, we can do a broader correlation test across all columns that look like TFs (there are many: ARNTL, ESR1, RORA, etc.). But usually, one focuses on a curated list—especially if these TFs are known from previous hepatic-development studies.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S5n7o6MIqtsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow to Focus on “Positive Correlation with ALB or Negative Correlation with AFP\n",
        "\n",
        "### Non-Neural-Netowork: Pearsons Correlation”\n",
        "\n",
        "Create a subset DataFrame with your TFs + ALB + AFP + HNF4A\n",
        "\n"
      ],
      "metadata": {
        "id": "u-iXvP83vPrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# these are the TFs we will work with in this dataset:last year plus FOXA1 (including FOXA1 since it has a role in hepatocyte development)\n",
        "tf_list = [\n",
        "    'FOXA1', 'FOXA2', 'FOXA3', 'HNF1A', 'CEBPA',\n",
        "    'PROX1', 'TBX3', 'HNF4A'\n",
        "]\n",
        "\n",
        "# We'll also include ALB, AFP, and HNF4A for correlation\n",
        "cols_of_interest = tf_list + ['ALB', 'AFP', 'HNF4A']  # 'HNF4A' is both TF & \"target\"\n",
        "\n",
        "df_sub = data[cols_of_interest].copy()\n"
      ],
      "metadata": {
        "id": "VDST_4ycm5tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**make sure to get rid of div/0 errors**"
      ],
      "metadata": {
        "id": "zSR8kEGYyUWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# List of columns to drop:\n",
        "# - data.columns[0] (the 0th column, e.g. 'cell_state')\n",
        "# - 'Unnamed' (the extra column name, if it exists)\n",
        "# - 'cell_number' (the numeric part you extracted but no longer need in the correlation)\n",
        "cols_to_drop = [data.columns[0], 'Unnamed', 'cell_number']\n",
        "\n",
        "# 1. Drop those columns (ignore errors if any don't exist)\n",
        "data_cln = data.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# 2. Replace '#DIV/0!' with NaN\n",
        "data_cln = data_cln.replace('#DIV/0!', np.nan)\n",
        "\n",
        "# 3. Convert columns to numeric (coerce invalid values to NaN)\n",
        "for col in data_cln.columns:\n",
        "    data_cln[col] = pd.to_numeric(data_cln[col], errors='coerce')\n",
        "\n",
        "# 4. Now compute correlation among all numeric columns\n",
        "corr_matrix = data_cln.corr(method='pearson')\n",
        "\n",
        "print(corr_matrix)\n"
      ],
      "metadata": {
        "id": "S0DhnCej0YvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute the Correlation Matrix"
      ],
      "metadata": {
        "id": "Lwua7JyL1z-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Replace the '#DIV/0!' strings with NaN in df_sub\n",
        "df_sub = df_sub.replace('#DIV/0!', np.nan)\n",
        "\n",
        "# Convert every column to numeric, coercing invalid to NaN\n",
        "for col in df_sub.columns:\n",
        "    df_sub[col] = pd.to_numeric(df_sub[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "626331zV2Qin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_sub.dtypes)\n"
      ],
      "metadata": {
        "id": "ePmRSRyV2aJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This TypeError often arises when pandas detects duplicate column names, causing `df_sub[col]` to return a DataFrame instead of a Series. In the error output, notice that 'HNF4A' is listed twice in the dtypes:\n",
        "\n",
        "FOXA1    float64\n",
        "\n",
        "FOXA2    float64\n",
        "\n",
        "FOXA3    float64\n",
        "\n",
        "HNF1A    float64\n",
        "\n",
        "CEBPA    float64\n",
        "\n",
        "PROX1    float64\n",
        "\n",
        "TBX3     float64\n",
        "\n",
        "HNF4A     object   <─ 1st HNF4A\n",
        "\n",
        "ALB       object\n",
        "\n",
        "AFP       object\n",
        "\n",
        "HNF4A     object   <─ 2nd HNF4A\n",
        "\n",
        "dtype: object\n",
        "\n"
      ],
      "metadata": {
        "id": "VLseXMHe2zFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename One of the Duplicated Columns\n",
        "maybe we want to keep both columns for some reason (e.g., they are slightly different “HNF4A” metrics), rename one"
      ],
      "metadata": {
        "id": "LBXvAfsM3qqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose we rename the second \"HNF4A\" to \"HNF4A_2\"\n",
        "dup_mask = df_sub.columns.duplicated(keep='first')\n",
        "# This mask is True for any duplicate columns after the first occurrence\n",
        "\n",
        "for i, dup in enumerate(dup_mask):\n",
        "    if dup:\n",
        "        old_col = df_sub.columns[i]\n",
        "        df_sub.rename(columns={old_col: old_col + \"_2\"}, inplace=True)\n",
        "\n",
        "# Now no columns share the exact same name\n",
        "# so \"HNF4A\" might become \"HNF4A_2\" for the second copy.\n"
      ],
      "metadata": {
        "id": "i9Iucl-R3ajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df_sub.columns:\n",
        "    df_sub[col] = pd.to_numeric(df_sub[col], errors='coerce')\n",
        "\n"
      ],
      "metadata": {
        "id": "_UueKmBLxjoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_sub columns:\\n\", df_sub.columns)\n",
        "print(\"\\nDuplicated columns?\\n\", df_sub.columns.duplicated(keep=False))\n"
      ],
      "metadata": {
        "id": "tMBSDDIJxoY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[\"HNF4A_2\"]\n"
      ],
      "metadata": {
        "id": "sdEHaEC14rZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = df_sub.loc[:, ~df_sub.columns.duplicated(keep='first')]\n"
      ],
      "metadata": {
        "id": "KpXuTQ5846kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub.replace('#DIV/0!', np.nan, inplace=True)\n",
        "\n",
        "for col in df_sub.columns:\n",
        "    df_sub[col] = pd.to_numeric(df_sub[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "v9tLNF1G5WCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Drop duplicate columns (if any remain)\n",
        "df_sub = df_sub.loc[:, ~df_sub.columns.duplicated(keep='first')]\n",
        "\n",
        "# 2. Replace '#DIV/0!' with NaN\n",
        "df_sub.replace('#DIV/0!', np.nan, inplace=True)\n",
        "\n",
        "# 3. Convert columns to numeric where possible\n",
        "for col in df_sub.columns:\n",
        "    df_sub[col] = pd.to_numeric(df_sub[col], errors='coerce')\n",
        "\n",
        "# 4. Compute Pearson correlation only on numeric columns\n",
        "corr_matrix = df_sub.corr(method='pearson')\n"
      ],
      "metadata": {
        "id": "JHTioBTe5aQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Focus on ALB, AFP, and HNF4A*\n",
        "Since PI and grad student want:\n",
        "\n",
        "* Positive correlations with ALB\n",
        "* Negative correlations with AFP\n",
        "* Correlation with HNF4A\n",
        "\n",
        "**We can pull those out**"
      ],
      "metadata": {
        "id": "eLEzjQ4X5vnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_with_alb = corr_matrix['ALB'].drop('ALB', errors='ignore').sort_values(ascending=False)\n",
        "corr_with_afp = corr_matrix['AFP'].drop('AFP', errors='ignore').sort_values(ascending=False)\n",
        "corr_with_hnf4a = corr_matrix['HNF4A'].drop('HNF4A', errors='ignore').sort_values(ascending=False)\n"
      ],
      "metadata": {
        "id": "Sfc0J-Ly5qhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_sub columns:\", df_sub.columns.tolist())\n"
      ],
      "metadata": {
        "id": "yd1qrli86Jya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_sub.corr(method='pearson')\n",
        "print(\"corr_matrix columns:\", corr_matrix.columns.tolist())\n"
      ],
      "metadata": {
        "id": "hVAQO5Jq6ZJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename 'HNF4A_2' back to 'HNF4A'\n",
        "df_sub.rename(columns={\"HNF4A_2\": \"HNF4A\"}, inplace=True)\n",
        "\n",
        "# Now df_sub.columns (and your correlation matrix) will show 'HNF4A' instead.\n",
        "print(df_sub.columns)\n"
      ],
      "metadata": {
        "id": "32OFYC3P6iF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_sub.corr(method='pearson')\n",
        "print(corr_matrix.columns)\n"
      ],
      "metadata": {
        "id": "u4_XMUXZ7REd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Extract correlation with HNF4A\n",
        "corr_with_hnf4a = corr_matrix[\"HNF4A\"].drop(\"HNF4A\", errors=\"ignore\")\n",
        "print(corr_with_hnf4a.sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "bWNQKQc17YiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df_sub already has numeric columns for all TFs + ALB + AFP\n",
        "corr_matrix = df_sub.corr(method='pearson')\n"
      ],
      "metadata": {
        "id": "rts4tcgP7tfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation with ALB (excluding ALB itself if present)\n",
        "corr_with_alb = corr_matrix[\"ALB\"].drop(\"ALB\", errors=\"ignore\")\n",
        "\n",
        "# Correlation with AFP (excluding AFP itself if present)\n",
        "corr_with_afp = corr_matrix[\"AFP\"].drop(\"AFP\", errors=\"ignore\")\n"
      ],
      "metadata": {
        "id": "9PK8IKK_8UNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort descending so that highest correlations appear first\n",
        "corr_with_alb_sorted = corr_with_alb.sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.barplot(\n",
        "    x=corr_with_alb_sorted.index,\n",
        "    y=corr_with_alb_sorted.values,\n",
        "    palette=\"Blues_r\"\n",
        ")\n",
        "plt.title(\"Non-Neural-Network Correlation with ALB\")\n",
        "plt.ylabel(\"Pearson Correlation\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show();\n"
      ],
      "metadata": {
        "id": "aNdCbd4V8Xgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_with_afp_sorted = corr_with_afp.sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.barplot(\n",
        "    x=corr_with_afp_sorted.index,\n",
        "    y=corr_with_afp_sorted.values,\n",
        "    palette=\"Reds_r\"\n",
        ")\n",
        "plt.title(\"Non-Neural-Network Correlation with AFP\")\n",
        "plt.ylabel(\"Pearson Correlation\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DyHOanem8bjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_with_hnf4a = corr_matrix[\"HNF4A\"].drop(\"HNF4A\", errors=\"ignore\").sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "sns.barplot(\n",
        "    x=corr_with_hnf4a.index,\n",
        "    y=corr_with_hnf4a.values,\n",
        "    palette=\"magma\"\n",
        ")\n",
        "plt.title(\"Non-Neural-Network Correlation with HNF4A\")\n",
        "plt.ylabel(\"Pearson Correlation\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "elfKYod48pdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n",
        "\n",
        "# PyTorch Neural Network Analysis for TF-Based Gene Expression Prediction\n",
        "\n",
        "In this section, we will introduce a **PyTorch**-based neural network to predict the expression of four key maturity genes (\\`ALB\\`, \\`AFP\\`, \\`APOA1\\`, and \\`TTR\\`) using the expression levels of 12 transcription factors (TFs) as inputs. These TFs include:\n",
        "\n",
        "1. **FOXA1**  \n",
        "2. **FOXA2**  \n",
        "3. **FOXA3**  \n",
        "4. **HNF1A**  \n",
        "5. **HNF1B**  \n",
        "6. **HNF4**  \n",
        "7. **HNF6**  \n",
        "8. **GATA4**  \n",
        "9. **CEBPA**  \n",
        "10. **TBX3**  \n",
        "11. **PROX1**  \n",
        "12. **HEX**\n",
        "\n",
        "## Objective\n",
        "To capture potentially **non-linear** and **multi-dimensional** relationships that go beyond traditional methods (e.g., Pearson correlation or linear regression). The network will output predictions for **ALB**, **AFP**, **APOA1**, and **TTR** simultaneously, allowing us to:\n",
        "\n",
        "- Estimate how well these TFs collectively predict each gene.  \n",
        "- Handle complex interactions among TFs.  \n",
        "- Provide a more robust framework for modeling gene regulation patterns in scRNA-seq data.\n",
        "\n",
        "## Approach Outline\n",
        "\n",
        "1. **Data Preparation**  \n",
        "   - **Subset** the relevant TF columns and the four maturity genes from the main DataFrame.  \n",
        "   - **Clean** any invalid strings (e.g., \\`#DIV/0!\\`) by replacing them with missing values (\\`NaN\\`) and coercing to numeric.  \n",
        "   - **Drop** rows containing \\`NaN\\` (or handle via imputation) to ensure the input matrix is valid for the model.\n",
        "\n",
        "2. **Train/Test Split**  \n",
        "   - We will split our dataset into **training** ~80% and **test** ~20% sets.  \n",
        "   - This split ensures an unbiased estimate of how the model generalizes to unseen data.\n",
        "\n",
        "3. **Neural Network Architecture**  \n",
        "   - **Input Layer**: 12 neurons, one for each TF.  \n",
        "   - **Hidden Layers**: Two fully connected layers (e.g., 64 units → 32 units), each followed by a ReLU activation.  \n",
        "   - **Output Layer**: 4 neurons, corresponding to \\`ALB\\`, \\`AFP\\`, \\`APOA1\\`, and \\`TTR\\`.  \n",
        "   - **Loss Function**: Mean Squared Error (MSE) to measure how close the predictions are to the actual gene expression.  \n",
        "   - **Optimizer**: Adam (with a default learning rate of 1e-3) for efficient gradient-based optimization.\n",
        "\n",
        "4. **Training**  \n",
        "   - **Epochs**: Typically 30-50 to start, adjusting based on validation loss curves.  \n",
        "   - **Batch Size**: 32 (tunable if we see memory or convergence issues).  \n",
        "   - **Loss Monitoring**: We will track the training loss per epoch, ensuring the model is learning without overfitting.\n",
        "\n",
        "5. **Evaluation**  \n",
        "   - **Test Set MSE**: To quantify predictive performance for each gene.  \n",
        "   - **R² and Correlation**: To measure how well the model explains variance in the target genes (and compare with simpler methods).  \n",
        "   - **Interpretability**: Consider advanced methods like permutation importance or SHAP analysis if we need deeper insight into TF contributions.\n",
        "\n",
        "By training this **multi-output** neural network, we aim to capture richer patterns in the transcriptomic data and potentially unveil complex gene regulatory interactions that a simpler correlation-based approach might overlook.\n"
      ],
      "metadata": {
        "id": "sXv1b-R2KlzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Last year** Define Columns of Interest as we did last year\n",
        "According to last years objective, you have 12 TFs and 4 genes representing differentiation:\n",
        "\n"
      ],
      "metadata": {
        "id": "4yXyLah9Mbwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns.tolist())\n"
      ],
      "metadata": {
        "id": "O3mZnQySM0aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the columns list, we see:\n",
        "\n",
        "* `HNF4A` is present (instead of `HNF4`).\n",
        "* `ONECUT1` is present (commonly the alias for `HNF6`).\n",
        "* `HHEX` is present (commonly the alias for `HEX`).\n",
        "\n",
        "So, `HNF4`, `HNF6`, and `HEX` are not in our DataFrame, but `HNF4A`, `ONECUT1`, and `HHEX` are. Similarly, `FOXA1`, `FOXA2`, `FOXA3`, `HNF1A`, `CEBPA`, `TBX3`, and `PROX1` do appear as you might expect."
      ],
      "metadata": {
        "id": "98MfA5limtO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Corrected Transcription Factors to match the columns you actually have\n",
        "tf_list = [\n",
        "    \"FOXA1\",  # present\n",
        "    \"FOXA2\",  # present\n",
        "    \"FOXA3\",  # present\n",
        "    \"HNF1A\",  # present\n",
        "    \"HNF1B\",  # present\n",
        "    \"HNF4A\",  # instead of HNF4\n",
        "    \"ONECUT1\",# instead of HNF6\n",
        "    \"GATA4\",  # present\n",
        "    \"CEBPA\",  # present\n",
        "    \"TBX3\",   # present\n",
        "    \"PROX1\",  # present\n",
        "    \"HHEX\"    # instead of HEX\n",
        "]\n",
        "\n",
        "# Differentiation Genes (confirmed present)\n",
        "target_genes = [\"ALB\", \"AFP\", \"APOA1\", \"TTR\"]\n",
        "\n",
        "# Combine them\n",
        "cols_of_interest = tf_list + target_genes\n",
        "\n",
        "# Now create the subset DataFrame without the KeyError\n",
        "df_sub = data[cols_of_interest].copy()\n"
      ],
      "metadata": {
        "id": "WNKiIFkImKkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a New DataFrame `data_nn` Without Unwanted Columns\n",
        "\n",
        "To facilitate our **neural network** analysis, we'll create a new DataFrame called **`data_nn`** that excludes certain non-essential columns—namely **`'Unnamed'`, `'cell_state'`, and `'cell_number'`**. This keeps our data cleaner and ready for machine-learning tasks.\n",
        "\n",
        "```python\n"
      ],
      "metadata": {
        "id": "0tm0Nf5ZnyRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame by dropping columns we don't need\n",
        "cols_to_drop = ['Unnamed', 'cell_state', 'cell_number']\n",
        "data_nn = data.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# Verify the columns in data_nn\n",
        "print(\"Columns in data_nn:\")\n",
        "print(data_nn.columns.tolist())\n"
      ],
      "metadata": {
        "id": "YT7xP7lznWvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example quick check:\n",
        "print(data_nn.isna().sum())\n",
        "print(data_nn[data_nn == '#DIV/0!'].count())  # if not already replaced\n"
      ],
      "metadata": {
        "id": "y8kwFITmoJMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "drop rows with error values\n"
      ],
      "metadata": {
        "id": "6zM9Ox7Npe3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Replace '#DIV/0!' with NaN in-place\n",
        "data_nn.replace('#DIV/0!', np.nan, inplace=True)\n",
        "\n",
        "# Convert columns to numeric where possible\n",
        "for col in data_nn.columns:\n",
        "    data_nn[col] = pd.to_numeric(data_nn[col], errors='coerce')\n",
        "\n",
        "# Optional: Check how many NaNs remain\n",
        "print(data_nn.isna().sum())\n",
        "\n",
        "# If it’s only that one row, you can:\n",
        "# (A) Drop that single row\n",
        "data_nn.dropna(subset=data_nn.columns, inplace=True)\n",
        "\n",
        "# OR (B) Impute with median/mean, e.g.:\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(strategy='median')\n",
        "# data_imputed = imputer.fit_transform(data_nn)\n",
        "# data_nn = pd.DataFrame(data_imputed, columns=data_nn.columns)\n"
      ],
      "metadata": {
        "id": "A29xdX0Ton4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_nn.isna().sum())               # Missing values\n",
        "print(data_nn[data_nn == '#DIV/0!'].sum())# Should be 0 occurrences\n"
      ],
      "metadata": {
        "id": "A74XoEyIpyPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**see how many rows were dropped**"
      ],
      "metadata": {
        "id": "W5-juVSoqXLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say you start with some shape for data_nn\n",
        "original_shape = data_nn.shape\n",
        "\n",
        "# Identify rows that have '#DIV/0!' in any column\n",
        "invalid_rows = data_nn.index[data_nn.eq('#DIV/0!').any(axis=1)]\n",
        "\n",
        "# Number of invalid rows\n",
        "num_invalid_rows = len(invalid_rows)\n",
        "print(f\"Number of rows containing '#DIV/0!': {num_invalid_rows}\")\n",
        "\n",
        "# Option 1: Drop those rows\n",
        "data_nn_dropped = data_nn.drop(index=invalid_rows)\n",
        "dropped_shape = data_nn_dropped.shape\n",
        "rows_dropped = original_shape[0] - dropped_shape[0]\n",
        "print(f\"Rows dropped: {rows_dropped}\")\n",
        "\n",
        "# Option 2: If you replaced '#DIV/0!' with NaN first and then used dropna():\n",
        "data_nn_replaced = data_nn.replace('#DIV/0!', np.nan)\n",
        "data_nn_droppedna = data_nn_replaced.dropna()\n",
        "rows_dropped_na = data_nn_replaced.shape[0] - data_nn_droppedna.shape[0]\n",
        "print(f\"Rows dropped after replacing with NaN and then dropna(): {rows_dropped_na}\")\n"
      ],
      "metadata": {
        "id": "677HCgsOqAa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_nn.head(10))\n"
      ],
      "metadata": {
        "id": "ZwZmwialqb8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Equivalent of the Previous TensorFlow Classification Workflow\n",
        "\n",
        "## Multi-Output Regression Example in PyTorch\n",
        "\n",
        "Why Multi-Output?\n",
        "In short, you want to predict or model multiple gene expressions simultaneously rather than running multiple single-gene regressions. That is precisely multi-output regression. It’s beneficial because it:\n",
        "\n",
        "* Treats the 4 target genes as a joint learning problem.\n",
        "* Potentially captures shared signals and dependencies in the data.\n",
        "* Reduces the overhead of training 4 separate networks.\n",
        "* Aligns nicely with the “multi-task” perspective in scRNA-seq data, where multiple genes are co-expressed and co-regulated by the same set of TFs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KAHmr4vHsEjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Just to confirm what df_sub has:\n",
        "print(df_sub.columns)\n",
        "\n",
        "# Suppose we want to predict [ALB, AFP, APOA1, TTR] from the 12 TFs\n",
        "tf_list = [\n",
        "    \"FOXA1\", \"FOXA2\", \"FOXA3\",\n",
        "    \"HNF1A\", \"HNF1B\", \"HNF4A\",\n",
        "    \"ONECUT1\", \"GATA4\", \"CEBPA\",\n",
        "    \"TBX3\", \"PROX1\", \"HHEX\"\n",
        "]\n",
        "\n",
        "target_genes = [\"ALB\", \"AFP\", \"APOA1\", \"TTR\"]\n",
        "\n",
        "# X: TF columns\n",
        "X = df_sub[tf_list].values  # shape: (num_samples, 12)\n",
        "\n",
        "# y: 4 gene columns\n",
        "y = df_sub[target_genes].values  # shape: (num_samples, 4)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "KINt6cevqySR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we explicitly chose the four differentiation genes\n",
        "‘\n",
        "𝐴\n",
        "𝐿\n",
        "𝐵\n",
        "‘\n",
        ",\n",
        "‘\n",
        "𝐴\n",
        "𝐹\n",
        "𝑃\n",
        "‘\n",
        ",\n",
        "‘\n",
        "𝐴\n",
        "𝑃\n",
        "𝑂\n",
        "𝐴\n",
        "1\n",
        "‘\n",
        ",\n",
        "‘\n",
        "𝑇\n",
        "𝑇\n",
        "𝑅\n",
        "‘\n",
        "‘ALB‘,‘AFP‘,‘APOA1‘,‘TTR‘ as our target_genes, our y DataFrame (or array) ends up having exactly 4 columns—one for each of those genes. In other words, we're doing multi-output regression where the four columns in y represent the expression levels (or values) of those four specific target genes"
      ],
      "metadata": {
        "id": "ZUUYqFbRt5nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_nn.replace('#DIV/0!', np.nan, inplace=True)\n"
      ],
      "metadata": {
        "id": "lSLcJpSXu7Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in data_nn.columns:\n",
        "    data_nn[col] = pd.to_numeric(data_nn[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "LqeMSi39u69P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_nn.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "FE_aEoJYu65D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "GwN97CEJtWI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train dtype:\", X_train.dtype)\n",
        "print(\"X_train sample:\\n\", X_train[:5])  # see the first 5 rows\n",
        "\n"
      ],
      "metadata": {
        "id": "qnx-R1sXvPQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replace '#DIV/0!' with NaN in your DataFrame\n",
        "df_sub = df_sub.replace('#DIV/0!', np.nan)\n",
        "\n",
        "# Option A: Drop rows with NaN\n",
        "# df_sub.dropna(inplace=True)\n",
        "\n",
        "# Option B: Or you can impute, e.g. fill with 0 or median\n",
        "df_sub.fillna(value=np.median, inplace=True)\n",
        "#  -- or use sklearn's SimpleImputer, etc.\n",
        "\n",
        "# Now your entire df_sub should be numeric or NaN-free\n"
      ],
      "metadata": {
        "id": "AwyGkkT_v1J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_sub[df_sub == '#DIV/0!'].sum())  # Should all be 0\n",
        "print(df_sub.isna().sum())               # Confirm minimal NaNs or 0 if you dropped/imputed\n"
      ],
      "metadata": {
        "id": "M-4pmw2pwOR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_sub.dtypes)  # Check which columns are \"object\"\n",
        "print(df_sub.head())  # Inspect the first rows for those columns\n"
      ],
      "metadata": {
        "id": "z39HuaoCwrQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace non-numeric strings (like '#DIV/0!') with NaN if you haven't already\n",
        "df_sub.replace(\"#DIV/0!\", np.nan, inplace=True)\n",
        "\n",
        "# Convert each column to numeric\n",
        "for col in df_sub.columns:\n",
        "    df_sub[col] = pd.to_numeric(df_sub[col], errors=\"coerce\")\n"
      ],
      "metadata": {
        "id": "1PlHgJJswufB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_sub.dtypes)\n"
      ],
      "metadata": {
        "id": "zY388QaQxGdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_sub[tf_list].values  # shape: (n_samples, n_TFs)\n",
        "y = df_sub[target_genes].values  # shape: (n_samples, n_genes)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n"
      ],
      "metadata": {
        "id": "o0sT1xInxL9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "QWwVvc3zuZSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is Multi-Output Regression?\n",
        "\n",
        "In a typical (single-output) regression problem, you might predict **one continuous variable** $y$ from your features $x_1, x_2, \\dots, x_n$. For example, in a simpler scenario, you might model **ALB** expression alone using 12 TFs.\n",
        "\n",
        "However, **multi-output regression** (sometimes called **multi-variate regression**, in the sense of multiple dependent variables) expands this approach to:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} \\;=\\; \\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_k\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "where $k$ is the number of target variables you want to predict. In your case, $k=4$ for $\\{\\text{ALB}, \\text{AFP}, \\text{APOA1}, \\text{TTR}\\}$.\n"
      ],
      "metadata": {
        "id": "8-lqehVezEqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 2. Define the Columns of Interest\n",
        "# Example:\n",
        "tf_list = [\n",
        "    \"FOXA1\", \"FOXA2\", \"FOXA3\",\n",
        "    \"HNF1A\", \"HNF1B\", \"HNF4A\",\n",
        "    \"ONECUT1\", \"GATA4\", \"CEBPA\",\n",
        "    \"TBX3\", \"PROX1\", \"HHEX\"\n",
        "]\n",
        "target_genes = [\"ALB\", \"AFP\", \"APOA1\", \"TTR\"]  # Our multi-output targets\n",
        "\n",
        "# df_sub is expected to have columns in tf_list + target_genes\n",
        "print(\"Columns in df_sub:\", df_sub.columns.tolist())\n"
      ],
      "metadata": {
        "id": "02hWtZnsulOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split into Features (X) and Targets (y)\n",
        "X = df_sub[tf_list].values      # shape: (num_samples, 12)\n",
        "y = df_sub[target_genes].values # shape: (num_samples, 4)\n"
      ],
      "metadata": {
        "id": "rBelMrxZ13Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shapes: \", X_test.shape,  y_test.shape)\n"
      ],
      "metadata": {
        "id": "dyglurwP2FdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Use a Dataset and DataLoader for mini-batch training\n",
        "class RNARegressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "Sz0KmzKz2KwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = RNARegressDataset(X_train_t, y_train_t)\n",
        "test_dataset  = RNARegressDataset(X_test_t,  y_test_t)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EwipbFtY2Y69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Define a Multi-Output Regression Network\n",
        "class TFRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TFRegressor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)  # 4 outputs for ALB, AFP, APOA1, TTR\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # shape: (batch_size, 4)\n",
        "        return x\n",
        "\n",
        "input_dim  = len(tf_list)      # e.g., 12 TFs\n",
        "output_dim = len(target_genes) # 4 genes\n",
        "model = TFRegressor(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "Nnh5jGUE2e9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Set Loss and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "_AXhZow_2mtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bug here"
      ],
      "metadata": {
        "id": "B32FZObl21OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Training Loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=20)"
      ],
      "metadata": {
        "id": "D-b-5pz52sAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_sub.isna().sum())     # Are there any NaNs in df_sub?\n"
      ],
      "metadata": {
        "id": "F7r6dw-h2y_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isinf(df_sub.values).any()\n"
      ],
      "metadata": {
        "id": "vTvrJesy3JT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Any NaNs in X_train_t?\", torch.isnan(X_train_t).any())\n",
        "print(\"Any Infs in X_train_t?\", torch.isinf(X_train_t).any())\n",
        "print(\"Any NaNs in y_train_t?\", torch.isnan(y_train_t).any())\n",
        "print(\"Any Infs in y_train_t?\", torch.isinf(y_train_t).any())\n"
      ],
      "metadata": {
        "id": "7MeUI5gn3QXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Then do train/test split on X_scaled\n"
      ],
      "metadata": {
        "id": "FyP92nDj3ZfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This shows you exactly which row(s) in df_sub have NaNs\n",
        "na_rows = df_sub[df_sub.isna().any(axis=1)]\n",
        "print(\"Rows with NaN:\\n\", na_rows)\n"
      ],
      "metadata": {
        "id": "VDL4n7zC3pOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any rows that have NaN in any column\n",
        "df_sub.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "# Double-check that no rows have NaN now:\n",
        "print(df_sub.isna().sum())\n"
      ],
      "metadata": {
        "id": "lC3SRF_3348H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into Features (X) and Targets (y)\n",
        "\n"
      ],
      "metadata": {
        "id": "YvgVNEQk5Eqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Features: TFs\n",
        "X = df_sub[tf_list].values    # Shape: (num_samples, 12)\n",
        "\n",
        "# Targets: 4 genes\n",
        "y = df_sub[target_genes].values  # Shape: (num_samples, 4)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "9UmmNxyh4J0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shapes: \", X_test.shape,  y_test.shape)\n"
      ],
      "metadata": {
        "id": "Eq0Rx8l55O_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Verify the scaling\n",
        "print(\"Mean of X_train_scaled:\", X_train_scaled.mean(axis=0))\n",
        "print(\"Std of X_train_scaled:\", X_train_scaled.std(axis=0))"
      ],
      "metadata": {
        "id": "FKtVNJbv5icH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "A2dhax0_57hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Dataset\n",
        "class RNARegressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = RNARegressDataset(X_train_t, y_train_t)\n",
        "test_dataset  = RNARegressDataset(X_test_t,  y_test_t)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EIHmbZMP6ax1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class TFRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TFRegressor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)  # 4 outputs for ALB, AFP, APOA1, TTR\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # Final output without activation for regression\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "input_dim  = len(tf_list)      # 12\n",
        "output_dim = len(target_genes) # 4\n",
        "model = TFRegressor(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "wYTdy29M6eVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "-YeYfFl46irK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=20)\n"
      ],
      "metadata": {
        "id": "3f37pwjS6noO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss is very high: Scale Targets and Retrain the Model"
      ],
      "metadata": {
        "id": "59TcyNpe7URI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize scalers\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "# Fit scaler on training data and transform both training and testing data\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled  = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled  = scaler_y.transform(y_test)\n",
        "\n",
        "# Verify the scaling\n",
        "print(\"Mean of X_train_scaled:\", X_train_scaled.mean(axis=0))\n",
        "print(\"Std of X_train_scaled:\", X_train_scaled.std(axis=0))\n",
        "print(\"Mean of y_train_scaled:\", y_train_scaled.mean(axis=0))\n",
        "print(\"Std of y_train_scaled:\", y_train_scaled.std(axis=0))\n"
      ],
      "metadata": {
        "id": "8V3bLaTB6qxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Convert to PyTorch Tensors\n",
        "import torch\n",
        "\n",
        "X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test_scaled,  dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "y_test_t  = torch.tensor(y_test_scaled, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "EHGr4uKp7aiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Create PyTorch Datasets and DataLoaders\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RNARegressDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = RNARegressDataset(X_train_t, y_train_t)\n",
        "test_dataset  = RNARegressDataset(X_test_t,  y_test_t)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "2mMW6cqp7emj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Define the Multi-Output Regression Network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TFRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TFRegressor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, output_dim)  # 4 outputs for ALB, AFP, APOA1, TTR\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # Final output without activation for regression\n",
        "        return x\n",
        "\n",
        "input_dim  = len(tf_list)      # 12\n",
        "output_dim = len(target_genes) # 4\n",
        "model = TFRegressor(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "3oO5mQJ47lnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Initialize Weights (Optional)\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(init_weights)\n"
      ],
      "metadata": {
        "id": "_4-zq1V87qUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Set Loss and Optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "k0qLJdS77uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Define the Training Loop with Debugging\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "\n",
        "            # Debug: Check for NaN in loss\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss detected at epoch {epoch+1}\")\n",
        "                return\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "qsv7wY3U7x5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Train the Model\n",
        "train_model(model, train_loader, criterion, optimizer, epochs=20)\n"
      ],
      "metadata": {
        "id": "Uq3Qzg-771QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Evaluate on Test Set\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "test_mse = evaluate_model(model, test_loader, criterion)\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "oSvL7RBh75G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. (Optional) Compute R² for Each Gene\n",
        "def compute_r2(model, data_loader, target_genes):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            all_preds.append(y_pred.numpy())\n",
        "            all_targets.append(y_batch.numpy())\n",
        "\n",
        "    all_preds   = np.concatenate(all_preds, axis=0)   # shape: (num_test_samples, 4)\n",
        "    all_targets = np.concatenate(all_targets, axis=0) # same shape\n",
        "\n",
        "    # Inverse transform to original scale if needed\n",
        "    all_preds_orig = scaler_y.inverse_transform(all_preds)\n",
        "    all_targets_orig = scaler_y.inverse_transform(all_targets)\n",
        "\n",
        "    r2_vals = r2_score(all_targets_orig, all_preds_orig, multioutput='raw_values')\n",
        "    print(\"R² per gene:\", dict(zip(target_genes, r2_vals)))\n",
        "\n",
        "    r2_mean = r2_score(all_targets_orig, all_preds_orig, multioutput='uniform_average')\n",
        "    print(f\"Mean R² across all 4 genes: {r2_mean:.3f}\")\n",
        "\n",
        "compute_r2(model, test_loader, target_genes)\n"
      ],
      "metadata": {
        "id": "dq7TlCzt79DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative Approaches to Model Interpretation\n",
        "Given the limitations with `DeepExplainer` for multi-output models, here are alternative methods to interpret your model:\n",
        "\n",
        "a. Using SHAP's KernelExplainer\n",
        "KernelExplainer is a model-agnostic explainer that works by approximating SHAP values through sampling. While it's computationally more intensive than DeepExplainer, it supports multi-output models.\n",
        "\n",
        "Implementation Steps:\n",
        "1. Prepare a Background Dataset: KernelExplainer requires a background dataset to simulate feature distributions.\n",
        "\n",
        "2. Define a Prediction Function: This function should return predictions for the entire set of target genes.\n",
        "\n",
        "3. Compute SHAP Values for Each Gene Separately: Since KernelExplainer doesn't inherently support multi-output explanations, you'll need to compute SHAP values for each gene individually."
      ],
      "metadata": {
        "id": "nH8w3Y6f8wuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n"
      ],
      "metadata": {
        "id": "944P-DoR_z6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SHAP if not already installed\n",
        "!pip install shap\n",
        "\n",
        "import shap\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the model is on CPU\n",
        "model.cpu()\n",
        "\n",
        "# Define a prediction function for KernelExplainer\n",
        "def predict_function(X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.tensor(X, dtype=torch.float32)\n",
        "        outputs = model(inputs).numpy()\n",
        "    return outputs\n",
        "\n",
        "# Select a background dataset (e.g., a subset of training data)\n",
        "background = X_train_scaled[:100]\n",
        "\n",
        "# Initialize KernelExplainer for each target gene\n",
        "explainer_dict = {}\n",
        "shap_values_dict = {}\n",
        "\n",
        "for i, gene in enumerate(target_genes):\n",
        "    print(f\"Computing SHAP values for {gene}...\")\n",
        "    # Define a function that returns only the current gene's prediction\n",
        "    def single_output_predict(X):\n",
        "        return predict_function(X)[:, i]\n",
        "\n",
        "    explainer = shap.KernelExplainer(single_output_predict, background)\n",
        "    shap_values = explainer.shap_values(X_test_scaled[:50], nsamples=100)  # Adjust nsamples as needed\n",
        "    explainer_dict[gene] = explainer\n",
        "    shap_values_dict[gene] = shap_values\n",
        "\n",
        "    # Plot SHAP summary plot for the gene\n",
        "    shap.summary_plot(shap_values, X_test_scaled[:50], feature_names=tf_list, show=False)\n",
        "    plt.title(f'SHAP Summary Plot for {gene}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pyPvm5ax80g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes:\n",
        "\n",
        "* Performance: KernelExplainer can be slow, especially with larger datasets. Adjust nsamples based on your computational resources.\n",
        "\n",
        "* Sample Size: Using a subset of test data (e.g., first 50 samples) can speed up computation. Ensure this subset is representative.\n",
        "\n",
        "* Visualization: The shap.summary_plot provides a global view of feature importance for each gene."
      ],
      "metadata": {
        "id": "h5Pmknd9-Ybg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming shap_values_dict and feature_importances are already computed\n",
        "combined_importances = {}\n",
        "\n",
        "for gene in target_genes:\n",
        "    # SHAP importances (absolute mean SHAP values)\n",
        "    shap_importances = np.abs(shap_values_dict[gene]).mean(axis=0)\n",
        "\n",
        "    # Permutation importances\n",
        "    perm_importances = feature_importances[gene]\n",
        "\n",
        "    combined_importances[gene] = {\n",
        "        'SHAP Importance': shap_importances,\n",
        "        'Permutation Importance': perm_importances\n",
        "    }\n",
        "\n",
        "# Create a combined DataFrame\n",
        "multi_importance_df = {}\n",
        "\n",
        "for gene in target_genes:\n",
        "    for i, tf in enumerate(tf_list):\n",
        "        multi_importance_df[(tf, gene)] = [\n",
        "            combined_importances[gene]['SHAP Importance'][i],\n",
        "            combined_importances[gene]['Permutation Importance'][i]\n",
        "        ]\n",
        "\n",
        "# Convert to DataFrame\n",
        "multi_importance_df = pd.DataFrame.from_dict(multi_importance_df, orient='index',\n",
        "                                             columns=['SHAP Importance', 'Permutation Importance'])\n",
        "multi_importance_df.index = pd.MultiIndex.from_tuples(multi_importance_df.index,\n",
        "                                                      names=['Transcription Factor', 'Target Gene'])\n",
        "\n",
        "# Pivot for heatmap\n",
        "shap_heatmap = multi_importance_df.reset_index().pivot('Transcription Factor', 'Target Gene', 'SHAP Importance')\n",
        "perm_heatmap = multi_importance_df.reset_index().pivot('Transcription Factor', 'Target Gene', 'Permutation Importance')\n",
        "\n",
        "# Plot SHAP Heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(shap_heatmap, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('SHAP Importance of TFs for Each Target Gene', fontsize=16)\n",
        "plt.xlabel('Target Gene', fontsize=14)\n",
        "plt.ylabel('Transcription Factor', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Permutation Importance Heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(perm_heatmap, annot=True, cmap='magma', fmt=\".2f\")\n",
        "plt.title('Permutation Feature Importance of TFs for Each Target Gene', fontsize=16)\n",
        "plt.xlabel('Target Gene', fontsize=14)\n",
        "plt.ylabel('Transcription Factor', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "31bh-k4d-BEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_threshold = 0.05  # Adjust based on your data\n",
        "perm_threshold = 0.02  # Adjust based on your data\n"
      ],
      "metadata": {
        "id": "UoP6HFmL_Qpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Initialize a graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add gene and TF nodes\n",
        "for gene in target_genes:\n",
        "    G.add_node(gene, type='gene', color='red')\n",
        "for tf in tf_list:\n",
        "    G.add_node(tf, type='tf', color='blue')\n",
        "\n",
        "# Add edges based on combined importance\n",
        "for gene in target_genes:\n",
        "    for tf in tf_list:\n",
        "        shap_imp = combined_importances[gene]['SHAP Importance'][tf_list.index(tf)]\n",
        "        perm_imp = combined_importances[gene]['Permutation Importance'][tf_list.index(tf)]\n",
        "\n",
        "        # Define a combined importance metric\n",
        "        combined_imp = shap_imp + perm_imp\n",
        "\n",
        "        if combined_imp > (shap_threshold + perm_threshold):\n",
        "            G.add_edge(tf, gene, weight=combined_imp)\n"
      ],
      "metadata": {
        "id": "7BdKhd1g_eCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4fk5K0a_imr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}