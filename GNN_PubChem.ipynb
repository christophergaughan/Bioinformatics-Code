{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "12go1bP9Ccvi5GjJG1b1WGE0J6vy48O0J",
      "authorship_tag": "ABX9TyMy8kePU+Nw+D2RsalXglRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Bioinformatics-Code/blob/main/GNN_PubChem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary for Antibiotic Compounds Dataset\n",
        "\n",
        "| **Field**            | **Description**                                                                                                                                   |\n",
        "|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **cid**              | The compound identifier (CID), a unique number assigned to the compound in the PubChem database.                                                  |\n",
        "| **cmpdname**         | The name of the compound.                                                                                                                         |\n",
        "| **cmpdsynonym**      | Synonyms for the compound. This can include alternate names, trade names, or common names.                                                        |\n",
        "| **mw**               | The molecular weight of the compound (in grams per mole).                                                                                         |\n",
        "| **mf**               | The molecular formula of the compound, representing the number and types of atoms.                                                                |\n",
        "| **polararea**        | The polar surface area of the compound.                                                                                                           |\n",
        "| **xlogp**            | A measure of the hydrophobicity (logarithm of the partition coefficient between octanol and water) of the compound.                                |\n",
        "| **heavycnt**         | The number of heavy atoms in the compound (atoms that are not hydrogen).                                                                          |\n",
        "| **hbonddonor**       | The number of hydrogen bond donors in the compound, which are atoms that can donate hydrogen atoms to form hydrogen bonds.                        |\n",
        "| **hbondacc**         | The number of hydrogen bond acceptors in the compound, which are atoms that can accept hydrogen atoms to form hydrogen bonds.                     |\n",
        "| **rotatablebond**    | The number of rotatable bonds in the compound, which indicates the flexibility of the molecule.                                                   |\n",
        "| **IUPAC Name**       | The International Union of Pure and Applied Chemistry (IUPAC) name of the compound, following standardized chemical nomenclature.                 |\n",
        "| **Isomeric SMILES**  | The Simplified Molecular Input Line Entry System (SMILES) string, representing the structure of the compound, including stereochemistry information. |\n",
        "| **InChIKey**         | A hashed version of the InChI string, providing a unique identifier for the compound.                                                             |\n",
        "| **InChI**            | The International Chemical Identifier (InChI), a textual representation of the compound’s structure.                                              |\n",
        "| **annotation**       | Additional annotation about the compound, including therapeutic uses and classifications, such as `Anti-Infective Agent`, `Antiprotozoal Agent`, or `Anti-Bacterial Agents`. |\n"
      ],
      "metadata": {
        "id": "Vp0J2qdCEFDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB6kytijC4fF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Check if the file exists\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/PubChem_compound_text_Antibiotics.csv'\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File exists and ready to load!\")\n",
        "else:\n",
        "    print(\"File not found. Check the file path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "6whTlDziFtyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Get basic statistics about the dataset\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Display data types to ensure consistency\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check the first few rows to ensure everything is loaded correctly\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "VZyBlUoBF3Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing xlogp Values Using Predictive Modeling\n",
        "\n",
        "#### What:\n",
        "We have found that 17 values in the xlogp column are missing, and the variability in the existing values is quite high. Simple imputation (e.g., mean or median) could introduce bias, as the distribution of xlogp is broad and not concentrated around a single value. Therefore, we will use predictive modeling to estimate these missing values based on other available features in the dataset.\n",
        "\n",
        "#### Why:\n",
        "Predictive imputation uses the relationships between the missing values and other features to provide more accurate estimations. In this case, we can build a model to predict xlogp using the rest of the columns as inputs. This approach is more robust than simply filling missing values with a constant, as it takes into account the actual distribution and relationships in the data.\n",
        "\n",
        "#### How:\n",
        "1. We will first prepare the dataset, ensuring that no columns used for prediction have missing values themselves.\n",
        "2. We'll split the dataset into two parts:\n",
        "    - One with known xlogp values (to train the model).\n",
        "    - One with missing xlogp values (to impute).\n",
        "3. A regression model (e.g., RandomForestRegressor) will be trained on the known values to predict xlogp.\n",
        "4. We will then use this model to predict the missing values and fill them back into the dataset.\n"
      ],
      "metadata": {
        "id": "0sjn4N_fJJP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/PubChem_compound_text_Antibiotics.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop rows where xlogp is missing for training, and separate the rows with missing xlogp for prediction\n",
        "data_with_xlogp = data.dropna(subset=['xlogp'])\n",
        "data_without_xlogp = data[data['xlogp'].isnull()]\n",
        "\n",
        "# Selecting features to use for predicting xlogp (excluding 'xlogp' itself)\n",
        "features = ['mw', 'polararea', 'heavycnt', 'hbonddonor', 'hbondacc', 'rotbonds', 'exactmass', 'monoisotopicmass']\n",
        "\n",
        "# Training data\n",
        "X_train = data_with_xlogp[features]\n",
        "y_train = data_with_xlogp['xlogp']\n",
        "\n",
        "# The rows without xlogp, which need to be predicted\n",
        "X_pred = data_without_xlogp[features]\n",
        "\n",
        "# Train a Random Forest Regressor model\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the missing xlogp values\n",
        "predicted_xlogp = rf.predict(X_pred)\n",
        "\n",
        "# Impute the missing xlogp values with the predicted values\n",
        "data.loc[data['xlogp'].isnull(), 'xlogp'] = predicted_xlogp\n",
        "\n",
        "# Check the imputation\n",
        "print(data[['xlogp']].isnull().sum())  # This should print 0, meaning all missing values have been filled.\n"
      ],
      "metadata": {
        "id": "-Jsk4bN4JviO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the features for rows where xlogp is missing\n",
        "X_missing = data_without_xlogp.drop(['xlogp'], axis=1)\n",
        "\n",
        "# Ensure that columns in X_train and X_missing are aligned\n",
        "X_missing = X_missing[X_train.columns]\n",
        "\n",
        "# Predict the xlogp values for the missing data\n",
        "predicted_xlogp_missing = rf.predict(X_missing)\n",
        "\n",
        "# Now plot the original vs predicted xlogp values and highlight the imputed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the actual vs predicted xlogp values for training data (in blue)\n",
        "sns.scatterplot(x=y_train, y=rf.predict(X_train), label=\"Predicted (Train)\", color=\"blue\")\n",
        "\n",
        "# Plot the red circles for imputed xlogp values\n",
        "sns.scatterplot(x=data_without_xlogp['xlogp'], y=predicted_xlogp_missing,\n",
        "                label=\"Imputed xlogp (Red Circles)\", color=\"red\", marker=\"o\", s=100)\n",
        "\n",
        "# Plot the perfect prediction line\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
        "         color='green', linestyle='--')\n",
        "\n",
        "plt.title('Actual vs Predicted xlogp Values (with Imputed Highlighted)')\n",
        "plt.xlabel('Actual xlogp')\n",
        "plt.ylabel('Predicted xlogp')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ip7X76wULlTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the features for rows where xlogp is missing\n",
        "X_missing = data_without_xlogp.drop(['xlogp'], axis=1)\n",
        "\n",
        "# Ensure that columns in X_train and X_missing are aligned\n",
        "X_missing = X_missing[X_train.columns]\n",
        "\n",
        "# Predict the xlogp values for the missing data\n",
        "predicted_xlogp_missing = rf.predict(X_missing)\n",
        "\n",
        "# Now plot the actual vs predicted xlogp values and highlight the imputed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the actual vs predicted xlogp values for training data (in blue)\n",
        "sns.scatterplot(x=y_train, y=rf.predict(X_train), label=\"Predicted (Train)\", color=\"blue\")\n",
        "\n",
        "# Plot the red circles for imputed xlogp values\n",
        "sns.scatterplot(x=predicted_xlogp_missing, y=predicted_xlogp_missing,\n",
        "                label=\"Imputed xlogp (Red Circles)\", color=\"red\", marker=\"o\", s=100)\n",
        "\n",
        "# Plot the perfect prediction line\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
        "         color='green', linestyle='--')\n",
        "\n",
        "plt.title('Actual vs Predicted xlogp Values (with Imputed Highlighted)')\n",
        "plt.xlabel('Actual xlogp')\n",
        "plt.ylabel('Predicted xlogp')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3MD7Fo1uMBhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns for correlation matrix\n",
        "numeric_data_with_xlogp = data_with_xlogp.select_dtypes(include=[float, int])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numeric_data_with_xlogp.corr()\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "heatmap.set_title('Correlation Matrix of Key Variables', fontdict={'fontsize': 12}, pad=12)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9f50FB3PNtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) and Correlation Simplification\n",
        "\n",
        "## Why Highly Correlated Variables Provide Redundant Information\n",
        "\n",
        "In any dataset, when two or more variables are highly correlated, it means that they provide similar information. Correlation measures the linear relationship between variables, and a correlation coefficient close to +1 or -1 indicates that one variable can be approximately predicted from the other. In simpler terms, if two variables are highly correlated (either positively or negatively), they are essentially measuring the same aspect of the data.\n",
        "\n",
        "### Positive Correlation (+1)\n",
        "A correlation of +1 indicates a perfect positive relationship, where as one variable increases, the other also increases proportionally. This means the two variables are redundant because they move together in the same direction.\n",
        "\n",
        "### Negative Correlation (-1)\n",
        "A correlation of -1 indicates a perfect negative relationship, where as one variable increases, the other decreases proportionally. In this case, the variables are inversely related, but still provide overlapping information.\n",
        "\n",
        "### Why Redundant Variables Are a Problem\n",
        "When variables are highly correlated, they introduce **multicollinearity** into statistical models like regression. Multicollinearity can cause problems because:\n",
        "- **Unstable Coefficients**: The model may struggle to determine which variable is more important, leading to large standard errors in estimated coefficients.\n",
        "- **Overfitting**: Including too many highly correlated variables increases the risk of overfitting the model to the training data, which may reduce its ability to generalize to new data.\n",
        "\n",
        "## Purpose of Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction technique that helps us simplify a dataset with many features (variables) into fewer dimensions, while preserving the essential information. By transforming correlated variables into a new set of uncorrelated variables called **principal components**, PCA allows us to:\n",
        "1. **Reduce Redundancy**: By combining correlated variables into fewer components, we remove redundant information.\n",
        "2. **Improve Model Performance**: Simplifying the dataset can help reduce the risk of overfitting and improve the stability of models.\n",
        "3. **Visualize Data**: PCA also allows us to visualize high-dimensional data in a lower-dimensional space, helping to understand its structure.\n",
        "\n",
        "## Why We Are Performing This Analysis\n",
        "\n",
        "In our dataset, we observed that some variables are highly correlated with each other, indicating redundancy. To simplify the analysis, reduce multicollinearity, and create a more efficient model, we are applying PCA. Specifically, we aim to:\n",
        "- **Reduce the number of features**: By using PCA, we can combine the information from correlated features into fewer principal components without losing critical information.\n",
        "- **Identify key patterns**: PCA helps us identify the underlying structure of the data and the relationships between variables.\n",
        "- **Prepare data for modeling**: After applying PCA, we will use the principal components as features in our predictive models, improving their performance by removing redundant information.\n",
        "\n",
        "## Steps for Applying PCA\n",
        "\n",
        "1. **Standardize the Data**: Before performing PCA, we need to standardize the data so that each feature has a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the variance in data.\n",
        "2. **Fit PCA**: We will fit PCA to the standardized data and compute the principal components.\n",
        "3. **Visualize Variance Explained**: PCA will return a set of principal components. We will examine how much variance is explained by each component, helping us decide how many components to keep.\n",
        "4. **Transform Data**: Finally, we will use the principal components to transform the dataset, replacing the original variables with the new components.\n"
      ],
      "metadata": {
        "id": "FbYN6RrtPqlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a correlation threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Find pairs of variables with high correlation\n",
        "high_corr_pairs = correlation_matrix.unstack().sort_values(kind=\"quicksort\").drop_duplicates()\n",
        "\n",
        "# Filter out only pairs that exceed the threshold\n",
        "high_corr_pairs = high_corr_pairs[(high_corr_pairs > threshold) | (high_corr_pairs < -threshold)]\n",
        "\n",
        "# Display the high correlation pairs\n",
        "print(high_corr_pairs)\n"
      ],
      "metadata": {
        "id": "UuMx_W2oN2hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Observations:\n",
        "1. Mass-related Variables (mw, exactmass, monoisotopicmass): These variables are perfectly correlated with one another (correlation of 1.0). In any subsequent analysis, we can keep just one of these and discard the others to reduce redundancy.\n",
        "\n",
        "2. Polar Surface Area (polararea): This variable is highly correlated with several other features, including complexity (0.842), hbondacc (0.911), and mw (0.880). We can consider keeping only one of these highly correlated variables.\n",
        "\n",
        "3. Hydrogen Bond Acceptors (hbondacc): This feature is strongly correlated with many other variables, including heavy atom count (0.899), molecular weight (0.913), and complexity (0.873). This suggests that hbondacc overlaps with a lot of the other variables, and we can consider dropping some.\n",
        "\n",
        "4. Complexity: Complexity is highly correlated with molecular weight (0.983) and heavy atom count (0.986). Like the mass variables, complexity provides redundant information in combination with these features."
      ],
      "metadata": {
        "id": "wCzYCtnUQQzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Perform PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique. It is especially useful in cases where the dataset contains highly correlated features. Highly correlated variables (positive or negative) provide redundant information. PCA transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first principal component (PC1), the second greatest variance on the second component (PC2), and so on.\n",
        "\n",
        "This allows us to simplify the dataset while retaining most of its variability. By focusing on a reduced number of principal components, we can capture the essential patterns of the data without having to process all of the original features.\n",
        "\n",
        "In this case, many of our features are highly correlated, and PCA will help us identify the most important features contributing to the variability in our data.\n"
      ],
      "metadata": {
        "id": "VWysyMjDQ0kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Select the features for PCA (excluding 'cid', 'cmpdname', etc.)\n",
        "selected_features = [\n",
        "    'mw', 'polararea', 'complexity', 'heavycnt', 'hbonddonor', 'hbondacc',\n",
        "    'rotbonds', 'exactmass', 'monoisotopicmass', 'charge',\n",
        "    'covalentunitcnt', 'totalatomstereocnt', 'definedatomstereocnt',\n",
        "    'undefinedatomstereocnt', 'totalbondstereocnt', 'definedbondstereocnt',\n",
        "    'undefinedbondstereocnt', 'pclidcnt', 'gpidcnt', 'gpfamilycnt', 'annothitcnt'\n",
        "]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_with_xlogp[selected_features])\n",
        "\n",
        "# Run PCA\n",
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Display explained variance per component\n",
        "for i, variance in enumerate(explained_variance, 1):\n",
        "    print(f'PC{i}: {variance * 100:.2f}%')\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--', color='b')\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance (%)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optional: Create a DataFrame to explore the PCA components\n",
        "pca_df = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
        "pca_df.head()\n"
      ],
      "metadata": {
        "id": "2G6AwD7_QAHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Interpret PCA Results\n",
        "\n",
        "When performing PCA, the output provides important insights into the structure of the dataset. Here’s how to interpret the results step by step:\n",
        "\n",
        "#### 1. **Explained Variance Ratio**\n",
        "   The **explained variance ratio** tells us how much of the total variance in the data is captured by each principal component (PC). Each PC is an orthogonal (uncorrelated) linear combination of the original variables, and the total variance in the dataset is distributed across these PCs.\n",
        "\n",
        "   - **PC1** will explain the largest amount of variance in the data.\n",
        "   - **PC2** will explain the second largest, and so on.\n",
        "\n",
        "   For example, if PC1 has an explained variance ratio of **40%**, that means 40% of the variance in the original data is captured by this component alone.\n",
        "\n",
        "   **Interpreting the first few PCs:**\n",
        "   - The first few principal components (usually PC1, PC2, PC3, etc.) are the most important because they capture most of the variability in the data. Components with very low explained variance (e.g., PC10, PC11) are less important and can be ignored if we want to reduce the dimensionality.\n",
        "   \n",
        "   You can use a rule of thumb that **selects the number of components** that explain around **85% to 95%** of the variance to retain most of the information.\n",
        "\n",
        "#### 2. **Cumulative Explained Variance Plot**\n",
        "   This plot shows how much variance is cumulatively explained by increasing the number of principal components. Typically, the curve will show that most of the variance is captured by the first few components, and then it levels off.\n",
        "\n",
        "   - If the curve flattens after a certain number of PCs (e.g., after PC4), it indicates that **adding more components beyond that point doesn't significantly increase the explained variance.**\n",
        "   - For example, if the plot shows that **90% of the variance** is explained by the first **4 components**, you may choose to keep those 4 components and discard the rest, simplifying the dataset without losing too much information.\n",
        "\n",
        "#### 3. **Principal Component Scores**\n",
        "   The transformed dataset after applying PCA gives us the **principal component scores** for each observation. These scores represent the coordinates of each observation in the new space defined by the principal components. They can be used for further analysis or as input to machine learning models.\n",
        "\n",
        "   - **PC1, PC2, etc.** form new axes where each observation is represented as a point in this new space. These axes capture the most significant patterns in the data.\n",
        "   - **Higher variance along a component** means that component contains more useful information about how the observations vary.\n",
        "\n",
        "#### 4. **Loadings (Contribution of Original Features to Each PC)**\n",
        "   Each principal component is a linear combination of the original features. The coefficients (also called **loadings**) for each original feature in these combinations tell us how much each feature contributes to each principal component.\n",
        "\n",
        "   - **Positive or negative loadings** show the direction of the relationship with that component. Large positive or negative values indicate a strong influence on the component.\n",
        "   - Features with high loadings on the first few principal components are the most important in explaining the variance in the dataset.\n",
        "\n",
        "   **Example Interpretation:**\n",
        "   - If PC1 has high positive loadings for features like `mw` (molecular weight) and `hbonddonor` (hydrogen bond donors), it means that these features are strongly associated with the first principal component.\n",
        "\n",
        "#### 5. **Dimensionality Reduction**\n",
        "   After running PCA, you can choose to keep only the principal components that explain a high percentage of variance (e.g., the first 4 or 5). This reduced set of components can then be used for further analysis or modeling.\n",
        "\n",
        "   - By reducing the dimensionality, we can simplify the data and reduce computational complexity, while still retaining the majority of the original information.\n",
        "\n",
        "### Practical Steps\n",
        "\n",
        "1. **Check how many components explain ~85-95% of the variance.** This will help decide how many PCs to retain.\n",
        "2. **Inspect the loadings** for the first few principal components to understand which features are contributing the most.\n",
        "3. **Use the reduced dataset** (with the selected number of principal components) for further analysis or as input to machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "Ek1TS2LvRSeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explained variance from the PCA\n",
        "pca_explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Plot the explained variance for the top PCA components\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, len(pca_explained_variance)+1), pca_explained_variance, color='blue', alpha=0.7)\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance by Principal Components')\n",
        "plt.xticks(range(1, len(pca_explained_variance)+1))\n",
        "plt.grid(True)\n",
        "\n",
        "# Annotate with variance percentages\n",
        "for i, v in enumerate(pca_explained_variance):\n",
        "    plt.text(i + 1, v + 0.01, f\"{v*100:.2f}%\", ha='center', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TaUE8eMoQ6Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns\n",
        "numerical_columns = data_with_xlogp.select_dtypes(include=[np.number])\n",
        "\n",
        "# Variance of original features (only for numerical columns)\n",
        "original_variance = numerical_columns.var()\n",
        "\n",
        "# Plot comparison of variance in original features and PCA components\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot variance of original features\n",
        "plt.subplot(1, 2, 1)\n",
        "original_variance.plot(kind='bar', color='green', alpha=0.7)\n",
        "plt.title('Variance of Original Features (Numerical)')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Variance')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot explained variance of PCA components\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(range(1, len(pca_explained_variance)+1), pca_explained_variance, color='blue', alpha=0.7)\n",
        "plt.title('Explained Variance of PCA Components')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.xticks(range(1, len(pca_explained_variance)+1))\n",
        "plt.grid(True)\n",
        "\n",
        "# Annotate with variance percentages for PCA\n",
        "for i, v in enumerate(pca_explained_variance):\n",
        "    plt.text(i + 1, v + 0.01, f\"{v*100:.2f}%\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LE_2jT1pSvzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing PCA Results with Original Numerical Features\n",
        "\n",
        "To ensure an accurate comparison between PCA components and the original dataset, we are focusing on **numerical** features only. The `cmpdname`, `cmpdsynonym`, and other non-numerical columns have been excluded from this analysis.\n",
        "\n",
        "- **Original Features**: The variance for each numerical feature is plotted to show how much variability each feature captures within the dataset.\n",
        "- **PCA Components**: The explained variance ratio for each principal component is plotted to demonstrate how well the PCA components summarize the data.\n",
        "\n",
        "This comparison helps us understand how much information is retained after reducing the dimensionality of the dataset with PCA.\n"
      ],
      "metadata": {
        "id": "XAG9uUTfTLFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the principal components without using ace_tools\n",
        "pca_components = pd.DataFrame(pca.components_[:num_components_to_keep],\n",
        "                              columns=numerical_columns.columns[:21],  # Ensure correct number of columns\n",
        "                              index=[f'PC{i+1}' for i in range(num_components_to_keep)])\n",
        "\n",
        "# Display the principal components DataFrame\n",
        "print(\"Principal Components after PCA:\")\n",
        "pca_components\n"
      ],
      "metadata": {
        "id": "uoRSGCoNT2in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components Analysis (PCA) - Results\n",
        "\n",
        "After running PCA on the dataset, we retain **8 components** that together explain 95% of the variance in the data. Each of these components is a linear combination of the original features, and the amount each feature contributes to each component is given by the loadings.\n",
        "\n",
        "- **Variance Explained**: The first few components capture the majority of the variance, allowing us to reduce the dimensionality of the data without losing significant information.\n",
        "- **Loadings**: These indicate how much each original feature contributes to a principal component. High absolute values indicate stronger contributions from that feature.\n"
      ],
      "metadata": {
        "id": "c7Ye2QkeUNy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the number of principal components to keep\n",
        "num_components_to_keep = 8\n",
        "\n",
        "# Update numerical_columns to reflect only those columns used in PCA\n",
        "used_columns = numerical_columns.columns[:len(pca.components_[0])]\n",
        "\n",
        "# Extract the top principal components\n",
        "pca_components_to_keep = pd.DataFrame(pca.components_[:num_components_to_keep],\n",
        "                                      columns=used_columns,\n",
        "                                      index=[f'PC{i+1}' for i in range(num_components_to_keep)])\n",
        "\n",
        "# Only display the top contributing variables for each principal component\n",
        "# We can display the absolute highest loadings for each component\n",
        "top_contributing_features = pca_components_to_keep.apply(lambda x: x.nlargest(5).index, axis=1)\n",
        "\n",
        "# Print the components that we will keep\n",
        "print(\"Top contributing features for each principal component:\")\n",
        "print(top_contributing_features)\n"
      ],
      "metadata": {
        "id": "gPB32ErQVHZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The first two principal components (PC1 and PC2) contribute significantly to explaining the variance, and they have 10 key features combined. Here’s a more refined breakdown:\n",
        "\n",
        "**PC1 captures variability primarily from features such as:**\n",
        "\n",
        "* rotbonds\n",
        "* hbondacc\n",
        "* cid\n",
        "* complexity\n",
        "* polararea\n",
        "\n",
        "**PC2 captures variability from features such as:**\n",
        "\n",
        "* definedbondstereocnt\n",
        "* undefinedbondstereocnt\n",
        "* polararea\n",
        "* gpidcnt\n",
        "* gpfamilycnt\n",
        "\n",
        "The 10 features in PC1 and PC2 give us a broad understanding of which features are influencing the first two principal components the most, which in turn explain a large portion of the variance in the dataset."
      ],
      "metadata": {
        "id": "ER3ak_nkW4c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Apply PCA to the selected numerical columns\n",
        "numerical_columns_for_pca = numerical_columns[['mw', 'polararea', 'complexity', 'xlogp', 'heavycnt',\n",
        "                                               'hbonddonor', 'hbondacc', 'rotbonds', 'exactmass',\n",
        "                                               'monoisotopicmass', 'charge', 'covalentunitcnt',\n",
        "                                               'isotopeatomcnt', 'totalatomstereocnt', 'definedatomstereocnt',\n",
        "                                               'undefinedatomstereocnt', 'totalbondstereocnt',\n",
        "                                               'definedbondstereocnt', 'undefinedbondstereocnt', 'pclidcnt',\n",
        "                                               'gpidcnt']]\n",
        "\n",
        "# Apply PCA to the data\n",
        "pca_components = pca.transform(numerical_columns_for_pca)\n",
        "\n",
        "# Define the target variable (xlogp values)\n",
        "target = data_with_xlogp['xlogp'].values\n",
        "\n",
        "# Check the shape to ensure consistency\n",
        "print(f\"PCA Components shape: {pca_components.shape}\")\n",
        "print(f\"Target shape: {target.shape}\")\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_components, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the Random Forest Regressor to the training data\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print out the model performance\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n"
      ],
      "metadata": {
        "id": "5MGaSRGoajYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr3cHLOFaukh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}