{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "18ubfD7fmgH3bOwlhENLFH5vWf_U6euyt",
      "authorship_tag": "ABX9TyNs61y0AIjr7Nq0eg2GYsC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Bioinformatics-Code/blob/main/Copy_of_VascuLogic_Model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suggestions for Improvement\n",
        "\n",
        "### 1. Enhanced Training Dataset\n",
        "* To train a neural network effectively, I need a high-quality, labeled dataset. I'll try to combine data from the following:\n",
        "Open-Source Biomedical Databases:\n",
        "    * PubMed Central (PMC): Open-access full-text articles from PubMed.\n",
        "    * CORD-19: COVID-19 research dataset with abstracts and full-text papers.\n",
        "    * BioASQ: Biomedical QA datasets, including PubMed citations.\n",
        "    * MIMIC-III: Clinical database with electronic health records (if applicable).\n",
        "### Custom Labeling:\n",
        "* Use your current workflow to label query-aspect pairs manually. For example:\n",
        "    * Positive examples: Relevant abstracts with high similarity scores.\n",
        "    * Negative examples: Irrelevant abstracts.\n",
        "\n",
        "### 2. Target Task for Neural Network\n",
        "* Focus on a specific task such as:\n",
        "    * Abstract Classification: Classify PubMed abstracts into predefined categories like \"Therapeutic Targets\" or \"Clinical Trials.\"\n",
        "    * Query-Result Relevance Ranking: Predict relevance scores between a query and abstract text.\n",
        "    * Abstract Summarization: Improve over existing summarization with fine-tuned models.\n",
        "\n",
        "### 3. Neural Network Architecture\n",
        "* Transformer-Based Models: Utilize pretrained models like BioBERT or PubMedBERT, fine-tuned for:\n",
        "    * Classification (e.g., abstract categories).\n",
        "    * Sequence similarity (e.g., query-abstract relevance).\n",
        "    * Custom Layers: Add dense layers for specific tasks like ranking or classification.\n",
        "    * Summarization Model Enhancement: Fine-tune models like T5 or Pegasus for domain-specific summarization.\n",
        "\n",
        "### 4. Integration with Current Workflow\n",
        "* Modify workflow to:\n",
        "    * Extract relevant training data from open sources.\n",
        "    * Preprocess the data (e.g., tokenization, query-abstract pairs).\n",
        "    * Train and evaluate the neural network.\n",
        "    * Replace or supplement the existing query expansion, summarization, or ranking modules with the trained model."
      ],
      "metadata": {
        "id": "iT5xDx5Dtq4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for Version 2 Notebook\n",
        "\n",
        "1. ### Data Collection:\n",
        "\n",
        "    * Write code to fetch, clean, and preprocess data from open-source databases.\n",
        "    * Augment with your existing queries and PubMed results for a richer dataset.\n",
        "\n",
        "2. ### Preprocessing:\n",
        "\n",
        "    * Implement tokenization, synonym expansion, and labeling pipelines.\n",
        "    * Use tools like `nltk`, `spacy`, or transformers for efficient preprocessing.\n",
        "\n",
        "3. ### Neural Network Design:\n",
        "\n",
        "    * Fine-tune BioBERT or similar models on your dataset.\n",
        "    * Evaluate with metrics like accuracy (for classification) or NDCG (for ranking).\n",
        "\n",
        "4. ### Validation and Deployment:\n",
        "\n",
        "    * Validate the trained model on a holdout dataset.\n",
        "    * Replace or supplement the components in the existing workflow with the neural network outputs.\n",
        "\n",
        "5. ### Feedback Loop:\n",
        "\n",
        "    * Use the neural network predictions to update and refine the dataset iteratively."
      ],
      "metadata": {
        "id": "dcj4YBN1vc3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Strategy\n",
        "### I propose we:\n",
        "\n",
        "1. Design the data collection and preprocessing pipeline for training data.\n",
        "2. Set up a BioBERT-based neural network for a chosen task.\n",
        "3. Integrate evaluation metrics and a feedback mechanism.\n",
        "Optimize for scalability (e.g., batch processing, GPU utilization)"
      ],
      "metadata": {
        "id": "HamiOqGKxod1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch-Specific Approach for Version 2\n",
        "## 1. Data Collection & Preprocessing\n",
        "1. Data Sources:\n",
        "    * Pull data from PubMed, PMC, CORD-19, and other open datasets.\n",
        "    * Format as pairs of queries and abstracts, with labels for training.\n",
        "## Preprocessing with PyTorch:\n",
        "    * Tokenize data using transformers (Hugging Face library) with models like BioBERT or PubMedBERT.\n",
        "    * Use PyTorch DataLoader for batching and efficient data pipeline setup.\n",
        "## 2. Neural Network Setup\n",
        "### Model Architecture:\n",
        "    * Base Model: Fine-tune pretrained transformer models (e.g., BioBERT) for tasks such as:\n",
        "    * Query-to-abstract relevance classification.\n",
        "    * Abstract summarization or classification into predefined categories.\n",
        "    * Custom Layers: Add dense layers for specific tasks with activation functions (e.g., ReLU, Softmax).\n",
        "### Loss Functions:\n",
        "    * Binary Cross-Entropy Loss for relevance classification.\n",
        "    * Cross-Entropy Loss for multi-class categorization.\n",
        "    * MSE or custom loss for ranking tasks.\n",
        "## 3. Training and Validation\n",
        "    * Use PyTorch training loops or torch.nn.Module with Trainer abstraction for efficient training.\n",
        "### GPU Acceleration:\n",
        "    * Utilize CUDA for efficient computation (model.to(device) and .cuda()).\n",
        "### Metrics:\n",
        "    * Accuracy, F1 Score for classification.\n",
        "### NDCG or MAP for ranking.\n",
        "**Techniques:**\n",
        "    * Early stopping to prevent overfitting.\n",
        "    * Learning rate scheduling with torch.optim.lr_scheduler.\n",
        "## 4. Integration and Workflow Update\n",
        "* Replace parts of the existing workflow with PyTorch models:\n",
        "    * Query expansion with a neural network.\n",
        "    * Abstract classification or ranking with a trained model.\n",
        "    * Summarization using fine-tuned T5 or Pegasus on PyTorch.\n",
        "    * <u>Use PyTorch outputs to populate the Excel file grid.</u>"
      ],
      "metadata": {
        "id": "LYHevwdQykIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load the excel sheet into pandas dataframe**"
      ],
      "metadata": {
        "id": "_WsLHNG45EdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/vasculogic/Updated_indications_and_assets.xlsx'\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "vasc_df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to ensure it is loaded correctly\n",
        "vasc_df.head()\n"
      ],
      "metadata": {
        "id": "mXJi_Ka95Dib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start with our rules for how to handle the excel sheet"
      ],
      "metadata": {
        "id": "Pj3Wx9sI3fqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "296BssWKtqEz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def group_queries(data):\n",
        "    \"\"\"\n",
        "    Groups subqueries under their respective major topics.\n",
        "    Rows starting with a number (e.g., '1.', '2.') are considered major topics.\n",
        "    Subqueries are grouped under the last identified major topic.\n",
        "    \"\"\"\n",
        "    grouped_data = []\n",
        "    current_topic = None\n",
        "\n",
        "    for item in data:\n",
        "        if re.match(r'^\\d+\\.', str(item)):  # Check if the item starts with a number followed by '.'\n",
        "            current_topic = item\n",
        "        elif current_topic:\n",
        "            grouped_data.append((item, current_topic))  # Append subquery with its topic\n",
        "\n",
        "    return grouped_data\n",
        "\n",
        "# Example Usage\n",
        "# Assuming `df` is a pandas DataFrame with the zeroth column containing the queries\n",
        "top_indication_col_0 = vasc_df.iloc[:, 0].tolist()  # Extract the zeroth column as a list\n",
        "grouped_queries = group_queries(top_indication_col_0)\n",
        "\n",
        "# Convert to a PyTorch-compatible Dataset\n",
        "class QueryDataset(Dataset):\n",
        "    def __init__(self, grouped_queries):\n",
        "        self.data = grouped_queries\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        query, topic = self.data[idx]\n",
        "        return {\"query\": query, \"topic\": topic}\n",
        "\n",
        "# Create a DataLoader\n",
        "query_dataset = QueryDataset(grouped_queries)\n",
        "query_loader = DataLoader(query_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Example iteration\n",
        "for batch in query_loader:\n",
        "    print(batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Immediate Next Steps\n",
        "### Set Up Data Collection Pipelines:\n",
        "\n",
        "* Start with scripts to pull data from open-source biomedical databases like PubMed, PMC, and CORD-19.\n",
        "* Focus on creating a clean and structured dataset in a format suitable for PyTorch training (e.g., CSV or JSON with fields for queries, abstracts, and labels).\n",
        "### Preprocess the Collected Data:\n",
        "\n",
        "* Tokenize the text using Hugging Face's transformers library and ensure compatibility with models like BioBERT or PubMedBERT.\n",
        "* Split data into training, validation, and testing sets.\n",
        "\n",
        "### Integrate Preprocessing into Workflow:\n",
        "\n",
        "* Create PyTorch-compatible Dataset and DataLoader classes for batching and efficient loading.\n",
        "\n",
        "### Define the Neural Network Task:\n",
        "\n",
        "* Decide on a specific task for initial implementation (e.g., query-to-abstract relevance classification).\n",
        "* Prepare the data labels accordingly."
      ],
      "metadata": {
        "id": "7G0jZdE28OVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Data Collection**\n",
        "\n",
        "Write a function to pull data from open-source databases:"
      ],
      "metadata": {
        "id": "vpCvAAFA8peL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def fetch_pubmed_abstracts(query, max_results=100):\n",
        "    \"\"\"\n",
        "    Fetches abstracts from PubMed based on a search query.\n",
        "    \"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": max_results,\n",
        "        \"retmode\": \"json\",\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "# Example usage\n",
        "abstract_ids = fetch_pubmed_abstracts(\"Cancer therapy\", max_results=10)\n",
        "print(abstract_ids)\n"
      ],
      "metadata": {
        "id": "OvSOqlsf5QIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Data Preprocessing**\n",
        "\n",
        "Use Hugging Face transformers for tokenization:"
      ],
      "metadata": {
        "id": "Ya6KJ7LW9G5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text for input to BioBERT.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "uDPe0BV88_q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: PyTorch Dataset and DataLoader**\n",
        "\n",
        "Structure the data for training:"
      ],
      "metadata": {
        "id": "rlGkxBBo9coK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # Add this import\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Tokenizes text for input to BioBERT.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    return tokens\n",
        "\n",
        "class PubMedDataset(Dataset):\n",
        "    def __init__(self, queries, abstracts, labels):\n",
        "        self.queries = queries\n",
        "        self.abstracts = abstracts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"query\": preprocess_text(self.queries[idx]),\n",
        "            \"abstract\": preprocess_text(self.abstracts[idx]),\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.float),\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "queries = [\"Cancer treatment\", \"Diabetes therapy\"]\n",
        "abstracts = [\"Abstract text 1\", \"Abstract text 2\"]\n",
        "labels = [1, 0]\n",
        "\n",
        "dataset = PubMedDataset(queries, abstracts, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for batch in data_loader:\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "gTe0LabT9qc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "#### 1. Expand Data Collection\n",
        "* Automate fetching data from open-source biomedical datasets like PubMed or PMC.\n",
        "* Write functions to download and parse abstracts based on queries and save them in a structured format (e.g., CSV or JSON).\n",
        "\n",
        "#### 2. Prepare a Unified Dataset\n",
        "* Organize the collected data into three key fields:\n",
        "* Query: The search term or question.\n",
        "* Abstract: The fetched text from the data source.\n",
        "* Label: Relevance or category for supervised learning.\n",
        "\n",
        "#### 3. Integrate Preprocessing\n",
        "* Preprocess the text data (queries and abstracts) using tokenization and other techniques to prepare it for PyTorch."
      ],
      "metadata": {
        "id": "EykqghqP_mEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation\n",
        "\n",
        "Combine queries, abstracts, and labels into a structured format:"
      ],
      "metadata": {
        "id": "dG7kHhdxA_rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "\n",
        "def fetch_pubmed_abstracts(query, max_results=10, api_delay=1.0):\n",
        "    \"\"\"\n",
        "    Fetch abstracts from PubMed based on the query.\n",
        "    \"\"\"\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": max_results,\n",
        "        \"retmode\": \"json\",\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    id_list = response.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "    if not id_list:\n",
        "        return []\n",
        "\n",
        "    # Fetch abstracts for the IDs\n",
        "    abstracts = []\n",
        "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    for pubmed_id in id_list:\n",
        "        fetch_params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": pubmed_id,\n",
        "            \"retmode\": \"xml\",\n",
        "        }\n",
        "        fetch_response = requests.get(fetch_url, params=fetch_params)\n",
        "        # Parse the abstract\n",
        "        if \"<AbstractText>\" in fetch_response.text:\n",
        "            start = fetch_response.text.find(\"<AbstractText>\") + len(\"<AbstractText>\")\n",
        "            end = fetch_response.text.find(\"</AbstractText>\")\n",
        "            abstract = fetch_response.text[start:end]\n",
        "            abstracts.append(abstract)\n",
        "        time.sleep(api_delay)  # Respect PubMed API usage limits\n",
        "\n",
        "    return abstracts\n",
        "\n",
        "# Example usage\n",
        "query = \"Cancer therapy\"\n",
        "abstracts = fetch_pubmed_abstracts(query, max_results=5)\n",
        "print(abstracts)\n"
      ],
      "metadata": {
        "id": "ZtkXf2g79-kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation\n",
        "\n",
        "Combine queries, abstracts, and labels into a structured format:"
      ],
      "metadata": {
        "id": "ZkLL4AXuA2Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "queries = [\"Cancer therapy\", \"Diabetes treatment\"]\n",
        "abstracts = [\n",
        "    \"Abstract about cancer therapy.\",\n",
        "    \"Abstract about diabetes treatment.\",\n",
        "]\n",
        "labels = [1, 0]  # Example labels: 1 = relevant, 0 = not relevant\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {\"query\": queries, \"abstract\": abstracts, \"label\": labels}\n",
        "dataset_df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV for reuse\n",
        "dataset_df.to_csv(\"training_dataset.csv\", index=False)\n",
        "print(dataset_df.head())\n"
      ],
      "metadata": {
        "id": "UP_kz0IlAZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Integration with Preprocessing\n",
        "\n",
        "Use the `PubMedDataset` class and `DataLoader` to prepare batches:"
      ],
      "metadata": {
        "id": "8xevB5vRBc37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load dataset\n",
        "dataset = PubMedDataset(dataset_df[\"query\"].tolist(), dataset_df[\"abstract\"].tolist(), dataset_df[\"label\"].tolist())\n",
        "\n",
        "# Create DataLoader\n",
        "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "for batch in data_loader:\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "T2bURG70BNyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating Data Collection\n",
        "\n",
        "write a script to fetch abstracts from PubMed or other open databases using APIs. For simplicity, we focus on PubMed using their E-utilities API."
      ],
      "metadata": {
        "id": "6uCJFMW4DckG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def fetch_pubmed_abstracts(query, max_results=10, api_delay=1.0):\n",
        "    \"\"\"\n",
        "    Fetch abstracts from PubMed based on a search query.\n",
        "    \"\"\"\n",
        "    # Step 1: Search for article IDs\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "    search_params = {\n",
        "        \"db\": \"pubmed\",\n",
        "        \"term\": query,\n",
        "        \"retmax\": max_results,\n",
        "        \"retmode\": \"json\",\n",
        "    }\n",
        "    search_response = requests.get(base_url, params=search_params)\n",
        "    id_list = search_response.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "    # Step 2: Fetch abstracts for the IDs\n",
        "    abstracts = []\n",
        "    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "    for pubmed_id in id_list:\n",
        "        fetch_params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": pubmed_id,\n",
        "            \"retmode\": \"xml\",\n",
        "        }\n",
        "        fetch_response = requests.get(fetch_url, params=fetch_params)\n",
        "        if \"<AbstractText>\" in fetch_response.text:\n",
        "            start = fetch_response.text.find(\"<AbstractText>\") + len(\"<AbstractText>\")\n",
        "            end = fetch_response.text.find(\"</AbstractText>\")\n",
        "            abstract = fetch_response.text[start:end]\n",
        "            abstracts.append(abstract)\n",
        "        time.sleep(api_delay)  # Avoid overloading the API\n",
        "\n",
        "    return abstracts\n",
        "\n",
        "# Example queries\n",
        "queries = [\"Cancer therapy\", \"Diabetes treatment\"]\n",
        "\n",
        "# Collect abstracts for all queries\n",
        "all_data = []\n",
        "for query in queries:\n",
        "    abstracts = fetch_pubmed_abstracts(query, max_results=5)\n",
        "    for abstract in abstracts:\n",
        "        all_data.append({\"query\": query, \"abstract\": abstract, \"label\": 1})  # Label 1 for relevant\n",
        "\n",
        "# Convert to DataFrame\n",
        "dataset_df = pd.DataFrame(all_data)\n",
        "dataset_df.to_csv(\"expanded_training_dataset.csv\", index=False)\n",
        "print(dataset_df.head())\n"
      ],
      "metadata": {
        "id": "s3m6LmKfBpgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Data\n",
        "\n",
        "Once the data is collected, <u>*preprocess*</u> to ensure it is tokenized and ready for training."
      ],
      "metadata": {
        "id": "mvyQjetDFZ6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer (e.g., BioBERT tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "def preprocess_text_column(df, column_name):\n",
        "    \"\"\"\n",
        "    Tokenize a column of text in a DataFrame.\n",
        "    \"\"\"\n",
        "    tokenized = df[column_name].apply(\n",
        "        lambda x: tokenizer(\n",
        "            x, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
        "        )\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "# Preprocess 'query' and 'abstract' columns\n",
        "dataset_df[\"query_tokens\"] = preprocess_text_column(dataset_df, \"query\")\n",
        "dataset_df[\"abstract_tokens\"] = preprocess_text_column(dataset_df, \"abstract\")\n",
        "print(dataset_df.head())\n"
      ],
      "metadata": {
        "id": "8dycw8G1DyMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integration with PyTorch DataLoader\n",
        "\n",
        "Use the `PubMedDataset` class developed earlier to load and batch the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "rbQhP2zIFzTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define Dataset and DataLoader\n",
        "dataset = PubMedDataset(\n",
        "    queries=dataset_df[\"query\"].tolist(),\n",
        "    abstracts=dataset_df[\"abstract\"].tolist(),\n",
        "    labels=dataset_df[\"label\"].tolist()\n",
        ")\n",
        "\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Example iteration\n",
        "for batch in data_loader:\n",
        "    print(batch)\n"
      ],
      "metadata": {
        "id": "P3s5dqKAFioU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "# Example queries and abstracts\n",
        "queries = [\"Cancer therapy\", \"Diabetes treatment\"]\n",
        "abstracts = [\n",
        "    \"This abstract discusses innovative cancer therapy approaches.\",\n",
        "    \"Recent advances in diabetes treatment are highlighted in this abstract.\"\n",
        "]\n",
        "\n",
        "# Tokenize queries\n",
        "query_tokens = tokenizer(\n",
        "    queries,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Tokenize abstracts\n",
        "abstract_tokens = tokenizer(\n",
        "    abstracts,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Print tensor shapes\n",
        "print(\"Query input_ids shape:\", query_tokens[\"input_ids\"].shape)\n",
        "print(\"Query attention_mask shape:\", query_tokens[\"attention_mask\"].shape)\n",
        "print(\"Abstract input_ids shape:\", abstract_tokens[\"input_ids\"].shape)\n",
        "print(\"Abstract attention_mask shape:\", abstract_tokens[\"attention_mask\"].shape)\n"
      ],
      "metadata": {
        "id": "mSFKr8FYF8hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Architecture\n",
        "### Base Model:\n",
        "\n",
        "* Use BioBERT as the base model for feature extraction.\n",
        "* Extract the [CLS] token representation from the final hidden state.\n",
        "\n",
        "### Custom Layers:\n",
        "\n",
        "* Add a fully connected (dense) layer to project the [CLS] token embedding to the desired output dimension.\n",
        "* Use a dropout layer to mitigate overfitting.\n",
        "\n",
        "### Output:\n",
        "\n",
        "* Apply a sigmoid activation function for binary classification.\n"
      ],
      "metadata": {
        "id": "LKrH8w-5G4DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "class BioBERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"dmis-lab/biobert-v1.1\", dropout_rate=0.3):\n",
        "        super(BioBERTClassifier, self).__init__()\n",
        "        # Load BioBERT model\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)  # Binary classification\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Forward pass through BioBERT\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token\n",
        "        # Apply dropout and classification layer\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return torch.sigmoid(logits).squeeze(-1)  # Sigmoid activation for binary output\n"
      ],
      "metadata": {
        "id": "cwMN6IRzIPNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging- getting problems with:\n",
        "\n",
        "The error RuntimeError: CUDA error: device-side assert triggered is usually related to data issues when using CUDA. In your case, it is likely triggered within the BioBERT model during the forward pass due to:\n",
        "\n",
        "1. Invalid Input: The input data (dummy_input_ids or dummy_attention_mask) might contain values outside the expected range, like negative indices, leading to an assertion failure in the CUDA kernel.\n",
        "2. Shape Mismatch: There might be a mismatch between the shapes of the dummy input tensors and what the model expects, triggering an assertion.\n",
        "3. Data Type: The input tensors might not be in the expected data type (e.g., torch.int64 for input_ids).\n",
        "4. Out of Memory: If your GPU is running out of memory, it can also trigger CUDA errors. Make sure your input sizes are manageable and no unnecessary tensors are lingering in the device memory.\n",
        "5. CUDA context issue: If a previous CUDA operation failed silently, it can sometimes surface as a seemingly unrelated error in a later call.\n"
      ],
      "metadata": {
        "id": "EgnrTg3khyT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Force use of CPU\n",
        "model = BioBERTClassifier().to(device)\n"
      ],
      "metadata": {
        "id": "9W-K3fZ1NXwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model without moving to the device\n",
        "model = BioBERTClassifier()\n",
        "print(\"Model initialized successfully.\")\n"
      ],
      "metadata": {
        "id": "kJ4TUakpNdEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available.\")\n",
        "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "eF6FFxx_O9T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}: {param.shape}\")\n"
      ],
      "metadata": {
        "id": "QZhBbNXKPBSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.vocab_size  # Get vocabulary size\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "print(\"Max Token ID in input_ids:\", torch.max(tokenized_inputs[\"input_ids\"]))\n",
        "print(\"Min Token ID in input_ids:\", torch.min(tokenized_inputs[\"input_ids\"]))\n",
        "\n"
      ],
      "metadata": {
        "id": "W5aYZM7fPGFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Use CPU for debugging\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device)\n",
        "model = BioBERTClassifier().to(device)\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "2N5iVPQIQYgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data type of input_ids:\", tokenized_inputs[\"input_ids\"].dtype)\n",
        "print(\"Data type of attention_mask:\", tokenized_inputs[\"attention_mask\"].dtype)\n"
      ],
      "metadata": {
        "id": "3C9fOBQ9QgxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n"
      ],
      "metadata": {
        "id": "aMbJ8HOSQoq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data type of input_ids:\", tokenized_inputs[\"input_ids\"].dtype)\n",
        "print(\"Shape of input_ids:\", tokenized_inputs[\"input_ids\"].shape)\n",
        "print(\"Input IDs:\", tokenized_inputs[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "csflVXhpTzbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BioBERTClassifier()\n",
        "print(\"Model parameters:\", list(model.named_parameters()))\n"
      ],
      "metadata": {
        "id": "dcR8KYHUT8Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device).float()\n"
      ],
      "metadata": {
        "id": "6QooNJ_JWL5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Allocated memory:\", torch.cuda.memory_allocated())\n",
        "print(\"Reserved memory:\", torch.cuda.memory_reserved())\n"
      ],
      "metadata": {
        "id": "1Riy57HRWZBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "-GDYBq_zWhOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device).float()\n",
        "\n",
        "# Move model to CPU\n",
        "model = BioBERTClassifier().to(device)\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "zrpVA_v1Wq5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "qFcJBGwLUJbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")  # Or \"cuda:1\" if you have a second GPU\n"
      ],
      "metadata": {
        "id": "sYcbtC2QW4dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=128,  # Reduced from 512\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "D1O4jMwFW-uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Allocated memory: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Reserved memory: {torch.cuda.memory_reserved()} bytes\")\n"
      ],
      "metadata": {
        "id": "NUo7UqdUW6kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "LTFB6sKpXLrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Force use of CPU\n",
        "\n",
        "# Move tensors to CPU\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device).float()\n",
        "\n",
        "# Move model to CPU\n",
        "model = BioBERTClassifier().to(device)\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "t5cnVfcNXz38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=64,  # Further reduce max_length\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "MHZcxv8oX2Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a single input example for debugging\n",
        "dummy_texts = [\"This is a test sentence.\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=64,  # Use smaller max_length\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "JSF1JKbHX6IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.init()\n"
      ],
      "metadata": {
        "id": "xcdfvoHlX_-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "rpWuR8B8YHqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")  # Force use of CPU\n",
        "\n",
        "# Move tensors to CPU\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device).float()\n",
        "\n",
        "# Move model to CPU\n",
        "model = BioBERTClassifier().to(device)\n",
        "\n",
        "# Forward pass\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "SvfAbdalabBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=64,  # Further reduce max_length\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "BcLVPrV0aj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a single input example for debugging\n",
        "dummy_texts = [\"This is a test sentence.\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=64,  # Use smaller max_length\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "zPUr70Okaj1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.init()\n"
      ],
      "metadata": {
        "id": "m7r7NQzUajyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "id": "KB3eF-uAajrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_texts = [\"Hello world.\"]\n",
        "tokenized_inputs = tokenizer(\n",
        "    dummy_texts,\n",
        "    max_length=32,  # Minimal size\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move tensors and model to GPU\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)\n",
        "dummy_attention_mask = tokenized_inputs[\"attention_mask\"].to(device).float()\n",
        "model = BioBERTClassifier().to(device)\n",
        "\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "LjO396c1bWIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):  # Match the sequence length\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Summing over the sequence dimension as a simple aggregation\n",
        "        aggregated_input = input_ids.sum(dim=1)  # Shape: [batch_size, input_dim]\n",
        "        return torch.sigmoid(self.linear(aggregated_input))\n",
        "\n",
        "# Dummy tensors for simplified testing\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Simulating embeddings with shape [batch_size, sequence_length]\n",
        "dummy_attention_mask = torch.ones((1, 32)).to(device).float()  # Shape [batch_size, sequence_length]\n",
        "\n",
        "# Test with SimpleClassifier\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Simple model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "biqKlYkHdUti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):  # Match sequence length\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Input size matches sequence length\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Pass the input directly to the linear layer\n",
        "        return torch.sigmoid(self.linear(input_ids))  # Shape: [batch_size, 1]\n",
        "\n",
        "# Dummy tensors for simplified testing\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Simulated embeddings with shape [batch_size, sequence_length]\n",
        "dummy_attention_mask = torch.ones((1, 32)).to(device).float()  # Shape [batch_size, sequence_length]\n",
        "\n",
        "# Test with SimpleClassifier\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Simple model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "3TesmguYdVrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try another class"
      ],
      "metadata": {
        "id": "F1gyYVFGiYBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):  # Match the sequence length\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return torch.sigmoid(self.linear(input_ids))\n",
        "\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Simulated embeddings\n",
        "dummy_attention_mask = torch.ones((1, 32)).to(device).float()\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(dummy_input_ids, dummy_attention_mask)\n",
        "print(\"Simple model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "1gI1rOLoeDBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose the input\n",
        "transposed_input = dummy_input_ids.transpose(1, 0)  # Switch dimensions\n",
        "print(\"Transposed input shape:\", transposed_input.shape)\n"
      ],
      "metadata": {
        "id": "CUghZvigikW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Shape [batch_size, sequence_length]\n"
      ],
      "metadata": {
        "id": "1_nyokl2i2F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_inputs = tokenizer(\n",
        "    [\"This is a test sentence.\"],\n",
        "    max_length=32,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "dummy_input_ids = tokenized_inputs[\"input_ids\"].to(device)  # Shape [batch_size, sequence_length]\n"
      ],
      "metadata": {
        "id": "RMAXT2Chi52c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dummy_input_ids:\", dummy_input_ids)\n",
        "print(\"Shape:\", dummy_input_ids.shape)\n"
      ],
      "metadata": {
        "id": "bgFLnt08i9TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Shape [batch_size, sequence_length]\n",
        "\n",
        "# Transpose the input\n",
        "transposed_input = dummy_input_ids.transpose(1, 0)  # Switch dimensions\n",
        "print(\"Transposed input shape:\", transposed_input.shape)\n",
        "\n",
        "# Example usage with a simple classifier\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return torch.sigmoid(self.linear(input_ids))\n",
        "\n",
        "# Create model\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(transposed_input)  # Ensure shapes align with the model\n",
        "print(\"Model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "uT1nqySjjAkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging concluded- we had a mismatch in matrix multipplication"
      ],
      "metadata": {
        "id": "C3-tDRswn4kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input with correct shape\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Shape [batch_size, input_dim]\n",
        "\n",
        "# Example usage with a simple classifier\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return torch.sigmoid(self.linear(input_ids))\n",
        "\n",
        "# Create model\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(dummy_input_ids)  # Directly use dummy_input_ids\n",
        "print(\"Model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "MJPx36TQjIva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input with transpose\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Shape [batch_size, sequence_length]\n",
        "transposed_input = dummy_input_ids.transpose(0, 1)  # Shape [sequence_length, batch_size]\n",
        "\n",
        "# Example usage with a simple classifier\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(1, input_dim)  # Adapt input dimension for transposed input\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return torch.sigmoid(self.linear(input_ids))\n",
        "\n",
        "# Create model\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(transposed_input)  # Ensure shapes align with the model\n",
        "print(\"Model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "5Nh6mMnYjx9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Adjustments\n",
        "We Want Binary Classification:\n",
        "Ensure input and model are configured as follows:"
      ],
      "metadata": {
        "id": "kYMTLqXokKQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input with shape [batch_size, input_dim]\n",
        "dummy_input_ids = torch.rand((1, 32)).to(device)  # Shape [1, 32]\n",
        "\n",
        "# SimpleClassifier for binary classification\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=32):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        return torch.sigmoid(self.linear(input_ids))\n",
        "\n",
        "# Create model\n",
        "model = SimpleClassifier(input_dim=32).to(device)\n",
        "outputs = model(dummy_input_ids)\n",
        "print(\"Model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "cw1Q6Xunj4US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input with shape [batch_size, sequence_length, embedding_dim]\n",
        "dummy_input_ids = torch.rand((1, 32, 64)).to(device)  # Shape [1, 32, 64]\n",
        "\n",
        "# Sequence-level SimpleClassifier\n",
        "class SequenceClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=64):\n",
        "        super(SequenceClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Apply linear layer to each token\n",
        "        return torch.sigmoid(self.linear(input_ids))  # Shape: [batch_size, sequence_length, 1]\n",
        "\n",
        "# Create model\n",
        "model = SequenceClassifier(input_dim=64).to(device)\n",
        "outputs = model(dummy_input_ids)\n",
        "print(\"Model outputs:\", outputs)\n"
      ],
      "metadata": {
        "id": "pZIQhqAQkV4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. For Binary Classification\n",
        "My task involves classifying an entire input (e.g., determining if a query and abstract are relevant), proceed with the binary classifier:\n",
        "\n",
        "Use the single output ([`0.4945`]) as the predicted score.\n",
        "Apply a threshold (e.g., `0.5`) to convert probabilities into class labels (`0` or `1`).\n",
        "B. For Sequence-Level Predictions\n",
        "Task involves token-level classification (e.g., tagging words in a sequence), continue with the sequence classifier:\n",
        "\n",
        "Interpret the 3D tensor output `[batch_size, sequence_length, 1]`.\n",
        "Flatten the output if necessary, e.g., `outputs.squeeze(-1)` for `shape [batch_size, sequence_length]`."
      ],
      "metadata": {
        "id": "Wwi0Fg2ok65P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a threshold to classify\n",
        "predicted_class = (outputs > 0.5).long()  # Convert probability to class label\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "id": "U6De8O0nkdwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Squeeze to remove the last dimension\n",
        "sequence_scores = outputs.squeeze(-1)  # Shape: [batch_size, sequence_length]\n",
        "\n",
        "# Apply thresholding for token-level classification\n",
        "token_predictions = (sequence_scores > 0.5).long()  # Binary class labels\n",
        "print(\"Token predictions:\", token_predictions)\n"
      ],
      "metadata": {
        "id": "9BeVibnzldEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Binary Classification\n",
        "    * If the task is document- or sentence-level classification, focus on the overall predicted class (e.g., `outputs > 0.5`).\n",
        "\n",
        "    * For training or evaluation:\n",
        "        * Use Binary Cross-Entropy Loss for optimizing binary classification.\n",
        "        * Evaluate using metrics such as accuracy, precision, recall, and F1 score.\n",
        "## B. Token-Level Classification\n",
        "If the task is sequence tagging, use the Token predictions output to:\n",
        "Evaluate individual tokens for their predicted classes.\n",
        "Align predictions with the input tokens for interpretability.\n"
      ],
      "metadata": {
        "id": "yHGqtPKtl-yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example for Metrics Evaluation\n",
        "\n",
        "Binary Classification:"
      ],
      "metadata": {
        "id": "_uIU2oGRm2eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth (example)\n",
        "true_class = torch.tensor([1])\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (predicted_class.squeeze(-1) == true_class).float().mean().item()\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "OsbXS44yll4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth (example for sequence-level labels)\n",
        "true_tokens = torch.tensor([[1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "                             1, 1, 1, 0, 1, 1, 0, 1]])\n",
        "\n",
        "# Token-level accuracy\n",
        "token_accuracy = (token_predictions == true_tokens).float().mean().item()\n",
        "print(f\"Token Accuracy: {token_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "xq5A33EFm9I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Analyze Data Distribution\n",
        "* Check the class distribution for both token-level and binary classification tasks.\n",
        "* Address any imbalances by oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "## 2. Add More Data\n",
        "* Expand your training dataset with real examples from PubMed, BioASQ, or other biomedical datasets to improve generalizability.\n",
        "\n",
        "## 3. Fine-Tune Hyperparameters\n",
        "### For Binary Classification:\n",
        "\n",
        "* Adjust the learning rate, batch size, and dropout rate.\n",
        "* Experiment with different optimizers like AdamW or learning rate schedules.\n",
        "For Token-Level Classification:\n",
        "\n",
        "* Experiment with the sequence length (max_length) to capture more contextual information.\n",
        "\n",
        "## 4. Incorporate Advanced Metrics\n",
        "* Evaluate with precision, recall, and F1 score for a deeper understanding of model performance:"
      ],
      "metadata": {
        "id": "ki26UEQ1oJRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Example for binary classification\n",
        "y_true = [1, 0, 1, 1, 0]  # Replace with ground truth labels\n",
        "y_pred = [1, 0, 1, 0, 0]  # Replace with model predictions\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\"]))\n"
      ],
      "metadata": {
        "id": "qq3oxvD4nEhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for Fine-Tuning\n",
        "Data Preparation and Tokenization\n",
        "Assume `data.csv` contains two columns: `text` (queries/abstracts) and `label` (0 or 1)."
      ],
      "metadata": {
        "id": "wiVTRI4fyzlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vasc_df.head())\n"
      ],
      "metadata": {
        "id": "e79s_BuLzHc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the actual column names\n",
        "text_column = \"Background References (Tile, Year)\"  # Replace with the correct column\n",
        "label_column = \"Preveleance Rank (H=3, M=2, L=1)\"  # Replace with the correct column\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = vasc_df[text_column].astype(str).tolist()\n",
        "label_mapping = {\"H=3\": 3, \"M=2\": 2, \"L=1\": 1}\n",
        "labels = vasc_df[label_column].map(label_mapping).astype(float).tolist()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "\n",
        "# Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.float),\n",
        "        }\n",
        "        return item\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = TextDataset(texts, labels, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "p4gb_cDk0wB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vasc_df.columns)\n"
      ],
      "metadata": {
        "id": "35DqSSwb1AQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = \"'Biomarkers'\"  # Update based on your column\n"
      ],
      "metadata": {
        "id": "TqrhgB5m1uJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping\n",
        "label_column = \"Preveleance Rank (H=3, M=2, L=1)\"\n",
        "label_mapping = {\"H=3\": 3, \"M=2\": 2, \"L=1\": 1}  # Map categorical ranks to numbers\n",
        "\n",
        "# Apply mapping to the column\n",
        "labels = vasc_df[label_column].map(label_mapping)\n",
        "\n",
        "# Check for unmapped or missing values\n",
        "if labels.isnull().any():\n",
        "    print(\"Warning: Some labels were not mapped. Check the data for unexpected values.\")\n",
        "    print(\"Unmapped values:\", vasc_df[label_column][labels.isnull()].unique())\n",
        "\n",
        "# Convert labels to a list of floats (ready for PyTorch)\n",
        "labels = labels.astype(float).tolist()\n"
      ],
      "metadata": {
        "id": "ZikLyEzs2Qig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vasc_df = vasc_df[vasc_df[label_column].isin(label_mapping.keys())]\n"
      ],
      "metadata": {
        "id": "ga3iZ2kD3Pk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "gpkM2yaL3are"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping\n",
        "label_column = \"Preveleance Rank (H=3, M=2, L=1)\"\n",
        "label_mapping = {\"H=3\": 3, \"M=2\": 2, \"L=1\": 1}  # Map categorical ranks to numbers\n",
        "\n",
        "# Apply mapping to the column\n",
        "labels = vasc_df[label_column].map(label_mapping)\n",
        "\n",
        "# Check for unmapped or missing values\n",
        "if labels.isnull().any():\n",
        "    print(\"Warning: Some labels were not mapped. Check the data for unexpected values.\")\n",
        "    print(\"Unmapped values:\", vasc_df[label_column][labels.isnull()].unique())\n",
        "\n",
        "# Convert labels to a list of floats (ready for PyTorch)\n",
        "labels = labels.astype(float).tolist()\n"
      ],
      "metadata": {
        "id": "5Hi3K_ZJ3hzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vasc_df = vasc_df[vasc_df[label_column].isin(label_mapping.keys())]\n"
      ],
      "metadata": {
        "id": "gyublPzZ5_Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample labels:\", labels[:10])\n"
      ],
      "metadata": {
        "id": "4fMHrU-L6KlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with the correct column name\n",
        "print(vasc_df[[\"Development Drug Count\", \"Clinical Burden Rank (Trial Duration, N) *PI\"]].head())\n"
      ],
      "metadata": {
        "id": "ZiuQS_Ob6i-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_column = \"Development Drug Count\"\n",
        "texts = vasc_df[\"Biomarkers\"].astype(str).tolist()\n",
        "labels = vasc_df[label_column].astype(float).tolist()\n"
      ],
      "metadata": {
        "id": "_9DWZdfN6-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vasc_df[[\"Development Drug Count\", \"Clinical Burden Rank (Trial Duration, N) *PI\"]].isnull().sum())\n"
      ],
      "metadata": {
        "id": "x-fi7m6r7Gvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows with non-missing values\n",
        "filtered_df = vasc_df.dropna(subset=[\"Development Drug Count\", \"Clinical Burden Rank (Trial Duration, N) *PI\"])\n",
        "\n",
        "# Check filtered data\n",
        "print(filtered_df[[\"Development Drug Count\", \"Clinical Burden Rank (Trial Duration, N) *PI\"]].head())\n"
      ],
      "metadata": {
        "id": "xd9TXS2p7U7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define label column\n",
        "label_column = \"Development Drug Count\"\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = filtered_df[\"Biomarkers\"].astype(str).tolist()\n",
        "labels = filtered_df[label_column].astype(float).tolist()\n",
        "\n",
        "# Verify\n",
        "print(\"Sample texts:\", texts[:5])\n",
        "print(\"Sample labels:\", labels[:5])\n"
      ],
      "metadata": {
        "id": "DXDz9K8z7mAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = vasc_df[\"Biomarkers\"].astype(str).tolist()\n"
      ],
      "metadata": {
        "id": "XrVb_k6Q7cic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load BioBERT\n",
        "model_name = \"dmis-lab/biobert-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Generate embeddings for each row\n",
        "embeddings = []\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
        "    outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "    embeddings.append(cls_embedding.detach().numpy())\n",
        "\n",
        "# Use embeddings for classification, ranking, etc.\n"
      ],
      "metadata": {
        "id": "nODmjvQT8pz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Assign random relevance scores (replace with model-based logic)\n",
        "vasc_df[\"Relevance Score\"] = [float(e[0, 0]) for e in embeddings]  # Replace with predictions\n",
        "\n",
        "# Save to Excel\n",
        "vasc_df.to_excel(\"updated_vasc_df.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "cK86tEGr80k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select input and target columns\n",
        "text_column = \"Biomarkers\"\n",
        "label_column = \"Preveleance Rank (H=3, M=2, L=1)\"\n",
        "\n",
        "# Extract texts and labels\n",
        "texts = vasc_df[text_column].astype(str).tolist()\n",
        "label_mapping = {\"H=3\": 3, \"M=2\": 2, \"L=1\": 1}\n",
        "labels = vasc_df[label_column].map(label_mapping).astype(float).tolist()\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_texts(train_texts, tokenizer)\n",
        "val_encodings = tokenize_texts(val_texts, tokenizer)\n"
      ],
      "metadata": {
        "id": "LOQfrZTI83ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Texts:\", texts[:5])\n",
        "print(\"Labels:\", labels[:5])\n"
      ],
      "metadata": {
        "id": "3ugdMPn8ADkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vasc_df[[\"Biomarkers\", \"Preveleance Rank (H=3, M=2, L=1)\"]].head())\n"
      ],
      "metadata": {
        "id": "t7zBiMttCtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_10jCbDzC0gG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}