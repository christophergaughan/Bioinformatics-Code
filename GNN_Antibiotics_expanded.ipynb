{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1NpUT_gA5p7UoGA4u1c0wHVBVpVCZrKkS",
      "authorship_tag": "ABX9TyNGKL5vhaxtfNBNbMnpWwRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Bioinformatics-Code/blob/main/GNN_Antibiotics_expanded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBIWde22kpAj"
      },
      "outputs": [],
      "source": [
        "!pip install chembl_webresource_client\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of the GNN Regression Approach for Antibiotic Prediction\n",
        "\n",
        "In this study, we aim to use **Graph Neural Networks (GNNs)** to predict the **bioactivity** of chemical compounds as potential antibiotics. Instead of classifying compounds as \"active\" or \"inactive,\" we will treat this as a **regression problem** where the GNN model will predict the **IC50** value directly. The **IC50** value represents the concentration at which a compound produces 50% of its maximal effect, and it is a widely used measure of compound potency.\n",
        "\n",
        "#### Why Use Regression Instead of Classification?\n",
        "- **Granularity of Data**: Predicting the IC50 value directly allows us to capture more nuanced information about the compound's activity. This provides greater granularity than a simple binary label and can help identify promising candidates that might be highly effective.\n",
        "- **Flexible Analysis**: By working with continuous targets, we retain all available bioactivity data, which can be useful for downstream analyses such as ranking compounds by potency or defining custom thresholds for activity.\n",
        "- **Model Performance**: With more information retained in the dataset, the GNN model can better learn relationships between compound structure and potency, potentially improving the model's accuracy and utility.\n",
        "\n",
        "#### Data Preparation Steps\n",
        "1. **Data Retrieval from ChEMBL**: We will use the **ChEMBL Python client** to retrieve compound information, including **molecular structure (SMILES)**, **molecular properties** (e.g., molecular weight, LogP, HBA), and **bioactivity data (IC50)**. We will use compounds that have an **IC50 value** available to provide a continuous target for the regression model.\n",
        "\n",
        "2. **Dataset Creation**: We will store the retrieved data in a **pandas DataFrame** and ensure each compound includes the necessary features (e.g., SMILES, molecular properties, IC50 value). The IC50 value will be used as the target for training the regression model.\n",
        "\n",
        "3. **GNN Model Setup**: We will define a **Graph Neural Network (GNN)** that takes molecular graph representations as input and predicts a single continuous output (IC50 value). The GNN will consist of **graph convolutional layers** (using **GCNConv**) followed by a **fully connected layer** to produce the IC50 value.\n",
        "\n",
        "4. **Training the Model**: We will train the model using a **regression loss function**, such as **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**, to minimize the difference between the predicted and true IC50 values.\n",
        "\n",
        "5. **Data Normalization**: Since IC50 values can vary significantly in magnitude, we may apply a **log transformation** (e.g., log(IC50)) to improve the model's learning process and numerical stability.\n",
        "\n",
        "#### Expected Outcome\n",
        "The GNN model will predict IC50 values for new compounds, allowing us to assess their potential as antibiotics based on potency. By predicting continuous values rather than simple classifications, we aim to identify promising compounds with greater precision, helping guide future experimental validation.\n"
      ],
      "metadata": {
        "id": "qPBQ9lWTHrcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Retrieval Criteria for the Study\n",
        "\n",
        "To ensure the quality and relevance of the dataset used in this study, we applied specific filtering criteria when retrieving compound information from ChEMBL:\n",
        "\n",
        "- **Small Molecule Compounds**: We focused on retrieving **small molecule compounds** by filtering out biotherapeutics (`biotherapeutic__isnull=True`). This ensures that we only include compounds suitable for small molecule drug discovery.\n",
        "- **Phase 3 Clinical Trials or Higher**: We selected compounds that have reached at least **Phase 3 clinical trials** (`max_phase__gte=3`). Reaching Phase 3 indicates that the compound has demonstrated sufficient **safety and efficacy** in earlier stages of testing. Compounds at this stage have undergone rigorous evaluation in human trials and are more likely to be effective and safe for use.\n",
        "\n",
        "These criteria help us ensure that the dataset consists of well-characterized compounds with promising potential for use as antibiotics.\n"
      ],
      "metadata": {
        "id": "B3qlf3ZbLrCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Binning Compounds Based on Regression Analysis: A key assumption of our model and what it means\n",
        "\n",
        "Once we have used the GNN model to predict **IC50 values** for the compounds, we can further categorize these compounds into **'active'** and **'inactive'** groups based on the predicted IC50 values. This binning process helps in defining a clear threshold for activity and allows us to identify compounds that are most promising for further development.\n",
        "\n",
        "- **Threshold Definition**: We can define a specific IC50 threshold to determine whether a compound is considered **'active'** or **'inactive'**. For example, compounds with **IC50 <= 1000 nM (1 µM)** could be labeled as **'active'**, while those with **IC50 > 1000 nM** could be labeled as **'inactive'**. Implicit in our assumption is that molecules with **IC50 <= 1 µM** are likely to be effective, but those with higher IC50 values are not necessarily completely inefficacious. We are aiming to identify antibiotic small molecules with **high binding efficiency**.\n",
        "- **Post-Processing Step**: This binning can be performed as a post-processing step after regression, allowing us to create binary labels for downstream analysis or decision-making.\n",
        "- **Advantages**: By first predicting IC50 values and then binning them, we retain the flexibility to experiment with different thresholds and refine the criteria for classifying compounds. This allows us to capture more nuanced insights about compound efficacy and adjust the analysis as needed.\n",
        "\n",
        "This approach ensures that we are able to both quantify the potency of the compounds and categorize them effectively, providing valuable insights for future experimental validation and prioritization.\n",
        "\n",
        "So this is a not a perfect method of binning our results into , however it is *a* beginning. We have to strat from somewhere\n",
        "\n"
      ],
      "metadata": {
        "id": "xp4F3dvrNPrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the ChEMBL web resource client\n",
        "!pip install chembl_webresource_client\n",
        "\n",
        "# Import required libraries\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "import pandas as pd\n",
        "import requests\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the ChEMBL client\n",
        "molecule = new_client.molecule\n",
        "activity = new_client.activity\n",
        "\n",
        "# Define a search for small molecules with activity data\n",
        "# Limiting the query to the first 100 compounds for testing purposes\n",
        "molecules = molecule.filter(biotherapeutic__isnull=True, max_phase__gte=3)[:100]\n",
        "\n",
        "# Create an empty list to store results\n",
        "data = []\n",
        "\n",
        "# Fetch details for each molecule and bioactivity data\n",
        "for idx, compound in enumerate(molecules):\n",
        "    try:\n",
        "        # Extract compound properties\n",
        "        molecule_chembl_id = compound.get('molecule_chembl_id')\n",
        "        molecule_name = None\n",
        "        if compound.get('molecule_properties'):\n",
        "            molecule_name = compound['molecule_properties'].get('full_molformula')\n",
        "\n",
        "        smiles = None\n",
        "        if compound.get('molecule_structures'):\n",
        "            smiles = compound['molecule_structures'].get('canonical_smiles')\n",
        "\n",
        "        molecular_weight = None\n",
        "        logp = None\n",
        "        hba = None\n",
        "        hbd = None\n",
        "        aromatic_rings = None\n",
        "        if compound.get('molecule_properties'):\n",
        "            molecular_weight = compound['molecule_properties'].get('mw_freebase')\n",
        "            logp = compound['molecule_properties'].get('alogp')\n",
        "            hba = compound['molecule_properties'].get('hba')\n",
        "            hbd = compound['molecule_properties'].get('hbd')\n",
        "            aromatic_rings = compound['molecule_properties'].get('aromatic_rings')\n",
        "\n",
        "        # Fetch bioactivity data for the compound\n",
        "        activities = activity.filter(molecule_chembl_id=molecule_chembl_id)\n",
        "\n",
        "        # Extract IC50 value if available\n",
        "        ic50_value = None\n",
        "        for act in activities:\n",
        "            if act['standard_type'] == 'IC50' and act['standard_value'] is not None:\n",
        "                ic50_value = float(act['standard_value'])\n",
        "                break  # Use the first available IC50 value\n",
        "\n",
        "        # Append the data only if SMILES and IC50 value are available\n",
        "        if smiles and ic50_value is not None:\n",
        "            data.append({\n",
        "                'Molecule ChEMBL ID': molecule_chembl_id,\n",
        "                'Molecule Name': molecule_name,\n",
        "                'SMILES': smiles,\n",
        "                'Molecular Weight': molecular_weight,\n",
        "                'LogP': logp,\n",
        "                'HBA': hba,\n",
        "                'HBD': hbd,\n",
        "                'Aromatic Rings': aromatic_rings,\n",
        "                'IC50 (nM)': ic50_value\n",
        "            })\n",
        "\n",
        "        # Print progress every 10 compounds\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Processed {idx + 1} compounds so far...\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API request failed for compound {molecule_chembl_id}: {e}, retrying in 5 seconds...\")\n",
        "        sleep(5)  # Wait for 5 seconds and then retry\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Add Log IC50 (nM) column to the DataFrame\n",
        "df['Log IC50 (nM)'] = df['IC50 (nM)'].apply(lambda x: np.log10(x))\n",
        "\n",
        "# Display the first few rows to verify\n",
        "df.head()"
      ],
      "metadata": {
        "id": "a-RhK4McbWRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration:\n",
        "\n",
        "Visualize the data to better understand distributions and relationships. For example, plot histograms of Molecular Weight, LogP, and Log IC50 (nM) to understand the variation in these features:"
      ],
      "metadata": {
        "id": "SQ5Khb-EfN3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histogram of Log IC50 values\n",
        "plt.hist(df['Log IC50 (nM)'], bins=20, color='blue', edgecolor='black')\n",
        "plt.xlabel('Log IC50 (nM)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Log IC50 Values')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TpAQPv7xctE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observations:\n",
        "### Log IC50 Values:\n",
        "\n",
        "Most of the Log IC50 values fall between 1 and 5, indicating that most compounds have IC50 values between 10 nM and 100,000 nM.\n",
        "There are a few outliers with very high Log IC50 values (e.g., greater than 10).\n",
        "Data Skewness:\n",
        "\n",
        "The distribution is right-skewed, with a concentration of compounds in the range of lower Log IC50 values, and a few outliers with very high IC50 values. This indicates that while many compounds have reasonable activity, a few may be quite ineffective.\n",
        "## Next Steps:\n",
        "Handle Outliers:\n",
        "\n",
        "Outliers with very high IC50 values may affect model training negatively. You could consider removing or capping these outliers to improve model robustness:"
      ],
      "metadata": {
        "id": "4Qn1KFVCfp_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove compounds with Log IC50 values above a threshold (e.g., 10)\n",
        "df_filtered = df[df['Log IC50 (nM)'] <= 10]\n",
        "\n"
      ],
      "metadata": {
        "id": "h-tsI1_Fcz7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance the Dataset:\n",
        "\n",
        "If the binary classification for active/inactive is highly imbalanced, consider balancing the dataset by undersampling inactive compounds or oversampling active ones:"
      ],
      "metadata": {
        "id": "oafpINlGdIOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary labels based on IC50 threshold\n",
        "df['Active'] = df['IC50 (nM)'].apply(lambda x: 1 if x <= 1000 else 0)\n",
        "\n",
        "# Check class distribution\n",
        "print(df['Active'].value_counts())\n"
      ],
      "metadata": {
        "id": "ffktkOmGZ4Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training Preparation:\n",
        "\n",
        "Now that we have insights into the distribution, we can move forward with splitting the dataset into training and testing sets for model training.\n",
        "We can also use SMILES to create molecular graphs for GNN input. Tools like RDKit can be used for this purpose.\n",
        "## Visualize Relationships:\n",
        "\n",
        "To further understand how features like LogP, Molecular Weight, etc., relate to IC50, you could use scatter plots:"
      ],
      "metadata": {
        "id": "4KvG0acCgyGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Scatter plot of Molecular Weight vs Log IC50\n",
        "sns.scatterplot(data=df, x='Molecular Weight', y='Log IC50 (nM)')\n",
        "plt.title('Molecular Weight vs Log IC50 (nM)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0-xuPE93dO9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Outliers and Dataset Balancing\n",
        "\n",
        "To proceed effectively with our dataset, we need to address outliers and any class imbalance issues.\n",
        "\n",
        "1. **Handle Outliers**:\n",
        "   - We can observe that there are some extreme values in **Log IC50** (e.g., values greater than **10**). These values can negatively affect the model's ability to learn relationships.\n",
        "   - Therefore, we will filter out compounds with **Log IC50 values greater than 10** to remove these outliers and focus on the more relevant range.\n",
        "   - Filtering code:\n",
        "     ```python\n",
        "     df_filtered = df[df['Log IC50 (nM)'] <= 10]  # Filtering out extreme values\n",
        "     ```\n",
        "\n",
        "2. **Balance the Dataset**:\n",
        "   - From the class distribution, we observed that the dataset is slightly imbalanced, with **52 inactive compounds** and **43 active compounds**.\n",
        "   - To balance the dataset, we will use the **SMOTE** (Synthetic Minority Over-sampling Technique) method to generate synthetic examples for the minority class.\n",
        "   - This approach will ensure our model is trained with a more balanced dataset and thus be better able to learn the differences between active and inactive compounds.\n",
        "   - Balancing code:\n",
        "     ```python\n",
        "     !pip install imbalanced-learn\n",
        "     from imblearn.over_sampling import SMOTE\n",
        "\n",
        "     # Features and target extraction\n",
        "     X = df_filtered[['Molecular Weight', 'LogP', 'HBA', 'HBD', 'Aromatic Rings', 'Log IC50 (nM)']]\n",
        "     y = df_filtered['Active']\n",
        "\n",
        "     # Oversample the minority class using SMOTE\n",
        "     smote = SMOTE(sampling_strategy='auto')\n",
        "     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "     # Create a balanced DataFrame\n",
        "     df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Active')], axis=1)\n",
        "     ```\n",
        "\n",
        "3. **Visualize Balanced Data**:\n",
        "   - After balancing, we will visualize the new class distribution to confirm that we have an equal number of **active** and **inactive** compounds.\n",
        "   - Visualization code:\n",
        "     ```python\n",
        "     print(df_balanced['Active'].value_counts())\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "UpLIjRDFieIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df[df['Log IC50 (nM)'] <= 10]  # Filtering out extreme values\n"
      ],
      "metadata": {
        "id": "FdPSoZWthAbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Features and target extraction\n",
        "X = df_filtered[['Molecular Weight', 'LogP', 'HBA', 'HBD', 'Aromatic Rings', 'Log IC50 (nM)']]\n",
        "y = df_filtered['Active']\n",
        "\n",
        "# Oversample the minority class using SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto')\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create a balanced DataFrame\n",
        "df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Active')], axis=1)\n"
      ],
      "metadata": {
        "id": "41rJTdriinmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_balanced['Active'].value_counts())\n"
      ],
      "metadata": {
        "id": "QKjzHWpyisPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for Graph Neural Network (GNN) Model Training\n",
        "\n",
        "1. **Convert SMILES to Molecular Graphs**:\n",
        "   - To train a GNN, we need to convert the **SMILES** representation of each compound into a molecular graph.\n",
        "   - We can use **RDKit** to convert SMILES into molecular graphs that are compatible with frameworks like **PyTorch Geometric**.\n",
        "   - Conversion code:\n",
        "     ```python\n",
        "     !pip install rdkit\n",
        "\n",
        "     from rdkit import Chem\n",
        "\n",
        "     # Convert SMILES to RDKit molecule objects\n",
        "     df_balanced['Mol'] = df_balanced['SMILES'].apply(lambda x: Chem.MolFromSmiles(x))\n",
        "     ```\n",
        "\n",
        "2. **Model Training Preparation**:\n",
        "   - With the dataset filtered, balanced, and converted into molecular graphs, we can proceed to split the dataset into **training** and **testing** sets and prepare the data for **GNN model training**.\n"
      ],
      "metadata": {
        "id": "ldx71UD-i_sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n"
      ],
      "metadata": {
        "id": "FVYa7P5zi1Ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Ensure SMILES is present in the DataFrame for later concatenation\n",
        "if 'SMILES' in df_filtered.columns:\n",
        "    X = df_filtered[['Molecular Weight', 'LogP', 'HBA', 'HBD', 'Aromatic Rings', 'Log IC50 (nM)']]\n",
        "    y = df_filtered['Active']\n",
        "    smiles = df_filtered['SMILES']  # Extract the SMILES column separately\n",
        "else:\n",
        "    raise KeyError(\"SMILES column not found in df_filtered DataFrame.\")\n",
        "\n",
        "# Oversample the minority class using SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto')\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create a balanced DataFrame and add the SMILES back\n",
        "df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns),\n",
        "                         pd.Series(y_resampled, name='Active'),\n",
        "                         smiles.reset_index(drop=True)], axis=1)\n"
      ],
      "metadata": {
        "id": "TDNb2Fx2jUPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n",
        "\n",
        "from rdkit import Chem\n",
        "\n",
        "# Convert SMILES to RDKit molecule objects\n",
        "if 'SMILES' in df_balanced.columns:\n",
        "    df_balanced['Mol'] = df_balanced['SMILES'].apply(lambda x: Chem.MolFromSmiles(x) if pd.notna(x) else None)\n",
        "else:\n",
        "    raise KeyError(\"SMILES column not found in df_balanced DataFrame.\")\n"
      ],
      "metadata": {
        "id": "Itdb6yvdkUR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps for GNN Model Training\n",
        "Now that we have handled outliers, balanced the dataset, and converted SMILES strings into molecular graphs, we are ready to proceed with Graph Neural Network (GNN) model training. Here’s what we need to do next:\n",
        "\n",
        "1. Split the Dataset into Training and Testing Sets\n",
        "Before training our GNN, we need to split the data into training and testing sets. This allows us to train the model and evaluate its performance on unseen data.\n",
        "Use Scikit-learn to split the dataset:"
      ],
      "metadata": {
        "id": "A5swRxOjk5qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "train_data, test_data = train_test_split(df_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Testing set size: {len(test_data)}\")\n"
      ],
      "metadata": {
        "id": "tHl8v1tBka8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create Graph Representations for GNN\n",
        "\n",
        "We need to convert our molecular data into graph representations that are compatible with frameworks like PyTorch Geometric.\n",
        "This involves extracting graph information such as nodes (atoms) and edges (bonds) from each molecule."
      ],
      "metadata": {
        "id": "QT4_KAcDlJ9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from rdkit.Chem import rdmolops\n",
        "\n",
        "# Function to convert RDKit Mol object to PyTorch Geometric Data object\n",
        "def mol_to_graph(mol):\n",
        "    if mol is None:\n",
        "        return None\n",
        "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    edge_index = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index.append([start, end])\n",
        "        edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Convert training and testing data\n",
        "train_data['graph'] = train_data['Mol'].apply(mol_to_graph)\n",
        "test_data['graph'] = test_data['Mol'].apply(mol_to_graph)\n"
      ],
      "metadata": {
        "id": "ocpE2St4lDJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Prepare Data for GNN Training\n",
        "* The training and testing data can now be prepared for use with PyTorch Geometric.\n",
        "* We will create a list of graph objects to feed into our GNN model."
      ],
      "metadata": {
        "id": "UTt-T6dclXyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out None graphs (in case of failed conversions)\n",
        "train_graphs = [graph for graph in train_data['graph'] if graph is not None]\n",
        "test_graphs = [graph for graph in test_data['graph'] if graph is not None]\n",
        "\n",
        "# Print the number of valid graphs\n",
        "print(f\"Number of training graphs: {len(train_graphs)}\")\n",
        "print(f\"Number of testing graphs: {len(test_graphs)}\")\n"
      ],
      "metadata": {
        "id": "vbIN09gglP12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Define the GNN Model\n",
        "\n",
        "* Define the architecture for the GNN model using PyTorch and PyTorch Geometric. The model will take in molecular graphs and output a predicted IC50 value."
      ],
      "metadata": {
        "id": "LVfppePNl0RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 16)\n",
        "        self.conv2 = GCNConv(16, 32)\n",
        "        self.fc1 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = torch.mean(x, dim=0)  # Global mean pooling\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fNSbWmrNlsiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Train the GNN Model\n",
        "* Now that we have defined our model, we can proceed with training.\n",
        "* Set up the optimizer, loss function, and the training loop.\n"
      ],
      "metadata": {
        "id": "edOD9USMl__e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Function to convert RDKit Mol object to PyTorch Geometric Data object, with target y value\n",
        "def mol_to_graph(mol, target):\n",
        "    if mol is None:\n",
        "        return None\n",
        "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    edge_index = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index.append([start, end])\n",
        "        edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    # Add target y value to the Data object\n",
        "    y = torch.tensor([target], dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Convert training and testing data, adding target y value\n",
        "train_data['graph'] = train_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "test_data['graph'] = test_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n"
      ],
      "metadata": {
        "id": "mtp7UQ88mIEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated Training Loop:\n"
      ],
      "metadata": {
        "id": "6w0mKLNVmiIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for data in train_data['graph']:\n",
        "        if data is None:  # Skip any None values\n",
        "            continue\n",
        "        data = data.to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.0001)  # Reduced learning rate\n",
        "        output = model(data)\n",
        "        loss = criterion(output, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "mhl9q36KmcXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the model is improving, but the loss reduction is still slow and might be plateauing again. The following suggestions can help improve convergence and model performance:\n",
        "\n",
        "## Suggestions for Further Improvement:\n",
        "Increase Model Capacity:\n",
        "\n",
        "Adding more layers or increasing the size of hidden units can help capture complex relationships in the data better."
      ],
      "metadata": {
        "id": "ZkcpfOtNnboY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)  # Increased number of channels\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.conv3 = GCNConv(128, 128)  # Adding an additional GCN layer\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = torch.mean(x, dim=0)  # Global mean pooling\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "tcSicjPXmnjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use a Learning Rate Scheduler:\n",
        "\n",
        "A learning rate scheduler can help dynamically adjust the learning rate during training to avoid getting stuck in plateaus."
      ],
      "metadata": {
        "id": "w18URd_2nnJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.7)  # Reduce LR every 10 epochs\n"
      ],
      "metadata": {
        "id": "_Tx0bOJBnilv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization:\n",
        "\n",
        "Adding batch normalization layers can help stabilize training by normalizing the outputs of the convolution layers."
      ],
      "metadata": {
        "id": "zCCboe0SnwpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import BatchNorm\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.batch_norm1 = BatchNorm(64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.batch_norm2 = BatchNorm(128)\n",
        "        self.conv3 = GCNConv(128, 128)\n",
        "        self.batch_norm3 = BatchNorm(128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.batch_norm1(self.conv1(x, edge_index)))\n",
        "        x = F.relu(self.batch_norm2(self.conv2(x, edge_index)))\n",
        "        x = F.relu(self.batch_norm3(self.conv3(x, edge_index)))\n",
        "        x = torch.mean(x, dim=0)\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "xA4qh7LQnsVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with Different Pooling Strategies:\n",
        "\n",
        "* Global mean pooling is one way to aggregate node features, but other pooling strategies like max pooling or sum pooling could work better for the data."
      ],
      "metadata": {
        "id": "O9g0ypMkn6qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.sum(x, dim=0)  # Global sum pooling\n"
      ],
      "metadata": {
        "id": "dF073O8An1O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated GNN Model with Sum Pooling:\n",
        "* Make sure the pooling operation is done within the `forward()` method of the model, where `x` and `edge_index` are properly defined.\n",
        "\n",
        "Here is an updated version of the GNN model that uses global sum pooling:"
      ],
      "metadata": {
        "id": "fXMOKYkRoVyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_add_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)  # Increased number of channels\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.conv3 = GCNConv(128, 128)  # Adding an additional GCN layer\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = global_add_pool(x, data.batch)  # Global sum pooling\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-zX6bl8koFlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation:\n",
        "* Global Pooling: The global_add_pool function from PyTorch Geometric takes care of the pooling operation by summing all the node features for each graph. This way, we don't need to manually define x outside of the forward method.\n",
        "* Batch Handling: Using data.batch allows for batch processing, which helps in dealing with multiple graphs at once during training.\n",
        "## Training the Updated Model:\n",
        "Make sure to use a DataLoader that can handle batch processing:"
      ],
      "metadata": {
        "id": "CdrHm_ypoo1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
        "\n",
        "# Updated training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNNModel().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "T9Gbbhizojl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AttributeError: `'NoneType' object has no attribute 'size'` error indicates that some of the `batch.y` values are `None`. This happens if the target value was not properly assigned when creating the graph objects or if there are invalid entries in the dataset.\n",
        "\n",
        "## How to Fix This:\n",
        "1. Check Target Assignment in Graph Objects:\n",
        "\n",
        "Ensure that each PyTorch Geometric `Data` object has a valid target value `(y)` when converting molecules into graph representations.\n",
        "2. Modify the Graph Conversion Function:\n",
        "\n",
        "Update the `mol_to_graph` function to include the target value only if it is not None and the molecule is valid.\n",
        "## Updated Graph Conversion Code:"
      ],
      "metadata": {
        "id": "ZlYg7sFqpGQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Function to convert RDKit Mol object to PyTorch Geometric Data object, with target y value\n",
        "def mol_to_graph(mol, target):\n",
        "    if mol is None or target is None:\n",
        "        return None  # Return None if mol or target is invalid\n",
        "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    edge_index = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index.append([start, end])\n",
        "        edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    # Add target y value to the Data object\n",
        "    y = torch.tensor([target], dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Convert training and testing data, adding target y value\n",
        "train_data['graph'] = train_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "test_data['graph'] = test_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "\n",
        "# Filter out None graphs (in case of failed conversions)\n",
        "train_graphs = [graph for graph in train_data['graph'] if graph is not None]\n",
        "test_graphs = [graph for graph in test_data['graph'] if graph is not None]\n"
      ],
      "metadata": {
        "id": "57ap5gn3ozzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop Update:\n",
        "Make sure the training loop **skips any invalid data** to avoid issues with missing target values.\n",
        "\n"
      ],
      "metadata": {
        "id": "Jy-p5k8lpvL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        # Skip batches that might have invalid data\n",
        "        if batch.y is None:\n",
        "            continue\n",
        "\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "P4n-QOMmprfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Outliers and Dataset Balancing\n",
        "\n",
        "To proceed effectively with our dataset, we need to address outliers and any class imbalance issues.\n",
        "\n",
        "1. **Handle Outliers**:\n",
        "   - We can observe that there are some extreme values in **Log IC50** (e.g., values greater than **10**). These values can negatively affect the model's ability to learn relationships.\n",
        "   - Therefore, we will filter out compounds with **Log IC50 values greater than 10** to remove these outliers and focus on the more relevant range.\n",
        "   - Filtering code:\n",
        "     ```python\n",
        "     df_filtered = df[df['Log IC50 (nM)'] <= 10]  # Filtering out extreme values\n",
        "     ```\n",
        "\n",
        "2. **Balance the Dataset**:\n",
        "   - From the class distribution, we observed that the dataset is slightly imbalanced, with **52 inactive compounds** and **43 active compounds**.\n",
        "   - To balance the dataset, we will use the **SMOTE** (Synthetic Minority Over-sampling Technique) method to generate synthetic examples for the minority class.\n",
        "   - This approach will ensure our model is trained with a more balanced dataset and thus be better able to learn the differences between active and inactive compounds.\n",
        "   - Balancing code:\n",
        "     ```python\n",
        "     !pip install imbalanced-learn\n",
        "     from imblearn.over_sampling import SMOTE\n",
        "\n",
        "     # Features and target extraction\n",
        "     if 'SMILES' in df_filtered.columns:\n",
        "         X = df_filtered[['Molecular Weight', 'LogP', 'HBA', 'HBD', 'Aromatic Rings', 'Log IC50 (nM)']]\n",
        "         y = df_filtered['Active']\n",
        "     else:\n",
        "         raise KeyError(\"SMILES column not found in df_filtered DataFrame.\")\n",
        "\n",
        "     # Oversample the minority class using SMOTE\n",
        "     smote = SMOTE(sampling_strategy='auto')\n",
        "     X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "     # Create a balanced DataFrame\n",
        "     df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Active'), df_filtered['SMILES'].reset_index(drop=True)], axis=1)\n",
        "     ```\n",
        "\n",
        "3. **Visualize Balanced Data**:\n",
        "   - After balancing, we will visualize the new class distribution to confirm that we have an equal number of **active** and **inactive** compounds.\n",
        "   - Visualization code:\n",
        "     ```python\n",
        "     print(df_balanced['Active'].value_counts())\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "ZqOlLkQDq-os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df[df['Log IC50 (nM)'] <= 10]  # Filtering out extreme values\n"
      ],
      "metadata": {
        "id": "f56QHirLp47I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Features and target extraction\n",
        "if 'SMILES' in df_filtered.columns:\n",
        "    X = df_filtered[['Molecular Weight', 'LogP', 'HBA', 'HBD', 'Aromatic Rings', 'Log IC50 (nM)']]\n",
        "    y = df_filtered['Active']\n",
        "else:\n",
        "    raise KeyError(\"SMILES column not found in df_filtered DataFrame.\")\n",
        "\n",
        "# Oversample the minority class using SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto')\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Create a balanced DataFrame\n",
        "df_balanced = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Active'), df_filtered['SMILES'].reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "hw1sHSjFrGEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_balanced['Active'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "I9ran7BfrOjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing for Graph Neural Network (GNN) Model Training\n",
        "\n",
        "1. **Convert SMILES to Molecular Graphs**:\n",
        "   - To train a GNN, we need to convert the **SMILES** representation of each compound into a molecular graph.\n",
        "   - We can use **RDKit** to convert SMILES into molecular graphs that are compatible with frameworks like **PyTorch Geometric**.\n",
        "   - Conversion code:\n",
        "     ```python\n",
        "     !pip install rdkit\n",
        "\n",
        "     from rdkit import Chem\n",
        "\n",
        "     # Convert SMILES to RDKit molecule objects\n",
        "     if 'SMILES' in df_balanced.columns:\n",
        "         df_balanced['Mol'] = df_balanced['SMILES'].apply(lambda x: Chem.MolFromSmiles(x) if pd.notna(x) else None)\n",
        "     else:\n",
        "         raise KeyError(\"SMILES column not found in df_balanced DataFrame.\")\n",
        "     ```\n",
        "\n",
        "2. **Model Training Preparation**:\n",
        "   - With the dataset filtered, balanced, and converted into molecular graphs, we can proceed to split the dataset into **training** and **testing** sets and prepare the data for **GNN model training**.\n",
        "   - Splitting code:\n",
        "     ```python\n",
        "     from sklearn.model_selection import train_test_split\n",
        "\n",
        "     # Split data into training and testing sets (80% training, 20% testing)\n",
        "     train_data, test_data = train_test_split(df_balanced, test_size=0.2, random_state=42)\n",
        "     ```\n",
        "\n",
        "3. **Create Graph Representations for GNN**:\n",
        "   - Convert the molecules in the training and testing sets into graph representations using **PyTorch Geometric**.\n",
        "   - Graph conversion code:\n",
        "     ```python\n",
        "     import torch\n",
        "     from torch_geometric.data import Data\n",
        "     from rdkit.Chem import rdmolops\n",
        "\n",
        "     # Function to convert RDKit Mol object to PyTorch Geometric Data object\n",
        "     def mol_to_graph(mol, target):\n",
        "         if mol is None or target is None:\n",
        "             return None  # Return None if mol or target is invalid\n",
        "         atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "         edge_index = []\n",
        "         for bond in mol.GetBonds():\n",
        "             start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "             edge_index.append([start, end])\n",
        "             edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "         edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "         x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "         # Add target y value to the Data object\n",
        "         y = torch.tensor([target], dtype=torch.float)\n",
        "\n",
        "         return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "     # Convert training and testing data, adding target y value\n",
        "     train_data['graph'] = train_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "     test_data['graph'] = test_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "\n",
        "     # Filter out None graphs (in case of failed conversions)\n",
        "     train_graphs = [graph for graph in train_data['graph'] if graph is not None]\n",
        "     test_graphs = [graph for graph in test_data['graph'] if graph is not None]\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "79XwH7KMrbI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n",
        "\n",
        "from rdkit import Chem\n",
        "\n",
        "# Convert SMILES to RDKit molecule objects\n",
        "if 'SMILES' in df_balanced.columns:\n",
        "    df_balanced['Mol'] = df_balanced['SMILES'].apply(lambda x: Chem.MolFromSmiles(x) if pd.notna(x) else None)\n",
        "else:\n",
        "    raise KeyError(\"SMILES column not found in df_balanced DataFrame.\")"
      ],
      "metadata": {
        "id": "lns7RPE9rUO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "train_data, test_data = train_test_split(df_balanced, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "K4aaAVBCriDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from rdkit.Chem import rdmolops\n",
        "\n",
        "# Function to convert RDKit Mol object to PyTorch Geometric Data object\n",
        "def mol_to_graph(mol, target):\n",
        "    if mol is None or target is None:\n",
        "        return None  # Return None if mol or target is invalid\n",
        "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    edge_index = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index.append([start, end])\n",
        "        edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    # Add target y value to the Data object\n",
        "    y = torch.tensor([target], dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Convert training and testing data, adding target y value\n",
        "train_data['graph'] = train_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "test_data['graph'] = test_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "\n",
        "# Filter out None graphs (in case of failed conversions)\n",
        "train_graphs = [graph for graph in train_data['graph'] if graph is not None]\n",
        "test_graphs = [graph for graph in test_data['graph'] if graph is not None]"
      ],
      "metadata": {
        "id": "GZBMobrxrnwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Split Data into Training and Testing Sets\n",
        "We will split the balanced dataset into training and testing sets. This allows us to train the model and evaluate its performance."
      ],
      "metadata": {
        "id": "PmWUlRn1r_7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "train_data, test_data = train_test_split(df_balanced, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "VouA6CPNru2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Molecules to Graph Representations for PyTorch Geometric\n",
        "We need to convert the molecules into graph representations that the GNN can use."
      ],
      "metadata": {
        "id": "9Te-gviNsL2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Function to convert RDKit Mol object to PyTorch Geometric Data object\n",
        "def mol_to_graph(mol, target):\n",
        "    if mol is None or target is None:\n",
        "        return None  # Return None if mol or target is invalid\n",
        "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "    edge_index = []\n",
        "    for bond in mol.GetBonds():\n",
        "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "        edge_index.append([start, end])\n",
        "        edge_index.append([end, start])  # Add both directions\n",
        "\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "    x = torch.tensor(atom_features, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    # Add target y value to the Data object\n",
        "    y = torch.tensor([target], dtype=torch.float)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Convert training and testing data, adding target y value\n",
        "train_data['graph'] = train_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "test_data['graph'] = test_data.apply(lambda row: mol_to_graph(row['Mol'], row['Log IC50 (nM)']), axis=1)\n",
        "\n",
        "# Filter out None graphs (in case of failed conversions)\n",
        "train_graphs = [graph for graph in train_data['graph'] if graph is not None]\n",
        "test_graphs = [graph for graph in test_data['graph'] if graph is not None]\n"
      ],
      "metadata": {
        "id": "Rwsleg_0sD51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a DataLoader for Training\n",
        "We will use a DataLoader to handle the batching for training the GNN."
      ],
      "metadata": {
        "id": "VV5ErduSshjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Create DataLoader objects for training and testing datasets\n",
        "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_graphs, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "ZArhtib1sHjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and Train the GNN Model\n",
        "Now, let's define a simple Graph Neural Network using PyTorch Geometric and train it."
      ],
      "metadata": {
        "id": "LLhY5It8sq1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import nn, optim\n",
        "\n",
        "# Define the GNN model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)  # Increased number of channels\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = torch.mean(x, dim=0)  # Global mean pooling\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNNModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "dxpv-Xkbsmwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The warning message we are receiving is due to a mismatch in the dimensions of the output tensor from the model and the target tensor (y). Specifically, the model is outputting a tensor of size [1], while the target size is [batch_size].\n",
        "\n",
        "To fix this, we need to make sure that the output from our model matches the size of our target tensor (y). Here are a couple of modifications to make:\n",
        "\n",
        "## Update the GNN Model to Output the Correct Dimension\n",
        "Global Pooling Per Graph: Use global pooling like global_mean_pool to handle batch-wise pooling, and modify the model to output the correct dimensions per batch.\n"
      ],
      "metadata": {
        "id": "luEKwMN9s_8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "# Updated GNN model\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)  # Global mean pooling per graph in the batch\n",
        "        x = self.fc1(x)\n",
        "        return x.view(-1)  # Output size should match batch size\n"
      ],
      "metadata": {
        "id": "VvcNA459swhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated Training Loop\n",
        "Ensure your training loop remains the same since the output dimensions now match the target dimensions."
      ],
      "metadata": {
        "id": "6eRD6AvutTGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "q4fVevQqtPKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss is reducing somewhat, but the values are fluctuating quite a bit, which may indicate that the model is not converging well. Here are a few suggestions for further improving your model training:\n",
        "\n",
        "## 1. Learning Rate Tuning\n",
        "\n",
        "The learning rate might be too high, which can lead to oscillations. Consider reducing the learning rate to stabilize the training process. You can try values like 0.0001 or use a learning rate scheduler."
      ],
      "metadata": {
        "id": "hPJGI_YTtlCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a lower learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "BNqVOSSatXh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Learning Rate Scheduler\n",
        "Implement a learning rate scheduler to reduce the learning rate gradually if the loss plateaus."
      ],
      "metadata": {
        "id": "ZLXqRDyit0U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Add a learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Update the training loop to include the scheduler\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "    scheduler.step(total_loss)\n"
      ],
      "metadata": {
        "id": "nFTW-GSKtv2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Batch Normalization\n",
        "Add batch normalization layers after each graph convolutional layer to help stabilize training and potentially improve convergence."
      ],
      "metadata": {
        "id": "5hcTluQouHV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import BatchNorm\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = global_mean_pool(x, batch)  # Global mean pooling per graph in the batch\n",
        "        x = self.fc1(x)\n",
        "        return x.view(-1)  # Output size should match batch size\n"
      ],
      "metadata": {
        "id": "3CMqHtLVt5p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Gradient Clipping\n",
        "Use gradient clipping to prevent exploding gradients, which can help stabilize training if there are sudden spikes in the loss."
      ],
      "metadata": {
        "id": "egH87tlduQM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with gradient clipping\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "e4FJBnqyuMTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss values seem to be fluctuating quite a bit, and the model does not appear to be consistently reducing the loss or converging to a satisfactory level. Here are some further suggestions to improve training stability and reduce fluctuations:\n",
        "\n",
        "## 1. Try a Lower Learning Rate\n",
        "While you've already adjusted the learning rate, consider lowering it even further. A smaller learning rate (e.g., 0.00005) might stabilize the training further.\n",
        "\n"
      ],
      "metadata": {
        "id": "SbvRJ6tluoKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n"
      ],
      "metadata": {
        "id": "l7fh84CNuVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Use Weight Decay (L2 Regularization)\n",
        "Adding weight decay to the optimizer can help prevent overfitting by penalizing large weights, which may lead to smoother convergence."
      ],
      "metadata": {
        "id": "zSmm-ERduwrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-5)\n"
      ],
      "metadata": {
        "id": "bcnmyPBcutFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Increase the Batch Size\n",
        "A larger batch size can help reduce the variance in the gradient estimates, leading to more stable training. Try increasing the batch size to 32 or 64, if the A100 on Google Colab GPU allows it."
      ],
      "metadata": {
        "id": "5bWK7ypzu5nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Mmm1n5eku1xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Use Dropout Regularization\n",
        "To avoid overfitting, add dropout layers after the graph convolutional layers. Dropout helps in regularizing the network and makes it less likely to memorize the training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "VU3mEfahvIVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "        x = global_mean_pool(x, batch)  # Global mean pooling per graph in the batch\n",
        "        x = self.fc1(x)\n",
        "        return x.view(-1)  # Output size should match batch size\n"
      ],
      "metadata": {
        "id": "wwsEGiNOvERc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Check Gradient Clipping\n",
        "If your gradients are occasionally exploding, gradient clipping can help by limiting the maximum value of gradients during training. Keep using it as it's beneficial."
      ],
      "metadata": {
        "id": "T6q9lkR2vRzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
      ],
      "metadata": {
        "id": "Rkr5bFAevNdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Switch to a Different Optimizer\n",
        "If Adam is not working well, you can try RMSprop or SGD with a momentum term. These optimizers might help achieve more stable training in certain scenarios."
      ],
      "metadata": {
        "id": "AtGoPz69vbtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to RMSprop\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.00005)\n"
      ],
      "metadata": {
        "id": "jYfYbIg6vXG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Implement Early Stopping\n",
        "If the validation loss starts to increase for several epochs, it indicates overfitting. Implement early stopping to stop training if no improvements are seen after a set number of epochs."
      ],
      "metadata": {
        "id": "VIKdE9Jgvp3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo-code for early stopping\n",
        "best_loss = float('inf')\n",
        "patience = 10\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if total_loss < best_loss:\n",
        "        best_loss = total_loss\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "T3eWKwtkvmWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It looks like the loss is decreasing more consistently now, which suggests that the adjustments are working well, and the model is converging more effectively. The early stopping was also appropriately triggered, which prevents overfitting and helps save time.\n",
        "\n"
      ],
      "metadata": {
        "id": "_41cgr8iwCys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model:\n",
        "\n",
        "* Now that the model is trained, it's time to evaluate its performance on the test dataset.\n",
        "* Calculate metrics such as Mean Squared Error **(MSE)**, Mean Absolute Error **(MAE)**, and **R²** score to understand how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "2l8Th6PrwKM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "        true_values.extend(batch.y.cpu().numpy())\n",
        "        predicted_values.extend(output.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(true_values, predicted_values)\n",
        "mae = mean_absolute_error(true_values, predicted_values)\n",
        "r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "MlVMD-NPvyGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error indicates that the true_values and predicted_values lists have different lengths, which prevents you from calculating the evaluation metrics.\n",
        "\n",
        "This issue might be because the model's output size does not match the batch size during evaluation, which can happen if there is only one value being returned instead of a full batch of predictions.\n",
        "\n",
        "Here's how you can fix it:\n",
        "\n",
        "## Debugging the Mismatch Issue\n",
        "1. Check Batch Size in the Model Output: The model might be outputting only one value for the entire batch instead of separate predictions for each sample. Update the model to ensure that it outputs the correct number of values.\n",
        "\n",
        "Make sure that the model outputs a prediction for each graph in the batch:\n",
        "\n"
      ],
      "metadata": {
        "id": "7X_DMH6-wu5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "        x = global_mean_pool(x, batch)  # Global mean pooling per graph in the batch\n",
        "        x = self.fc1(x)\n",
        "        return x  # Output shape should be [batch_size, 1]\n"
      ],
      "metadata": {
        "id": "jTx4tom-wgl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Evaluation Code to Flatten Predictions: During evaluation, ensure that the output and true values are reshaped properly to match the expected dimensions."
      ],
      "metadata": {
        "id": "w5X__tz9w9X0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "        true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "        predicted_values.extend(output.cpu().numpy().flatten())\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(true_values, predicted_values)\n",
        "mae = mean_absolute_error(true_values, predicted_values)\n",
        "r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "eXVRi5dJw4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error persists because there is still a mismatch between the number of true values and the predicted values. It seems like the model is outputting one value per batch instead of individual values for each graph in the batch. Here are some updates to ensure everything matches correctly:\n",
        "\n",
        "## Debugging Steps\n",
        "1. Ensure Correct Global Pooling: Double-check the pooling operation to ensure it's pooling each graph within the batch separately, rather than the entire batch into a single output.\n",
        "\n",
        "2. Modify Output to Handle Batched Graphs Properly: Instead of using global_mean_pool incorrectly, which could potentially collapse multiple graphs into a single output, make sure that the model outputs predictions for each graph in the batch.\n",
        "\n",
        "## Model and Evaluation Adjustments:\n",
        "Adjusted Model Code"
      ],
      "metadata": {
        "id": "MFUaJysAxNMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "        x = global_mean_pool(x, batch)  # Pool each graph in the batch individually\n",
        "        x = self.fc1(x)\n",
        "        return x.view(-1)  # Ensure output has shape [batch_size]\n"
      ],
      "metadata": {
        "id": "3lgDtqNIxB1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated Evaluation Code\n",
        "We need to ensure that both true_values and predicted_values are aligned properly. Make sure the model predicts an output for each graph, and that you collect the true and predicted values in a way that matches."
      ],
      "metadata": {
        "id": "8g1TYbnzxgZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)  # Output should have shape [batch_size]\n",
        "        true_values.extend(batch.y.cpu().numpy().flatten())  # True values for all graphs in batch\n",
        "        predicted_values.extend(output.cpu().numpy().flatten())  # Predicted values for all graphs in batch\n",
        "\n",
        "# Ensure consistent lengths of true and predicted values\n",
        "assert len(true_values) == len(predicted_values), \"Mismatch between true and predicted values length.\"\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(true_values, predicted_values)\n",
        "mae = mean_absolute_error(true_values, predicted_values)\n",
        "r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "qIv6zQdexbd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `AssertionError` indicates that there is still a mismatch in the number of true values and predicted values. This might be due to:\n",
        "\n",
        "Empty or Missing Predictions: Some batches might not have valid predictions, which could be caused by failed conversions or filtering during data preparation.\n",
        "Batches with Uneven Sizes: There may be an issue where some batches are smaller or dropped during training, leading to inconsistencies.\n",
        "## Steps to Debug and Fix\n",
        "1. Add Debugging to Check Batch Sizes\n",
        "Add a print statement in the evaluation loop to verify the size of `batch.y` and `output`."
      ],
      "metadata": {
        "id": "MKRLr5saxz2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"Batch size (true values): {batch.y.shape[0]}, Output size: {output.shape[0]}\")\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match.\")\n"
      ],
      "metadata": {
        "id": "p1wt-KQMxmaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue we're encountering indicates that all of the batches are being skipped due to a size mismatch, which results in no valid data being available for evaluation (`true_values` and `predicted_values` are both empty).\n",
        "\n",
        "## Root Cause:\n",
        "The model is outputting a single value instead of a value for each graph in the batch. This typically happens when the pooling operation (e.g., `global_mean_pool`) is not applied correctly, causing the entire batch to be reduced to a single output rather than producing one output per graph.\n",
        "\n",
        "## Solution:\n",
        "1. Ensure Correct Batch-wise Output:\n",
        "* The pooling operation (global_mean_pool) should be applied to each graph in the batch individually, and the final linear layer should output a prediction for each graph.\n",
        "2. Revisit the Model's Forward Pass:\n",
        "* Ensure that the model outputs a tensor of shape [batch_size, 1] rather than [1]. The issue is likely due to incorrectly pooling over the entire batch, rather than each graph.\n",
        "## Model Update:\n",
        "Here's an updated version of the model to ensure that each graph gets an individual output:"
      ],
      "metadata": {
        "id": "eZknMPEByVsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "        x = global_mean_pool(x, batch)  # Apply global mean pooling for each graph in the batch\n",
        "        x = self.fc1(x)\n",
        "        return x.view(-1)  # Make sure the output has the correct shape [batch_size]\n"
      ],
      "metadata": {
        "id": "r4qTjHjSyD2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Changes:\n",
        "* Global Mean Pooling: Make sure `global_mean_pool` is used correctly to create a feature vector for each graph in the batch, not the entire batch.\n",
        "* Return Output with Correct Shape: Use `.view(-1)` to ensure that the output tensor has a shape of `[batch_size]`.\n",
        "\n",
        "## Re-run the Evaluation:\n",
        "After updating the model, try the evaluation code again:"
      ],
      "metadata": {
        "id": "bNSVyHGrzFit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "U_WYCwxHy_be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still getting \"Skipping batch due to size mismatch,\" it indicates that all batches are producing a size mismatch between `true_values` and `predicted_values`. This suggests that the model's output size is not matching what is expected, potentially due to improper handling of the batch during forward propagation or a misunderstanding in the shape of the output after pooling.\n",
        "\n",
        "To address this, let’s take a step-by-step approach to debug and fix it:\n",
        "\n",
        "## Step 1: Check Data Batching and Output Sizes\n",
        "* Verify the Size of Batch Components: Print the sizes of the input features and labels (`data.x`, `data.y`, and `data.batch`) at different stages of the forward pass.\n",
        "* Ensure Proper Pooling: The pooling function (`global_mean_pool`) should produce a feature vector for each graph in the batch.\n",
        "## Step 2: Print Intermediate Tensor Sizes in the Model\n",
        "Add print statements inside the model to understand the shapes of the intermediate tensors:\n",
        "\n"
      ],
      "metadata": {
        "id": "somh1W1az0af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        print(f\"Input x shape: {x.shape}\")  # Print input feature size\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        print(f\"Shape after conv1 and dropout: {x.shape}\")\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        print(f\"Shape after conv2 and dropout: {x.shape}\")\n",
        "        x = global_mean_pool(x, batch)  # Global mean pooling per graph in the batch\n",
        "\n",
        "        print(f\"Shape after global mean pool: {x.shape}\")\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        print(f\"Output shape before reshape: {x.shape}\")\n",
        "        return x.view(-1)  # Make sure the output has the correct shape [batch_size]\n"
      ],
      "metadata": {
        "id": "abFshmbOziOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Debug the Data Loader and Model Output\n",
        "When running the evaluation loop, add similar print statements to see if the output matches the expected shape:"
      ],
      "metadata": {
        "id": "COsXd31B0Z_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "\n",
        "        # Print the shapes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "ojZoaWu10U5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statements show that the model is outputting a single value (`torch.Size([1]`)) for the entire batch, rather than a value for each graph in the batch (`torch.Size([19]`)). This means the `global_mean_pool` function is aggregating the entire batch into one output rather than keeping the predictions for individual graphs.\n",
        "\n",
        "## Fixing the Pooling and Output Issue\n",
        "We need to ensure that each graph in the batch gets its own output, which means `global_mean_pool` should be applied to each graph separately.\n",
        "\n",
        "Here’s how we can fix the model and make it output predictions for each graph in the batch:\n",
        "\n",
        "## Corrected Model\n",
        "1. Apply Proper Pooling for Batched Graphs: The pooling function should create separate feature vectors for each graph in the batch.\n",
        "2. Ensure Output Matches Batch Size: Modify the model so that the output dimension matches the number of graphs in each batch."
      ],
      "metadata": {
        "id": "8jKKA0Un0smo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Pool the graph-level embeddings\n",
        "        x = global_mean_pool(x, batch)  # Pool each graph in the batch\n",
        "\n",
        "        # Final fully connected layer to output the predicted value for each graph\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return x.view(-1)  # Ensure output shape is [batch_size]\n"
      ],
      "metadata": {
        "id": "85Au9c-G0f8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "FkWcNJwu1iXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue persists, meaning that the model is still outputting a single value instead of multiple values for each graph in the batch. This suggests that the pooling or linear layer is incorrectly aggregating the entire batch. Let’s address this step-by-step:\n",
        "\n",
        "## Step-by-Step Debugging and Fixing:\n",
        "Understand the Data Flow:\n",
        "\n",
        "The input to the model (data) represents multiple graphs.\n",
        "After applying convolution layers, the pooling operation (global_mean_pool) should aggregate node features per graph in the batch to create a feature vector for each graph.\n",
        "Batch Processing and Proper Pooling:\n",
        "\n",
        "global_mean_pool(x, batch) should aggregate nodes that belong to the same graph into a single feature vector for each graph, resulting in a tensor of size [batch_size, hidden_dim].\n"
      ],
      "metadata": {
        "id": "oNV3pI4G1ybZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        print(f\"Shape after first GCN layer: {x.shape}\")  # Expected: [num_nodes, 64]\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        print(f\"Shape after second GCN layer: {x.shape}\")  # Expected: [num_nodes, 128]\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Pool node features per graph\n",
        "\n",
        "        print(f\"Shape after global mean pooling: {x.shape}\")  # Expected: [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to produce final output for each graph\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        print(f\"Output shape after fc1: {x.shape}\")  # Expected: [batch_size, 1]\n",
        "\n",
        "        return x.view(-1)  # Ensure output shape is [batch_size]\n",
        "\n"
      ],
      "metadata": {
        "id": "qEskHDSh1jbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "boDbQLNK3B0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        print(f\"Shape after first GCN layer: {x.shape}\")  # Expected: [num_nodes, 64]\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        print(f\"Shape after second GCN layer: {x.shape}\")  # Expected: [num_nodes, 128]\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Pool node features per graph\n",
        "\n",
        "        print(f\"Shape after global mean pooling: {x.shape}\")  # Expected: [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to produce final output for each graph\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        print(f\"Output shape after fc1: {x.shape}\")  # Expected: [batch_size, 1]\n",
        "\n",
        "        return x  # Ensure output shape is [batch_size, 1]\n",
        "\n"
      ],
      "metadata": {
        "id": "c50HiUhC3GSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)  # Output shape should be [batch_size, 1]\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")  # Expected: [batch_size]\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")  # Expected: [batch_size, 1]\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "Cv7AoCVk3PLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        print(f\"Shape after first GCN layer: {x.shape}\")  # Expected: [num_nodes, 64]\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        print(f\"Shape after second GCN layer: {x.shape}\")  # Expected: [num_nodes, 128]\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Pool node features per graph\n",
        "\n",
        "        print(f\"Shape after global mean pooling: {x.shape}\")  # Expected: [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to produce final output for each graph\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        print(f\"Output shape after fc1: {x.shape}\")  # Expected: [batch_size, 1]\n",
        "\n",
        "        return x.squeeze(-1)  # Ensure output shape is [batch_size]\n",
        "\n"
      ],
      "metadata": {
        "id": "5Gg1YjVU3S-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")  # Expected: [batch_size]\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")  # Expected: [batch_size]\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "9Bxcvshx3a8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Debug: Print batch information\n",
        "        print(f\"Batch tensor content: {batch}\")  # Expect values from 0 to batch_size - 1\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        print(f\"Shape after first GCN layer: {x.shape}\")  # Expected: [num_nodes, 64]\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        print(f\"Shape after second GCN layer: {x.shape}\")  # Expected: [num_nodes, 128]\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Pool node features per graph\n",
        "\n",
        "        print(f\"Shape after global mean pooling: {x.shape}\")  # Expected: [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to produce final output for each graph\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        print(f\"Output shape after fc1: {x.shape}\")  # Expected: [batch_size, 1]\n",
        "\n",
        "        return x.squeeze(-1)  # Ensure output shape is [batch_size]\n",
        "\n"
      ],
      "metadata": {
        "id": "kup7M5pL3eX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Debugging: Print sizes of batch.y and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")  # Expected: [batch_size]\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")  # Expected: [batch_size]\n",
        "\n",
        "        # Ensure both batch.y and output are valid before appending\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ],
      "metadata": {
        "id": "iJmnOdjL3p7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Batch tensor unique values: {batch.unique()}\")  # Should contain multiple unique graph IDs\n"
      ],
      "metadata": {
        "id": "WOQ_jlC23tLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Batch tensor unique values: {batch.unique()}\")  # Should contain multiple unique graph IDs\n"
      ],
      "metadata": {
        "id": "9wzRhgON32ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Batch tensor unique values: {batch.unique().tolist()}\")  # Should contain multiple unique graph IDs\n"
      ],
      "metadata": {
        "id": "pNY5_uvo3_hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using batch.unique(), let's print the raw batch tensor\n",
        "print(f\"Batch tensor: {batch}\")  # Expect values from 0 to batch_size - 1\n",
        "\n",
        "# Inspect the type and details\n",
        "print(f\"Batch type: {type(batch)}\")  # Should be a torch.Tensor\n",
        "print(f\"Batch shape: {batch.shape}\")  # Should provide the number of nodes, each assigned to a graph index\n"
      ],
      "metadata": {
        "id": "87vewmBx4HfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print batch details\n",
        "print(f\"Batch tensor: {batch}\")  # Print general batch details\n",
        "\n",
        "# Access and print specific attributes of the batch\n",
        "print(f\"Batch attribute values: {batch.batch}\")  # This gives node-level mapping to graphs\n",
        "print(f\"Batch type: {type(batch.batch)}\")  # Type of the batch attribute (should be torch.Tensor)\n",
        "print(f\"Batch shape: {batch.batch.shape}\")  # Should give the number of nodes\n",
        "\n",
        "# Extract unique graph IDs in the batch\n",
        "unique_values = torch.unique(batch.batch)\n",
        "print(f\"Unique graph IDs in batch tensor: {unique_values.tolist()}\")  # List of unique graph IDs\n"
      ],
      "metadata": {
        "id": "plRHTkWV4POz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Apply global mean pooling\n",
        "        x = global_mean_pool(x, batch)  # Shape after pooling: [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to produce final output for each graph\n",
        "        x = self.fc1(x)  # Shape after fc1: [batch_size, 1]\n",
        "\n",
        "        # Return output with shape [batch_size] by squeezing the last dimension\n",
        "        return x.squeeze(-1)  # Ensure output shape is [batch_size]\n",
        "\n"
      ],
      "metadata": {
        "id": "vERiSVLG4XBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)  # Output shape: [batch_size]\n",
        "\n",
        "        # Debugging: Print shapes of true values and output\n",
        "        print(f\"True values shape (batch.y): {batch.y.shape}\")  # Expected: [batch_size]\n",
        "        print(f\"Predicted values shape (output): {output.shape}\")  # Expected: [batch_size]\n",
        "\n",
        "        # Ensure shapes match before computing loss\n",
        "        if output.shape == batch.y.shape:\n",
        "            loss = criterion(output, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nbEG1pYd4jbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Corrected output to ensure one value per graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Apply global mean pooling to get a single representation for each graph\n",
        "        x = global_mean_pool(x, batch)  # Shape should be [batch_size, 128]\n",
        "\n",
        "        # Apply fully connected layer to get a scalar value for each graph\n",
        "        x = self.fc1(x)  # Shape should be [batch_size, 1]\n",
        "\n",
        "        # Squeeze the last dimension to match target shape\n",
        "        return x.squeeze(-1)  # Final shape should be [batch_size]\n"
      ],
      "metadata": {
        "id": "qfmenyrf4mxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Check if the output matches the target shape\n",
        "        if output.shape == batch.y.shape:\n",
        "            loss = criterion(output, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch due to size mismatch. Output shape: {output.shape}, True values shape: {batch.y.shape}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "DrX64LnT452I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Corrected output size to ensure one value per graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply GCN layers\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Apply global mean pooling to get a single representation for each graph\n",
        "        x = global_mean_pool(x, batch)  # Shape should be [batch_size, 128]\n",
        "\n",
        "        # Apply the fully connected layer to get a scalar value for each graph\n",
        "        x = self.fc1(x)  # Shape should be [batch_size, 1]\n",
        "\n",
        "        # Squeeze the last dimension to match target shape\n",
        "        return x.squeeze(-1)  # Final shape should be [batch_size]\n"
      ],
      "metadata": {
        "id": "J7PmBU0V49Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Check if the output matches the target shape\n",
        "        if output.shape == batch.y.shape:\n",
        "            loss = criterion(output, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch due to size mismatch. Output shape: {output.shape}, True values shape: {batch.y.shape}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZxkkPMOR5Jms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply GCN layers\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "\n",
        "        # Global mean pooling to get graph-level representation\n",
        "        x = global_mean_pool(x, batch)  # Shape: [num_graphs, 128]\n",
        "\n",
        "        # Fully connected layer to predict value for each graph\n",
        "        x = self.fc1(x)  # Shape: [num_graphs, 1]\n",
        "\n",
        "        return x.squeeze()  # Final shape: [num_graphs]\n"
      ],
      "metadata": {
        "id": "ZVUnrSgS5Nkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Check if the output matches the target shape\n",
        "        if output.shape == batch.y.shape:\n",
        "            loss = criterion(output, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch due to size mismatch. Output shape: {output.shape}, True values shape: {batch.y.shape}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "u1YWZQqX5ZDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.fc1 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply GCN layers\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "\n",
        "        # Global mean pooling to get graph-level representation\n",
        "        x = global_mean_pool(x, batch)  # Shape: [num_graphs, 128]\n",
        "\n",
        "        # Fully connected layer to predict value for each graph\n",
        "        x = self.fc1(x)  # Shape: [num_graphs, 1]\n",
        "\n",
        "        return x.view(-1)  # Shape: [num_graphs]\n"
      ],
      "metadata": {
        "id": "PjWTaESB5cMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)  # Output shape should be [batch_size]\n",
        "\n",
        "        # Make sure the output matches the target shape\n",
        "        if output.shape == batch.y.shape:\n",
        "            loss = criterion(output, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            print(f\"Skipping batch due to size mismatch. Output shape: {output.shape}, True values shape: {batch.y.shape}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "3drUobDZ5qbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example to ensure the output matches the batch size:\n",
        "def forward(self, x):\n",
        "    # Assuming x is of shape [batch_size, input_features]\n",
        "    # The output should be of shape [batch_size, output_features]\n",
        "    x = self.layer(x)  # Ensure this layer produces output for each sample\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "GzfopVvz5tww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inside the training loop\n",
        "for data in train_loader:\n",
        "    # Assuming data is a tuple of (inputs, labels)\n",
        "    inputs, labels = data\n",
        "    print(\"Input shape:\", inputs.shape)\n",
        "    print(\"Label shape:\", labels.shape)\n",
        "    outputs = model(inputs)\n",
        "    print(\"Output shape:\", outputs.shape)\n",
        "\n",
        "    if outputs.shape == labels.shape:\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Proceed with backpropagation\n",
        "    else:\n",
        "        print(f\"Skipping batch due to size mismatch: output shape {outputs.shape}, label shape {labels.shape}\")\n"
      ],
      "metadata": {
        "id": "uoFxOwA459DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_loader:\n",
        "    print(\"Data:\", data)\n",
        "    break  # Just to see one example\n"
      ],
      "metadata": {
        "id": "HD_fVoGg6Abb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels, additional_info = data\n"
      ],
      "metadata": {
        "id": "swuDJS_E6LQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = data['inputs']\n",
        "labels = data['labels']\n"
      ],
      "metadata": {
        "id": "8cbv5hYy6QXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = data.x  # Node features\n",
        "labels = data.y  # Labels\n",
        "\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "print(\"Label shape:\", labels.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Fxf1Tf2y6XCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_loader:\n",
        "    # Extract inputs and labels from the DataBatch object\n",
        "    inputs = data.x           # Node features (inputs)\n",
        "    labels = data.y           # Labels\n",
        "    edge_index = data.edge_index  # Graph connectivity, if needed\n",
        "\n",
        "    # Print the shapes to verify\n",
        "    print(\"Input shape:\", inputs.shape)\n",
        "    print(\"Label shape:\", labels.shape)\n",
        "    print(\"Edge index shape:\", edge_index.shape)\n",
        "\n",
        "    # Your model training code here\n",
        "    outputs = model(inputs, edge_index)  # You may need to pass edge_index if it's required by your model\n",
        "\n",
        "    # Assuming you have a loss function\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "46h__6lw6flB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        # Example layers\n",
        "        self.conv1 = pyg_nn.GCNConv(1, 16)  # Change to your desired configuration\n",
        "        self.conv2 = pyg_nn.GCNConv(16, 32)\n",
        "        self.fc = torch.nn.Linear(32, 1)    # Output layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = pyg_nn.global_mean_pool(x, batch=None)  # Aggregate the graph embeddings if needed\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "4cj4wyiC9mrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOGmaJXxG9Jc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}