{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christophergaughan/Bioinformatics-Code/blob/main/GNN_Antibiotics_with_query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drhQDL4aiwgh",
        "outputId": "87897158-bdb6-4d2b-c1f3-467ec77fecd8"
      },
      "id": "drhQDL4aiwgh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYP7YQREiJs4",
        "outputId": "4c4e4586-ab21-457c-b800-d635a3bfc51e"
      },
      "id": "UYP7YQREiJs4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall any existing versions to prevent conflicts\n",
        "!pip uninstall -y torch torch-geometric torch-scatter torch-sparse torchaudio torchvision\n",
        "\n",
        "# Install a compatible version of PyTorch\n",
        "!pip install torch==2.0.0\n",
        "\n",
        "# Install Torch Geometric dependencies for PyTorch 2.0.0 with CUDA support\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Reinstall torchaudio and torchvision compatible with PyTorch 2.0.0\n",
        "!pip install torchaudio==2.0.1\n",
        "!pip install torchvision==0.15.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY46yQF-mSNg",
        "outputId": "2a88bcaa-4447-4af5-8651-fbce86766af9"
      },
      "id": "bY46yQF-mSNg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.0\n",
            "Uninstalling torch-2.0.0:\n",
            "  Successfully uninstalled torch-2.0.0\n",
            "Found existing installation: torch-geometric 2.6.1\n",
            "Uninstalling torch-geometric-2.6.1:\n",
            "  Successfully uninstalled torch-geometric-2.6.1\n",
            "Found existing installation: torch-scatter 2.1.2+pt20cu118\n",
            "Uninstalling torch-scatter-2.1.2+pt20cu118:\n",
            "  Successfully uninstalled torch-scatter-2.1.2+pt20cu118\n",
            "Found existing installation: torch-sparse 0.6.18+pt20cu118\n",
            "Uninstalling torch-sparse-0.6.18+pt20cu118:\n",
            "  Successfully uninstalled torch-sparse-0.6.18+pt20cu118\n",
            "Found existing installation: torchaudio 2.4.1+cu121\n",
            "Uninstalling torchaudio-2.4.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.4.1+cu121\n",
            "Found existing installation: torchvision 0.19.1+cu121\n",
            "Uninstalling torchvision-0.19.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.19.1+cu121\n",
            "Collecting torch==2.0.0\n",
            "  Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Using cached torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.17 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting torchaudio==2.0.1\n",
            "  Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchaudio==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchaudio==2.0.1) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchaudio==2.0.1) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchaudio==2.0.1) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchaudio==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchaudio==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchaudio==2.0.1) (1.3.0)\n",
            "Downloading torchaudio-2.0.1-cp310-cp310-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchaudio\n",
            "Successfully installed torchaudio-2.0.1\n",
            "Collecting torchvision==0.15.2\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchvision==0.15.2)\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (71.0.4)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision==0.15.2) (0.44.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision==0.15.2) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision==0.15.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision==0.15.2) (1.3.0)\n",
            "Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.1 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1 torchvision-0.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall any existing versions to prevent conflicts\n",
        "!pip uninstall -y torch torch-geometric torch-scatter torch-sparse torchaudio torchvision\n",
        "\n",
        "# Install PyTorch with CUDA 11.8 support (which should be compatible with A100)\n",
        "!pip install torch==2.0.0+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install PyTorch Geometric dependencies for PyTorch 2.0.0 with CUDA 11.8 support\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6bSsWGOmyWu",
        "outputId": "cf5d754e-b34c-4182-fead-050ceabeee9f"
      },
      "id": "Y6bSsWGOmyWu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1\n",
            "Uninstalling torch-2.0.1:\n",
            "  Successfully uninstalled torch-2.0.1\n",
            "Found existing installation: torch-geometric 2.6.1\n",
            "Uninstalling torch-geometric-2.6.1:\n",
            "  Successfully uninstalled torch-geometric-2.6.1\n",
            "Found existing installation: torch-scatter 2.1.2+pt20cu118\n",
            "Uninstalling torch-scatter-2.1.2+pt20cu118:\n",
            "  Successfully uninstalled torch-scatter-2.1.2+pt20cu118\n",
            "Found existing installation: torch-sparse 0.6.18+pt20cu118\n",
            "Uninstalling torch-sparse-0.6.18+pt20cu118:\n",
            "  Successfully uninstalled torch-sparse-0.6.18+pt20cu118\n",
            "Found existing installation: torchaudio 2.0.1\n",
            "Uninstalling torchaudio-2.0.1:\n",
            "  Successfully uninstalled torchaudio-2.0.1\n",
            "Found existing installation: torchvision 0.15.2\n",
            "Uninstalling torchvision-0.15.2:\n",
            "  Successfully uninstalled torchvision-0.15.2\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.0.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m940.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.1.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install torch==2.0.0+cu118 and torchvision==0.15.2+cu118 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested torch==2.0.0+cu118\n",
            "    torchvision 0.15.2+cu118 depends on torch==2.0.1\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "Installing collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Using cached https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.8)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRcseFu9nivD",
        "outputId": "40452981-1413-412c-bb8a-4d3a5829a41b"
      },
      "id": "tRcseFu9nivD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.0.0+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.0%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (3.1.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0+cu118) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0+cu118) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0+cu118) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.17 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndV92KmIn5Kt",
        "outputId": "9e6bd48b-38b6-4c8e-fe45-67fe7ee9b41d"
      },
      "id": "ndV92KmIn5Kt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torchvision==0.15.2+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchvision==0.15.2+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/rocm5.4.2/torch-2.0.1%2Brocm5.4.2-cp310-cp310-linux_x86_64.whl (1536.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 GB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.1.4)\n",
            "Collecting pytorch-triton-rocm<2.1,>=2.0.0 (from torch==2.0.1->torchvision==0.15.2+cu118)\n",
            "  Downloading pytorch_triton_rocm-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch==2.0.1->torchvision==0.15.2+cu118) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch==2.0.1->torchvision==0.15.2+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision==0.15.2+cu118) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision==0.15.2+cu118) (1.3.0)\n",
            "Downloading pytorch_triton_rocm-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.4 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/167.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:49\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 466, in read\n",
            "    s = self.fp.read(amt)\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1303, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1159, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 179, in resolve\n",
            "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 554, in prepare_linked_requirements_more\n",
            "    self._complete_partial_requirements(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 469, in _complete_partial_requirements\n",
            "    for link, (filepath, _) in batch_download:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 184, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision==0.15.2+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs08Yf_Eo3nb",
        "outputId": "9bc94def-be35-42f4-c028-35501a121d87"
      },
      "id": "Qs08Yf_Eo3nb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torchvision==0.15.2+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Collecting torch==2.0.1 (from torchvision==0.15.2+cu118)\n",
            "  Using cached https://download.pytorch.org/whl/rocm5.4.2/torch-2.0.1%2Brocm5.4.2-cp310-cp310-linux_x86_64.whl (1536.4 MB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2+cu118) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision==0.15.2+cu118) (3.1.4)\n",
            "Collecting pytorch-triton-rocm<2.1,>=2.0.0 (from torch==2.0.1->torchvision==0.15.2+cu118)\n",
            "  Using cached pytorch_triton_rocm-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch==2.0.1->torchvision==0.15.2+cu118) (3.30.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from pytorch-triton-rocm<2.1,>=2.0.0->torch==2.0.1->torchvision==0.15.2+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision==0.15.2+cu118) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision==0.15.2+cu118) (1.3.0)\n",
            "Downloading pytorch_triton_rocm-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.4/167.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-triton-rocm, torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "Successfully installed pytorch-triton-rocm-2.0.2 torch-2.0.1+rocm5.4.2 torchvision-0.15.2+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "from torch_geometric.data import DataLoader, Data\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLzbAO1ypVJa",
        "outputId": "7fc429da-bb40-4d73-efd6-40eb0d67feb5"
      },
      "id": "MLzbAO1ypVJa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chembl_webresource_client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNxWQ8udrIuX",
        "outputId": "bbe2478a-b791-4da2-b764-3bf0178de145"
      },
      "id": "xNxWQ8udrIuX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chembl_webresource_client\n",
            "  Downloading chembl_webresource_client-0.10.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from chembl_webresource_client) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from chembl_webresource_client) (2.32.3)\n",
            "Collecting requests-cache~=1.2 (from chembl_webresource_client)\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from chembl_webresource_client) (1.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->chembl_webresource_client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->chembl_webresource_client) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->chembl_webresource_client) (2024.8.30)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache~=1.2->chembl_webresource_client) (24.2.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache~=1.2->chembl_webresource_client)\n",
            "  Downloading cattrs-24.1.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-cache~=1.2->chembl_webresource_client) (4.3.6)\n",
            "Collecting url-normalize>=1.4 (from requests-cache~=1.2->chembl_webresource_client)\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache~=1.2->chembl_webresource_client) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions!=4.6.3,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache~=1.2->chembl_webresource_client) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url-normalize>=1.4->requests-cache~=1.2->chembl_webresource_client) (1.16.0)\n",
            "Downloading chembl_webresource_client-0.10.9-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-24.1.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Installing collected packages: url-normalize, cattrs, requests-cache, chembl_webresource_client\n",
            "Successfully installed cattrs-24.1.2 chembl_webresource_client-0.10.9 requests-cache-1.2.1 url-normalize-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chembl_webresource_client.new_client import new_client\n",
        "import pandas as pd\n",
        "\n",
        "# Function to query ChEMBL database and retrieve compound data\n",
        "def query_chembl_data():\n",
        "    # Access the molecule endpoint\n",
        "    molecule = new_client.molecule\n",
        "    # Query for compounds with specific properties, limit to 100 samples\n",
        "    results = molecule.filter(molecule_properties__aromatic_rings__gt=0).only(['molecule_chembl_id', 'molecule_structures', 'molecule_properties'])[:100]\n",
        "    return results\n",
        "\n",
        "# Query ChEMBL data\n",
        "chembl_data = query_chembl_data()\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "if chembl_data:\n",
        "    compounds_df = pd.DataFrame(chembl_data)\n",
        "    print(f\"Retrieved {len(compounds_df)} compounds from ChEMBL.\")\n",
        "else:\n",
        "    print(\"No data retrieved from ChEMBL.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRvHwKVWrPAe",
        "outputId": "9f8ba8fb-f652-472d-838f-6aa6599e88d1"
      },
      "id": "vRvHwKVWrPAe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 100 compounds from ChEMBL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLk3X7B2r4Ks",
        "outputId": "7e031c80-73c3-4e2f-b7ee-a01b91fda5fa"
      },
      "id": "jLk3X7B2r4Ks",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compounds_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSlL68vcsHX3",
        "outputId": "4f3ae8ae-4017-493d-df82-6b64917e4d3a"
      },
      "id": "QSlL68vcsHX3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'molecule_properties', 'molecule_structures'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand the molecule_structures column to extract canonical_smiles\n",
        "compounds_df = pd.concat([compounds_df.drop(['molecule_structures'], axis=1),\n",
        "                          compounds_df['molecule_structures'].apply(pd.Series)], axis=1)\n",
        "\n",
        "# Check the updated DataFrame columns\n",
        "print(compounds_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j404S9q_spMC",
        "outputId": "7a9b3893-5428-4c35-a484-2077c6df0616"
      },
      "id": "j404S9q_spMC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'molecule_properties', 'canonical_smiles',\n",
            "       'molfile', 'standard_inchi', 'standard_inchi_key'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx\n",
        "import torch\n",
        "\n",
        "# Function to process SMILES strings into graph data\n",
        "def process_smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add atoms as nodes with features\n",
        "    for atom in mol.GetAtoms():\n",
        "        features = [\n",
        "            atom.GetAtomicNum(),            # Atomic number\n",
        "            atom.GetDegree(),               # Degree\n",
        "            atom.GetImplicitValence(),      # Implicit valence\n",
        "            int(atom.GetIsAromatic())       # Aromaticity (binary)\n",
        "        ]\n",
        "        graph.add_node(atom.GetIdx(), x=torch.tensor(features, dtype=torch.float))\n",
        "\n",
        "    # Add bonds as edges with features\n",
        "    for bond in mol.GetBonds():\n",
        "        bond_type = bond.GetBondTypeAsDouble()\n",
        "        graph.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx(), weight=bond_type)\n",
        "\n",
        "    return from_networkx(graph)\n",
        "\n",
        "# Convert compounds DataFrame to PyG Data objects\n",
        "data_list = []\n",
        "for idx, row in compounds_df.iterrows():\n",
        "    # Access 'canonical_smiles' directly\n",
        "    if 'canonical_smiles' in row and pd.notna(row['canonical_smiles']):\n",
        "        graph_data = process_smiles_to_graph(row['canonical_smiles'])\n",
        "        if graph_data:\n",
        "            # Extract molecular weight (or other target property)\n",
        "            if 'molecule_properties' in row and pd.notna(row['molecule_properties']):\n",
        "                properties = row['molecule_properties']\n",
        "                if isinstance(properties, dict) and 'full_mwt' in properties:\n",
        "                    molecular_weight = float(properties['full_mwt'])\n",
        "                    graph_data.y = torch.tensor([molecular_weight], dtype=torch.float)\n",
        "                    data_list.append(graph_data)\n",
        "\n",
        "# Check how many valid data points we have\n",
        "print(f\"Processed {len(data_list)} valid graph data points for regression.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKzoptcWs1a9",
        "outputId": "62522cd6-af3c-46f4-e030-5e83633d9360"
      },
      "id": "iKzoptcWs1a9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 valid graph data points for regression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader  # Updated import\n",
        "\n",
        "# Define DataLoader with the updated module\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "84TJ7cmGtTcl"
      },
      "id": "84TJ7cmGtTcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "        # Flatten the output to match the target shape\n",
        "        out = out.view(-1)  # Convert from shape [batch_size, 1] to [batch_size]\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(out, batch.y)\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkinip1Htmrm",
        "outputId": "f31a99d4-ec4c-427e-8032-51044299ca6f"
      },
      "id": "Hkinip1Htmrm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 11769.1728515625\n",
            "Epoch 2, Loss: 12865.124267578125\n",
            "Epoch 3, Loss: 11980.004760742188\n",
            "Epoch 4, Loss: 11608.927490234375\n",
            "Epoch 5, Loss: 12467.000732421875\n",
            "Epoch 6, Loss: 11050.271362304688\n",
            "Epoch 7, Loss: 11718.073486328125\n",
            "Epoch 8, Loss: 13056.15478515625\n",
            "Epoch 9, Loss: 12283.845947265625\n",
            "Epoch 10, Loss: 14253.5927734375\n",
            "Epoch 11, Loss: 10716.021118164062\n",
            "Epoch 12, Loss: 13588.406982421875\n",
            "Epoch 13, Loss: 13868.614501953125\n",
            "Epoch 14, Loss: 12201.07861328125\n",
            "Epoch 15, Loss: 14384.63671875\n",
            "Epoch 16, Loss: 11634.850219726562\n",
            "Epoch 17, Loss: 10826.485778808594\n",
            "Epoch 18, Loss: 11645.938720703125\n",
            "Epoch 19, Loss: 11240.302124023438\n",
            "Epoch 20, Loss: 14142.62939453125\n",
            "Epoch 21, Loss: 11381.371826171875\n",
            "Epoch 22, Loss: 15598.714111328125\n",
            "Epoch 23, Loss: 11437.75634765625\n",
            "Epoch 24, Loss: 14799.457275390625\n",
            "Epoch 25, Loss: 11082.479431152344\n",
            "Epoch 26, Loss: 15058.246826171875\n",
            "Epoch 27, Loss: 14031.683349609375\n",
            "Epoch 28, Loss: 14856.8837890625\n",
            "Epoch 29, Loss: 14618.6162109375\n",
            "Epoch 30, Loss: 13146.517333984375\n",
            "Epoch 31, Loss: 14210.66943359375\n",
            "Epoch 32, Loss: 11758.353393554688\n",
            "Epoch 33, Loss: 11452.146728515625\n",
            "Epoch 34, Loss: 11474.541259765625\n",
            "Epoch 35, Loss: 14406.83837890625\n",
            "Epoch 36, Loss: 13493.596923828125\n",
            "Epoch 37, Loss: 12086.88232421875\n",
            "Epoch 38, Loss: 12764.580810546875\n",
            "Epoch 39, Loss: 13954.6474609375\n",
            "Epoch 40, Loss: 16231.176025390625\n",
            "Epoch 41, Loss: 14994.740234375\n",
            "Epoch 42, Loss: 12603.080078125\n",
            "Epoch 43, Loss: 14459.708984375\n",
            "Epoch 44, Loss: 15144.326904296875\n",
            "Epoch 45, Loss: 12040.78759765625\n",
            "Epoch 46, Loss: 11984.082397460938\n",
            "Epoch 47, Loss: 11533.927490234375\n",
            "Epoch 48, Loss: 13731.06201171875\n",
            "Epoch 49, Loss: 14051.481201171875\n",
            "Epoch 50, Loss: 11266.523559570312\n",
            "Epoch 51, Loss: 16002.372314453125\n",
            "Epoch 52, Loss: 17699.3857421875\n",
            "Epoch 53, Loss: 10495.345092773438\n",
            "Epoch 54, Loss: 12230.748901367188\n",
            "Epoch 55, Loss: 16228.56689453125\n",
            "Epoch 56, Loss: 14139.9853515625\n",
            "Epoch 57, Loss: 14485.9638671875\n",
            "Epoch 58, Loss: 9997.125350952148\n",
            "Epoch 59, Loss: 12915.307373046875\n",
            "Epoch 60, Loss: 12163.026611328125\n",
            "Epoch 61, Loss: 11218.499755859375\n",
            "Epoch 62, Loss: 11610.284423828125\n",
            "Epoch 63, Loss: 11862.553955078125\n",
            "Epoch 64, Loss: 10977.665649414062\n",
            "Epoch 65, Loss: 12297.988037109375\n",
            "Epoch 66, Loss: 12479.726318359375\n",
            "Epoch 67, Loss: 13590.821044921875\n",
            "Epoch 68, Loss: 13031.926025390625\n",
            "Epoch 69, Loss: 11417.189208984375\n",
            "Epoch 70, Loss: 14607.531982421875\n",
            "Epoch 71, Loss: 15458.652099609375\n",
            "Epoch 72, Loss: 13854.145263671875\n",
            "Epoch 73, Loss: 14841.318481445312\n",
            "Epoch 74, Loss: 10522.405029296875\n",
            "Epoch 75, Loss: 12245.866943359375\n",
            "Epoch 76, Loss: 11300.279663085938\n",
            "Epoch 77, Loss: 14576.905517578125\n",
            "Epoch 78, Loss: 12278.04345703125\n",
            "Epoch 79, Loss: 14215.872802734375\n",
            "Epoch 80, Loss: 13515.051025390625\n",
            "Epoch 81, Loss: 13300.565185546875\n",
            "Epoch 82, Loss: 13125.846435546875\n",
            "Epoch 83, Loss: 13328.8359375\n",
            "Epoch 84, Loss: 12389.038818359375\n",
            "Epoch 85, Loss: 10954.668029785156\n",
            "Epoch 86, Loss: 12796.939208984375\n",
            "Epoch 87, Loss: 13947.190673828125\n",
            "Epoch 88, Loss: 17152.2978515625\n",
            "Epoch 89, Loss: 14848.138427734375\n",
            "Epoch 90, Loss: 13341.32275390625\n",
            "Epoch 91, Loss: 12763.16748046875\n",
            "Epoch 92, Loss: 11147.254150390625\n",
            "Epoch 93, Loss: 12432.42236328125\n",
            "Epoch 94, Loss: 13955.666870117188\n",
            "Epoch 95, Loss: 11343.337768554688\n",
            "Epoch 96, Loss: 12683.870849609375\n",
            "Epoch 97, Loss: 11907.800537109375\n",
            "Epoch 98, Loss: 11289.968872070312\n",
            "Epoch 99, Loss: 12563.009765625\n",
            "Epoch 100, Loss: 10776.516723632812\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n"
      ],
      "metadata": {
        "id": "E7x6xd_Vt0A6"
      },
      "id": "E7x6xd_Vt0A6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx\n",
        "import torch\n",
        "\n",
        "# Function to process SMILES strings into graph data with more atom features\n",
        "def process_smiles_to_graph(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        # Add more features for each atom, such as atomic number, degree, implicit valence, aromaticity, etc.\n",
        "        features = [\n",
        "            atom.GetAtomicNum(),          # Atomic number\n",
        "            atom.GetDegree(),             # Degree\n",
        "            atom.GetImplicitValence(),    # Implicit valence\n",
        "            int(atom.GetIsAromatic()),    # Aromaticity\n",
        "            atom.GetHybridization(),      # Hybridization state\n",
        "            atom.GetFormalCharge()        # Formal charge\n",
        "        ]\n",
        "        graph.add_node(atom.GetIdx(), x=torch.tensor(features, dtype=torch.float))\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        graph.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
        "\n",
        "    return from_networkx(graph)\n",
        "\n",
        "# Convert compounds DataFrame to PyG Data objects\n",
        "data_list = []\n",
        "for idx, row in compounds_df.iterrows():\n",
        "    graph_data = process_smiles_to_graph(row['canonical_smiles'])\n",
        "    if graph_data:\n",
        "        graph_data.y = torch.tensor([float(row.get('molecule_properties.molecular_weight', 0))], dtype=torch.float)\n",
        "        data_list.append(graph_data)\n",
        "\n",
        "# Check how many valid data points we have\n",
        "print(f\"Processed {len(data_list)} valid graph data points for regression.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFBviqhmuDw5",
        "outputId": "718283d7-122b-4be9-e632-3f85b16e0a48"
      },
      "id": "vFBviqhmuDw5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 valid graph data points for regression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n"
      ],
      "metadata": {
        "id": "LgaPOj9QuMfW"
      },
      "id": "LgaPOj9QuMfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "9Ncg6dcVuOGx"
      },
      "id": "9Ncg6dcVuOGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.loader import DataLoader  # Use the updated import\n",
        "\n",
        "# Define the GCN model with BatchNorm and additional layer for improved learning\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        self.conv1 = GCNConv(6, hidden_channels)  # Input features: 6 (updated features)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.lin = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = global_mean_pool(x, batch)  # Pooling layer\n",
        "        x = self.lin(x)  # Linear layer to predict molecular weight\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = GCN(hidden_channels=64)\n",
        "\n",
        "# Set up the DataLoader\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        out = out.view(-1)  # Flatten output to match target shape\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Optimizer step\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frPmk3MpuVDR",
        "outputId": "36e04d36-950c-4f1a-ea6b-d04f94053586"
      },
      "id": "frPmk3MpuVDR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.006826828874181956\n",
            "Epoch 2, Loss: 0.005030019441619515\n",
            "Epoch 3, Loss: 0.003698592772707343\n",
            "Epoch 4, Loss: 0.002400223311269656\n",
            "Epoch 5, Loss: 0.00429762844578363\n",
            "Epoch 6, Loss: 0.0008872855396475643\n",
            "Epoch 7, Loss: 0.0018391318953945301\n",
            "Epoch 8, Loss: 0.0015068486536620185\n",
            "Epoch 9, Loss: 0.0017886753630591556\n",
            "Epoch 10, Loss: 0.0009955130881280638\n",
            "Epoch 11, Loss: 0.0019504653610056266\n",
            "Epoch 12, Loss: 0.0010817864967975765\n",
            "Epoch 13, Loss: 0.0006536901637446135\n",
            "Epoch 14, Loss: 0.0005684328207280487\n",
            "Epoch 15, Loss: 0.00022098709814599715\n",
            "Epoch 16, Loss: 0.0004612212360370904\n",
            "Epoch 17, Loss: 0.0002746454847510904\n",
            "Epoch 18, Loss: 0.00041016667273652274\n",
            "Epoch 19, Loss: 0.00029942354922241066\n",
            "Epoch 20, Loss: 0.00025804340111790225\n",
            "Epoch 21, Loss: 0.0006145847037259955\n",
            "Epoch 22, Loss: 0.0003269013268436538\n",
            "Epoch 23, Loss: 0.0002599874642328359\n",
            "Epoch 24, Loss: 0.00017390722860000096\n",
            "Epoch 25, Loss: 0.0002486468729330227\n",
            "Epoch 26, Loss: 0.0005501960404217243\n",
            "Epoch 27, Loss: 0.00014444311455008574\n",
            "Epoch 28, Loss: 0.0003444259309617337\n",
            "Epoch 29, Loss: 0.0002693936494324589\n",
            "Epoch 30, Loss: 0.00028715948610624764\n",
            "Epoch 31, Loss: 0.00016180625243578106\n",
            "Epoch 32, Loss: 0.00020233388568158261\n",
            "Epoch 33, Loss: 0.00020254154514987022\n",
            "Epoch 34, Loss: 0.0003124777213088237\n",
            "Epoch 35, Loss: 0.00016142704407684505\n",
            "Epoch 36, Loss: 0.00017913362717081327\n",
            "Epoch 37, Loss: 0.0002243976341560483\n",
            "Epoch 38, Loss: 0.00014134616503724828\n",
            "Epoch 39, Loss: 0.00010396459401817992\n",
            "Epoch 40, Loss: 8.48028066684492e-05\n",
            "Epoch 41, Loss: 0.00017025181477947626\n",
            "Epoch 42, Loss: 7.450173211509536e-05\n",
            "Epoch 43, Loss: 0.00020150392811046913\n",
            "Epoch 44, Loss: 0.0002743964250839781\n",
            "Epoch 45, Loss: 0.00019258125848864438\n",
            "Epoch 46, Loss: 0.00014592817387892865\n",
            "Epoch 47, Loss: 8.777197581366636e-05\n",
            "Epoch 48, Loss: 0.00014803468002355658\n",
            "Epoch 49, Loss: 8.474263267999049e-05\n",
            "Epoch 50, Loss: 0.00018911880761152133\n",
            "Epoch 51, Loss: 0.00010445241969136987\n",
            "Epoch 52, Loss: 0.00018650624861038523\n",
            "Epoch 53, Loss: 0.00010554324580880348\n",
            "Epoch 54, Loss: 0.00017592675067135133\n",
            "Epoch 55, Loss: 0.00011250653915340081\n",
            "Epoch 56, Loss: 0.00013046208914602175\n",
            "Epoch 57, Loss: 9.681271330919117e-05\n",
            "Epoch 58, Loss: 0.0001326709971181117\n",
            "Epoch 59, Loss: 5.975364911137149e-05\n",
            "Epoch 60, Loss: 9.115400462178513e-05\n",
            "Epoch 61, Loss: 7.437726890202612e-05\n",
            "Epoch 62, Loss: 6.470331481978064e-05\n",
            "Epoch 63, Loss: 0.0001532379546915763\n",
            "Epoch 64, Loss: 0.00013543826389650349\n",
            "Epoch 65, Loss: 8.879245706339134e-05\n",
            "Epoch 66, Loss: 0.00013625766041513998\n",
            "Epoch 67, Loss: 0.0001243279348273063\n",
            "Epoch 68, Loss: 4.652712937058823e-05\n",
            "Epoch 69, Loss: 0.00010350339744036319\n",
            "Epoch 70, Loss: 0.00011970297873631353\n",
            "Epoch 71, Loss: 0.00012829507068090606\n",
            "Epoch 72, Loss: 0.00010735473915701732\n",
            "Epoch 73, Loss: 0.00010790243322844617\n",
            "Epoch 74, Loss: 7.051012835290749e-05\n",
            "Epoch 75, Loss: 0.00010547176134423353\n",
            "Epoch 76, Loss: 7.373178277703119e-05\n",
            "Epoch 77, Loss: 0.00010787983774207532\n",
            "Epoch 78, Loss: 0.00011880093552463222\n",
            "Epoch 79, Loss: 7.878444193920586e-05\n",
            "Epoch 80, Loss: 6.78402516314236e-05\n",
            "Epoch 81, Loss: 9.102103922487004e-05\n",
            "Epoch 82, Loss: 0.00030542055537807755\n",
            "Epoch 83, Loss: 0.00015380628065031487\n",
            "Epoch 84, Loss: 0.0001855253958638059\n",
            "Epoch 85, Loss: 0.00031261226831702515\n",
            "Epoch 86, Loss: 9.03107866179198e-05\n",
            "Epoch 87, Loss: 0.00016452263662358746\n",
            "Epoch 88, Loss: 0.00014699298844789155\n",
            "Epoch 89, Loss: 6.880229466332821e-05\n",
            "Epoch 90, Loss: 8.542646719433833e-05\n",
            "Epoch 91, Loss: 5.8189510127704125e-05\n",
            "Epoch 92, Loss: 0.00013157408648112323\n",
            "Epoch 93, Loss: 9.731057980388869e-05\n",
            "Epoch 94, Loss: 0.00014233810634323163\n",
            "Epoch 95, Loss: 0.0001116354260375374\n",
            "Epoch 96, Loss: 9.966889410861768e-05\n",
            "Epoch 97, Loss: 0.00010718861085479148\n",
            "Epoch 98, Loss: 5.138073811394861e-05\n",
            "Epoch 99, Loss: 6.0656977439066395e-05\n",
            "Epoch 100, Loss: 4.461312255443772e-05\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "mdKEKqEHuoxb"
      },
      "id": "mdKEKqEHuoxb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import DataLoader from PyG loader module\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Create DataLoader for train and test sets\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Q1XQiazA1czZ"
      },
      "id": "Q1XQiazA1czZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model again\n",
        "model = GCN(hidden_channels=64)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Training loop on training data\n",
        "model.train()\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        out = out.view(-1)  # Flatten output to match target shape\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Optimizer step\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-cAx3qH1iI4",
        "outputId": "c1b23dbc-df32-4afb-b1cb-bf6e73164271"
      },
      "id": "R-cAx3qH1iI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.008713425835594535\n",
            "Epoch 2, Loss: 0.0035704688634723425\n",
            "Epoch 3, Loss: 0.0025928063162912927\n",
            "Epoch 4, Loss: 0.0017028095511098702\n",
            "Epoch 5, Loss: 0.0012839099120659132\n",
            "Epoch 6, Loss: 0.0006295771260435382\n",
            "Epoch 7, Loss: 0.0009204260034797093\n",
            "Epoch 8, Loss: 0.000829724109886835\n",
            "Epoch 9, Loss: 0.0005018867862721285\n",
            "Epoch 10, Loss: 0.0003666625707410276\n",
            "Epoch 11, Loss: 0.00026083852571900934\n",
            "Epoch 12, Loss: 0.0004560409385400514\n",
            "Epoch 13, Loss: 0.00028770970917927724\n",
            "Epoch 14, Loss: 0.0002172900033959498\n",
            "Epoch 15, Loss: 0.00023465543442095319\n",
            "Epoch 16, Loss: 0.0002986475155921653\n",
            "Epoch 17, Loss: 0.00024022741126827896\n",
            "Epoch 18, Loss: 0.00021041166231346628\n",
            "Epoch 19, Loss: 0.00015329341598165533\n",
            "Epoch 20, Loss: 0.00021642168333831555\n",
            "Epoch 21, Loss: 0.00017349007248412818\n",
            "Epoch 22, Loss: 9.827773950140302e-05\n",
            "Epoch 23, Loss: 0.00014881168317515403\n",
            "Epoch 24, Loss: 0.0001202219112504584\n",
            "Epoch 25, Loss: 0.00024396560426491\n",
            "Epoch 26, Loss: 0.0002510811949226384\n",
            "Epoch 27, Loss: 0.00017556720558786765\n",
            "Epoch 28, Loss: 0.00016044511964234212\n",
            "Epoch 29, Loss: 0.00017910238481514776\n",
            "Epoch 30, Loss: 0.0001869440651110684\n",
            "Epoch 31, Loss: 8.274235369754024e-05\n",
            "Epoch 32, Loss: 9.192327221777911e-05\n",
            "Epoch 33, Loss: 5.798848724225536e-05\n",
            "Epoch 34, Loss: 7.495542619532596e-05\n",
            "Epoch 35, Loss: 6.46547681147543e-05\n",
            "Epoch 36, Loss: 9.006724576465786e-05\n",
            "Epoch 37, Loss: 6.989214186129782e-05\n",
            "Epoch 38, Loss: 0.00014125102461548522\n",
            "Epoch 39, Loss: 0.00010155993125711878\n",
            "Epoch 40, Loss: 8.089832772384398e-05\n",
            "Epoch 41, Loss: 0.00010049665191521247\n",
            "Epoch 42, Loss: 0.00011139788208917405\n",
            "Epoch 43, Loss: 0.00015655701281502843\n",
            "Epoch 44, Loss: 9.313852691169207e-05\n",
            "Epoch 45, Loss: 0.00016845905338414013\n",
            "Epoch 46, Loss: 0.00010074723347012575\n",
            "Epoch 47, Loss: 8.718702520127408e-05\n",
            "Epoch 48, Loss: 9.79597765156844e-05\n",
            "Epoch 49, Loss: 7.349479468151306e-05\n",
            "Epoch 50, Loss: 7.873174035921693e-05\n",
            "Epoch 51, Loss: 4.873918563437959e-05\n",
            "Epoch 52, Loss: 0.00013805432536173612\n",
            "Epoch 53, Loss: 8.629880176158622e-05\n",
            "Epoch 54, Loss: 6.474732193358553e-05\n",
            "Epoch 55, Loss: 3.8746248416525e-05\n",
            "Epoch 56, Loss: 9.295785397019547e-05\n",
            "Epoch 57, Loss: 0.0001217316882199763\n",
            "Epoch 58, Loss: 8.098813971931425e-05\n",
            "Epoch 59, Loss: 0.00010325591332123925\n",
            "Epoch 60, Loss: 0.00014786413460872913\n",
            "Epoch 61, Loss: 7.950036282030244e-05\n",
            "Epoch 62, Loss: 8.736168092582375e-05\n",
            "Epoch 63, Loss: 7.579155741647507e-05\n",
            "Epoch 64, Loss: 9.083836387920503e-05\n",
            "Epoch 65, Loss: 0.0001703753053637532\n",
            "Epoch 66, Loss: 7.318987991311587e-05\n",
            "Epoch 67, Loss: 0.00016333701690503707\n",
            "Epoch 68, Loss: 0.00013093993523701405\n",
            "Epoch 69, Loss: 8.564844514088084e-05\n",
            "Epoch 70, Loss: 0.00010752648813650012\n",
            "Epoch 71, Loss: 4.697606103339543e-05\n",
            "Epoch 72, Loss: 5.701570141051585e-05\n",
            "Epoch 73, Loss: 6.207825451080377e-05\n",
            "Epoch 74, Loss: 4.555610697328424e-05\n",
            "Epoch 75, Loss: 5.767926631961018e-05\n",
            "Epoch 76, Loss: 3.486537116259569e-05\n",
            "Epoch 77, Loss: 6.075744871244145e-05\n",
            "Epoch 78, Loss: 0.0001239547273144126\n",
            "Epoch 79, Loss: 5.8821600759983994e-05\n",
            "Epoch 80, Loss: 5.825278397727137e-05\n",
            "Epoch 81, Loss: 5.9204890324811764e-05\n",
            "Epoch 82, Loss: 6.928846414666623e-05\n",
            "Epoch 83, Loss: 4.207112033327576e-05\n",
            "Epoch 84, Loss: 5.3312634311926864e-05\n",
            "Epoch 85, Loss: 5.955666104758469e-05\n",
            "Epoch 86, Loss: 4.6538102954703696e-05\n",
            "Epoch 87, Loss: 5.9692928819761924e-05\n",
            "Epoch 88, Loss: 5.700767542293761e-05\n",
            "Epoch 89, Loss: 3.709702044337367e-05\n",
            "Epoch 90, Loss: 3.8444442907348275e-05\n",
            "Epoch 91, Loss: 5.118970026766571e-05\n",
            "Epoch 92, Loss: 3.824766705899189e-05\n",
            "Epoch 93, Loss: 6.134405703051016e-05\n",
            "Epoch 94, Loss: 3.7209246026274435e-05\n",
            "Epoch 95, Loss: 4.6861498655440904e-05\n",
            "Epoch 96, Loss: 5.7715054329795144e-05\n",
            "Epoch 97, Loss: 9.643792630716537e-05\n",
            "Epoch 98, Loss: 3.034147933552352e-05\n",
            "Epoch 99, Loss: 0.00010074520287162159\n",
            "Epoch 100, Loss: 6.46637918180204e-05\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize metrics\n",
        "test_loss = 0\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "    for batch in test_loader:\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        out = out.view(-1)\n",
        "        loss = criterion(out, batch.y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        all_targets.extend(batch.y.tolist())\n",
        "        all_predictions.extend(out.tolist())\n",
        "\n",
        "# Calculate average test loss\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "print(f\"Test Loss (MSE): {avg_test_loss}\")\n",
        "\n",
        "# Calculate MAE and R^2 metrics\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "mae = mean_absolute_error(all_targets, all_predictions)\n",
        "r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "print(f\"Test Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"Test R^2 Score: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fsKWj661x2M",
        "outputId": "b9d3b35d-931a-428a-9cf9-648d153076bd"
      },
      "id": "9fsKWj661x2M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (MSE): 6.43791354377754e-05\n",
            "Test Mean Absolute Error (MAE): 0.006852268241345882\n",
            "Test R^2 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compounds_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0kPfbEi2ULK",
        "outputId": "02a30b51-b115-4975-88c1-1045f7a7814d"
      },
      "id": "R0kPfbEi2ULK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'molecule_properties', 'canonical_smiles',\n",
            "       'molfile', 'standard_inchi', 'standard_inchi_key'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "properties_df = pd.json_normalize(compounds_df['molecule_properties'])\n"
      ],
      "metadata": {
        "id": "sVt4ri95228H"
      },
      "id": "sVt4ri95228H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "properties_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "bijs1SVk28--",
        "outputId": "56b27813-9a6b-463b-8d30-5b86df12a5d4"
      },
      "id": "bijs1SVk28--",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  alogp  aromatic_rings cx_logd cx_logp cx_most_apka cx_most_bpka  \\\n",
              "0  2.11               3    2.69    3.63         6.48         None   \n",
              "1  1.33               3    1.82    2.88         6.33         None   \n",
              "2  2.27               3    2.64    3.70         6.33         None   \n",
              "3  1.46               3    1.97    3.02         6.33         None   \n",
              "4  2.11               3    2.57    3.63         6.33         None   \n",
              "\n",
              "  full_molformula full_mwt  hba  hba_lipinski  ...  molecular_species  \\\n",
              "0    C17H12ClN3O3   341.75    5             6  ...               ACID   \n",
              "1      C18H12N4O3   332.32    6             7  ...               ACID   \n",
              "2    C18H16ClN3O3   357.80    5             6  ...               ACID   \n",
              "3      C17H13N3O3   307.31    5             6  ...               ACID   \n",
              "4    C17H12ClN3O3   341.75    5             6  ...               ACID   \n",
              "\n",
              "   mw_freebase  mw_monoisotopic np_likeness_score num_lipinski_ro5_violations  \\\n",
              "0       341.75         341.0567             -1.56                           0   \n",
              "1       332.32         332.0909             -1.59                           0   \n",
              "2       357.80         357.0880             -0.82                           0   \n",
              "3       307.31         307.0957             -1.10                           0   \n",
              "4       341.75         341.0567             -1.49                           0   \n",
              "\n",
              "  num_ro5_violations     psa  qed_weighted  ro3_pass rtb  \n",
              "0                  0   84.82          0.74         N   3  \n",
              "1                  0  108.61          0.73         N   3  \n",
              "2                  0   87.98          0.75         N   3  \n",
              "3                  0   84.82          0.74         N   3  \n",
              "4                  0   84.82          0.74         N   3  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cee6e5d7-db77-451a-9f53-c565a5270963\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alogp</th>\n",
              "      <th>aromatic_rings</th>\n",
              "      <th>cx_logd</th>\n",
              "      <th>cx_logp</th>\n",
              "      <th>cx_most_apka</th>\n",
              "      <th>cx_most_bpka</th>\n",
              "      <th>full_molformula</th>\n",
              "      <th>full_mwt</th>\n",
              "      <th>hba</th>\n",
              "      <th>hba_lipinski</th>\n",
              "      <th>...</th>\n",
              "      <th>molecular_species</th>\n",
              "      <th>mw_freebase</th>\n",
              "      <th>mw_monoisotopic</th>\n",
              "      <th>np_likeness_score</th>\n",
              "      <th>num_lipinski_ro5_violations</th>\n",
              "      <th>num_ro5_violations</th>\n",
              "      <th>psa</th>\n",
              "      <th>qed_weighted</th>\n",
              "      <th>ro3_pass</th>\n",
              "      <th>rtb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.11</td>\n",
              "      <td>3</td>\n",
              "      <td>2.69</td>\n",
              "      <td>3.63</td>\n",
              "      <td>6.48</td>\n",
              "      <td>None</td>\n",
              "      <td>C17H12ClN3O3</td>\n",
              "      <td>341.75</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>ACID</td>\n",
              "      <td>341.75</td>\n",
              "      <td>341.0567</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>84.82</td>\n",
              "      <td>0.74</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.33</td>\n",
              "      <td>3</td>\n",
              "      <td>1.82</td>\n",
              "      <td>2.88</td>\n",
              "      <td>6.33</td>\n",
              "      <td>None</td>\n",
              "      <td>C18H12N4O3</td>\n",
              "      <td>332.32</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>ACID</td>\n",
              "      <td>332.32</td>\n",
              "      <td>332.0909</td>\n",
              "      <td>-1.59</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>108.61</td>\n",
              "      <td>0.73</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.27</td>\n",
              "      <td>3</td>\n",
              "      <td>2.64</td>\n",
              "      <td>3.70</td>\n",
              "      <td>6.33</td>\n",
              "      <td>None</td>\n",
              "      <td>C18H16ClN3O3</td>\n",
              "      <td>357.80</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>ACID</td>\n",
              "      <td>357.80</td>\n",
              "      <td>357.0880</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>87.98</td>\n",
              "      <td>0.75</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.46</td>\n",
              "      <td>3</td>\n",
              "      <td>1.97</td>\n",
              "      <td>3.02</td>\n",
              "      <td>6.33</td>\n",
              "      <td>None</td>\n",
              "      <td>C17H13N3O3</td>\n",
              "      <td>307.31</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>ACID</td>\n",
              "      <td>307.31</td>\n",
              "      <td>307.0957</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>84.82</td>\n",
              "      <td>0.74</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.11</td>\n",
              "      <td>3</td>\n",
              "      <td>2.57</td>\n",
              "      <td>3.63</td>\n",
              "      <td>6.33</td>\n",
              "      <td>None</td>\n",
              "      <td>C17H12ClN3O3</td>\n",
              "      <td>341.75</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>ACID</td>\n",
              "      <td>341.75</td>\n",
              "      <td>341.0567</td>\n",
              "      <td>-1.49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>84.82</td>\n",
              "      <td>0.74</td>\n",
              "      <td>N</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cee6e5d7-db77-451a-9f53-c565a5270963')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cee6e5d7-db77-451a-9f53-c565a5270963 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cee6e5d7-db77-451a-9f53-c565a5270963');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff38ed09-8709-455c-8037-006e1d22042f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff38ed09-8709-455c-8037-006e1d22042f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff38ed09-8709-455c-8037-006e1d22042f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "properties_df"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df = pd.concat([compounds_df.drop(columns=['molecule_properties']), properties_df], axis=1)\n"
      ],
      "metadata": {
        "id": "V4y-nSOd3OZ7"
      },
      "id": "V4y-nSOd3OZ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(expanded_df.columns)\n",
        "print(expanded_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEIN37eX3RQr",
        "outputId": "5924ea29-4e3c-4a29-8fb5-a0883c0e9392"
      },
      "id": "wEIN37eX3RQr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'canonical_smiles', 'molfile', 'standard_inchi',\n",
            "       'standard_inchi_key', 'alogp', 'aromatic_rings', 'cx_logd', 'cx_logp',\n",
            "       'cx_most_apka', 'cx_most_bpka', 'full_molformula', 'full_mwt', 'hba',\n",
            "       'hba_lipinski', 'hbd', 'hbd_lipinski', 'heavy_atoms',\n",
            "       'molecular_species', 'mw_freebase', 'mw_monoisotopic',\n",
            "       'np_likeness_score', 'num_lipinski_ro5_violations',\n",
            "       'num_ro5_violations', 'psa', 'qed_weighted', 'ro3_pass', 'rtb'],\n",
            "      dtype='object')\n",
            "  molecule_chembl_id                                  canonical_smiles  \\\n",
            "0         CHEMBL6329      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "1         CHEMBL6328   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "2       CHEMBL265667  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1   \n",
            "3         CHEMBL6362      Cc1ccc(C(=O)c2ccc(-n3ncc(=O)[nH]c3=O)cc2)cc1   \n",
            "4       CHEMBL267864    Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(Cl)cc1   \n",
            "\n",
            "                                             molfile  \\\n",
            "0  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "1  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "2  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "3  \\n     RDKit          2D\\n\\n 23 25  0  0  0  0...   \n",
            "4  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "\n",
            "                                      standard_inchi  \\\n",
            "0  InChI=1S/C17H12ClN3O3/c1-10-8-11(21-17(24)20-1...   \n",
            "1  InChI=1S/C18H12N4O3/c1-11-8-14(22-18(25)21-16(...   \n",
            "2  InChI=1S/C18H16ClN3O3/c1-10-7-14(22-18(25)21-1...   \n",
            "3  InChI=1S/C17H13N3O3/c1-11-2-4-12(5-3-11)16(22)...   \n",
            "4  InChI=1S/C17H12ClN3O3/c1-10-8-13(21-17(24)20-1...   \n",
            "\n",
            "            standard_inchi_key alogp  aromatic_rings cx_logd cx_logp  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N  2.11               3    2.69    3.63   \n",
            "1  ZJYUMURGSZQFMH-UHFFFAOYSA-N  1.33               3    1.82    2.88   \n",
            "2  YOMWDCALSDWFSV-UHFFFAOYSA-N  2.27               3    2.64    3.70   \n",
            "3  PSOPUAQFGCRDIP-UHFFFAOYSA-N  1.46               3    1.97    3.02   \n",
            "4  KEZNSCMBVRNOHO-UHFFFAOYSA-N  2.11               3    2.57    3.63   \n",
            "\n",
            "  cx_most_apka  ... molecular_species mw_freebase mw_monoisotopic  \\\n",
            "0         6.48  ...              ACID      341.75        341.0567   \n",
            "1         6.33  ...              ACID      332.32        332.0909   \n",
            "2         6.33  ...              ACID      357.80        357.0880   \n",
            "3         6.33  ...              ACID      307.31        307.0957   \n",
            "4         6.33  ...              ACID      341.75        341.0567   \n",
            "\n",
            "   np_likeness_score  num_lipinski_ro5_violations  num_ro5_violations     psa  \\\n",
            "0              -1.56                            0                   0   84.82   \n",
            "1              -1.59                            0                   0  108.61   \n",
            "2              -0.82                            0                   0   87.98   \n",
            "3              -1.10                            0                   0   84.82   \n",
            "4              -1.49                            0                   0   84.82   \n",
            "\n",
            "   qed_weighted ro3_pass rtb  \n",
            "0          0.74        N   3  \n",
            "1          0.73        N   3  \n",
            "2          0.75        N   3  \n",
            "3          0.74        N   3  \n",
            "4          0.74        N   3  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['alogp', 'aromatic_rings', 'hba', 'hbd', 'psa', 'num_lipinski_ro5_violations', 'qed_weighted']\n",
        "feature_df = expanded_df[features].astype(float)  # Convert to float to avoid issues during training\n"
      ],
      "metadata": {
        "id": "BPcWktpd3RNf"
      },
      "id": "BPcWktpd3RNf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the molecular properties as additional node-level features for each graph\n",
        "for idx, graph_data in enumerate(data_list):\n",
        "    row = feature_df.iloc[idx]\n",
        "    additional_features = torch.tensor(row.values, dtype=torch.float).unsqueeze(0)\n",
        "    graph_data.x = torch.cat([graph_data.x, additional_features.repeat(graph_data.num_nodes, 1)], dim=1)\n",
        "\n",
        "# After adding features, check the shape of `graph_data.x` to confirm additional properties are included\n",
        "print(data_list[0].x.shape)  # Should have the new features added to each node\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MR5iTBI3RKG",
        "outputId": "33a08350-b269-47f3-bc37-217098304739"
      },
      "id": "7MR5iTBI3RKG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select additional features for training\n",
        "additional_features = ['alogp', 'aromatic_rings', 'hba', 'hbd', 'psa', 'num_lipinski_ro5_violations',\n",
        "                       'qed_weighted', 'mw_freebase', 'cx_logp', 'heavy_atoms', 'np_likeness_score']\n",
        "\n",
        "# Extract the new feature DataFrame and convert to float\n",
        "expanded_feature_df = expanded_df[additional_features].astype(float)\n",
        "\n",
        "# Update graph data objects with the new features\n",
        "for idx, graph_data in enumerate(data_list):\n",
        "    row = expanded_feature_df.iloc[idx]\n",
        "    add_features = torch.tensor(row.values, dtype=torch.float).unsqueeze(0)\n",
        "    graph_data.x = torch.cat([graph_data.x, add_features.repeat(graph_data.num_nodes, 1)], dim=1)\n",
        "\n",
        "# Verify updated node feature size\n",
        "print(data_list[0].x.shape)  # Should now include the additional features per node\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf-t1l983RG7",
        "outputId": "6f7c60ab-af9b-4f85-b13a-30e0f7846d6c"
      },
      "id": "tf-t1l983RG7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(24, 64)  # Updated to match the new number of features\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "K_dBD1Xy3RDR"
      },
      "id": "K_dBD1Xy3RDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GCNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(24, 64)  # Update input size from 13 to 24 to match the new number of features\n",
        "        self.conv2 = GCNConv(64, 32)\n",
        "        self.fc1 = nn.Linear(32, 16)\n",
        "        self.fc2 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = GCNModel()\n"
      ],
      "metadata": {
        "id": "pF9YX7Wf3Q_e"
      },
      "id": "pF9YX7Wf3Q_e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Extract numerical features to normalize\n",
        "numerical_features = compounds_df[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\n",
        "                                   'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms', 'mw_freebase',\n",
        "                                   'mw_monoisotopic', 'np_likeness_score', 'num_lipinski_ro5_violations',\n",
        "                                   'psa', 'qed_weighted', 'rtb']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "# Add normalized features back to the dataframe\n",
        "for i, column in enumerate(numerical_features.columns):\n",
        "    compounds_df[column] = normalized_features[:, i]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "rU6prRc23Q79",
        "outputId": "ef53a2fa-edbe-425a-c189-2964dd3fabdc"
      },
      "id": "rU6prRc23Q79",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\\n       'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms', 'mw_freebase',\\n       'mw_monoisotopic', 'np_likeness_score', 'num_lipinski_ro5_violations',\\n       'psa', 'qed_weighted', 'rtb'],\\n      dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-62b10e3359b6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract numerical features to normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m numerical_features = compounds_df[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka', \n\u001b[0m\u001b[1;32m      6\u001b[0m                                    \u001b[0;34m'cx_most_bpka'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full_mwt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hbd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'heavy_atoms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mw_freebase'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                    \u001b[0;34m'mw_monoisotopic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'np_likeness_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_lipinski_ro5_violations'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\\n       'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms', 'mw_freebase',\\n       'mw_monoisotopic', 'np_likeness_score', 'num_lipinski_ro5_violations',\\n       'psa', 'qed_weighted', 'rtb'],\\n      dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(compounds_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARQXpIcT3Q4J",
        "outputId": "0cc49849-e5b7-4727-849c-730e4037cd1b"
      },
      "id": "ARQXpIcT3Q4J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'molecule_properties', 'canonical_smiles',\n",
            "       'molfile', 'standard_inchi', 'standard_inchi_key'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the nested properties into separate columns\n",
        "molecule_props_df = compounds_df['molecule_properties'].apply(pd.Series)\n",
        "\n",
        "# Merge these new columns back into the main dataframe\n",
        "compounds_df = pd.concat([compounds_df, molecule_props_df], axis=1)\n",
        "\n",
        "# Drop the original 'molecule_properties' column\n",
        "compounds_df.drop(columns=['molecule_properties'], inplace=True)\n",
        "\n",
        "# Check the new columns available\n",
        "print(compounds_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwwcsVVd3Q0Z",
        "outputId": "9baaa0b1-7082-4f9e-f06c-690fb4d6d9fd"
      },
      "id": "MwwcsVVd3Q0Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['molecule_chembl_id', 'canonical_smiles', 'molfile', 'standard_inchi',\n",
            "       'standard_inchi_key', 'alogp', 'aromatic_rings', 'cx_logd', 'cx_logp',\n",
            "       'cx_most_apka', 'cx_most_bpka', 'full_molformula', 'full_mwt', 'hba',\n",
            "       'hba_lipinski', 'hbd', 'hbd_lipinski', 'heavy_atoms',\n",
            "       'molecular_species', 'mw_freebase', 'mw_monoisotopic',\n",
            "       'np_likeness_score', 'num_lipinski_ro5_violations',\n",
            "       'num_ro5_violations', 'psa', 'qed_weighted', 'ro3_pass', 'rtb'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Extract numerical features to normalize\n",
        "numerical_features = compounds_df[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\n",
        "                                   'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms', 'mw_freebase',\n",
        "                                   'mw_monoisotopic', 'np_likeness_score', 'num_lipinski_ro5_violations',\n",
        "                                   'psa', 'qed_weighted', 'rtb']]\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "# Add normalized features back to the dataframe\n",
        "for i, column in enumerate(numerical_features.columns):\n",
        "    compounds_df[column] = normalized_features[:, i]\n",
        "\n",
        "# Confirm that the DataFrame has been updated with normalized values\n",
        "print(compounds_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dORpqDk23QrJ",
        "outputId": "e29e543d-afd2-4772-e5bf-118206ff9e02"
      },
      "id": "dORpqDk23QrJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  molecule_chembl_id                                  canonical_smiles  \\\n",
            "0         CHEMBL6329      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "1         CHEMBL6328   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "2       CHEMBL265667  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1   \n",
            "3         CHEMBL6362      Cc1ccc(C(=O)c2ccc(-n3ncc(=O)[nH]c3=O)cc2)cc1   \n",
            "4       CHEMBL267864    Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(Cl)cc1   \n",
            "\n",
            "                                             molfile  \\\n",
            "0  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "1  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "2  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "3  \\n     RDKit          2D\\n\\n 23 25  0  0  0  0...   \n",
            "4  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "\n",
            "                                      standard_inchi  \\\n",
            "0  InChI=1S/C17H12ClN3O3/c1-10-8-11(21-17(24)20-1...   \n",
            "1  InChI=1S/C18H12N4O3/c1-11-8-14(22-18(25)21-16(...   \n",
            "2  InChI=1S/C18H16ClN3O3/c1-10-7-14(22-18(25)21-1...   \n",
            "3  InChI=1S/C17H13N3O3/c1-11-2-4-12(5-3-11)16(22)...   \n",
            "4  InChI=1S/C17H12ClN3O3/c1-10-8-13(21-17(24)20-1...   \n",
            "\n",
            "            standard_inchi_key     alogp  aromatic_rings   cx_logd   cx_logp  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N -0.419268       -0.029663  0.474818  0.698606   \n",
            "1  ZJYUMURGSZQFMH-UHFFFAOYSA-N -0.791866       -0.029663  0.014907  0.255290   \n",
            "2  YOMWDCALSDWFSV-UHFFFAOYSA-N -0.342838       -0.029663  0.448387  0.739982   \n",
            "3  PSOPUAQFGCRDIP-UHFFFAOYSA-N -0.729766       -0.029663  0.094202  0.338043   \n",
            "4  KEZNSCMBVRNOHO-UHFFFAOYSA-N -0.419268       -0.029663  0.411382  0.698606   \n",
            "\n",
            "   cx_most_apka  ...  molecular_species mw_freebase  mw_monoisotopic  \\\n",
            "0      0.033934  ...               ACID   -0.448795        -0.451194   \n",
            "1     -0.005025  ...               ACID   -0.530105        -0.528555   \n",
            "2     -0.005025  ...               ACID   -0.310404        -0.312870   \n",
            "3     -0.005025  ...               ACID   -0.745754        -0.744223   \n",
            "4     -0.005025  ...               ACID   -0.448795        -0.451194   \n",
            "\n",
            "   np_likeness_score  num_lipinski_ro5_violations  num_ro5_violations  \\\n",
            "0          -1.242120                    -0.498922                   0   \n",
            "1          -1.287658                    -0.498922                   0   \n",
            "2          -0.118854                    -0.498922                   0   \n",
            "3          -0.543873                    -0.498922                   0   \n",
            "4          -1.135865                    -0.498922                   0   \n",
            "\n",
            "        psa  qed_weighted ro3_pass       rtb  \n",
            "0  0.193694      0.771613        N -0.323383  \n",
            "1  0.929192      0.726410        N -0.323383  \n",
            "2  0.291389      0.816815        N -0.323383  \n",
            "3  0.193694      0.771613        N -0.323383  \n",
            "4  0.193694      0.771613        N -0.323383  \n",
            "\n",
            "[5 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to process SMILES strings into graph data with additional features\n",
        "def process_smiles_to_graph(smiles, row):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_idx = atom.GetIdx()\n",
        "        atom_features = [\n",
        "            atom.GetAtomicNum(),  # Atomic number\n",
        "            atom.GetDegree(),     # Degree\n",
        "            atom.GetImplicitValence(),  # Implicit valence\n",
        "            atom.GetFormalCharge(),     # Formal charge\n",
        "        ]\n",
        "        # Add normalized features from the dataframe\n",
        "        additional_features = row[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\n",
        "                                   'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms',\n",
        "                                   'mw_freebase', 'mw_monoisotopic', 'np_likeness_score',\n",
        "                                   'num_lipinski_ro5_violations', 'psa', 'qed_weighted', 'rtb']].values.tolist()\n",
        "        features = torch.tensor(atom_features + additional_features, dtype=torch.float)\n",
        "        graph.add_node(atom_idx, x=features)\n",
        "\n",
        "    for bond in mol.GetBonds():\n",
        "        graph.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
        "\n",
        "    return from_networkx(graph)\n",
        "\n",
        "# Convert compounds DataFrame to PyG Data objects\n",
        "data_list = []\n",
        "for idx, row in compounds_df.iterrows():\n",
        "    graph_data = process_smiles_to_graph(row['canonical_smiles'], row)\n",
        "    if graph_data:\n",
        "        graph_data.y = torch.tensor([float(row.get('full_mwt', 0))], dtype=torch.float)\n",
        "        data_list.append(graph_data)\n",
        "\n",
        "# Check how many valid data points we have\n",
        "print(f\"Processed {len(data_list)} valid graph data points for regression.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iccWTa_05s3n",
        "outputId": "ed38d462-9e22-426d-a599-bc10012d5f23"
      },
      "id": "iccWTa_05s3n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 valid graph data points for regression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Categorical columns to encode\n",
        "categorical_features = ['molecular_species', 'ro3_pass']\n",
        "\n",
        "# OneHotEncode the categorical variables\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\n",
        "encoded_features = encoder.fit_transform(compounds_df[categorical_features])\n",
        "\n",
        "# Convert encoded features to DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "# Drop original categorical columns and add encoded features\n",
        "compounds_df = compounds_df.drop(columns=categorical_features)\n",
        "compounds_df = pd.concat([compounds_df, encoded_df], axis=1)\n",
        "\n",
        "# Normalize the numerical features again (as we concatenated new columns)\n",
        "numerical_features = compounds_df[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\n",
        "                                   'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms', 'mw_freebase',\n",
        "                                   'mw_monoisotopic', 'np_likeness_score', 'num_lipinski_ro5_violations',\n",
        "                                   'psa', 'qed_weighted', 'rtb']]\n",
        "\n",
        "# Apply StandardScaler again\n",
        "scaler = StandardScaler()\n",
        "normalized_features = scaler.fit_transform(numerical_features)\n",
        "\n",
        "# Update compounds_df with normalized features\n",
        "for i, column in enumerate(numerical_features.columns):\n",
        "    compounds_df[column] = normalized_features[:, i]\n",
        "\n",
        "print(compounds_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNsuyB425swc",
        "outputId": "a5125f6d-f566-4b7a-dc92-56b1c9567418"
      },
      "id": "CNsuyB425swc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  molecule_chembl_id                                  canonical_smiles  \\\n",
            "0         CHEMBL6329      Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccccc1Cl   \n",
            "1         CHEMBL6328   Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(C#N)cc1   \n",
            "2       CHEMBL265667  Cc1cc(-n2ncc(=O)[nH]c2=O)cc(C)c1C(O)c1ccc(Cl)cc1   \n",
            "3         CHEMBL6362      Cc1ccc(C(=O)c2ccc(-n3ncc(=O)[nH]c3=O)cc2)cc1   \n",
            "4       CHEMBL267864    Cc1cc(-n2ncc(=O)[nH]c2=O)ccc1C(=O)c1ccc(Cl)cc1   \n",
            "\n",
            "                                             molfile  \\\n",
            "0  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "1  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "2  \\n     RDKit          2D\\n\\n 25 27  0  0  0  0...   \n",
            "3  \\n     RDKit          2D\\n\\n 23 25  0  0  0  0...   \n",
            "4  \\n     RDKit          2D\\n\\n 24 26  0  0  0  0...   \n",
            "\n",
            "                                      standard_inchi  \\\n",
            "0  InChI=1S/C17H12ClN3O3/c1-10-8-11(21-17(24)20-1...   \n",
            "1  InChI=1S/C18H12N4O3/c1-11-8-14(22-18(25)21-16(...   \n",
            "2  InChI=1S/C18H16ClN3O3/c1-10-7-14(22-18(25)21-1...   \n",
            "3  InChI=1S/C17H13N3O3/c1-11-2-4-12(5-3-11)16(22)...   \n",
            "4  InChI=1S/C17H12ClN3O3/c1-10-8-13(21-17(24)20-1...   \n",
            "\n",
            "            standard_inchi_key     alogp  aromatic_rings   cx_logd   cx_logp  \\\n",
            "0  OWRSAHYFSSNENM-UHFFFAOYSA-N -0.419268       -0.029663  0.474818  0.698606   \n",
            "1  ZJYUMURGSZQFMH-UHFFFAOYSA-N -0.791866       -0.029663  0.014907  0.255290   \n",
            "2  YOMWDCALSDWFSV-UHFFFAOYSA-N -0.342838       -0.029663  0.448387  0.739982   \n",
            "3  PSOPUAQFGCRDIP-UHFFFAOYSA-N -0.729766       -0.029663  0.094202  0.338043   \n",
            "4  KEZNSCMBVRNOHO-UHFFFAOYSA-N -0.419268       -0.029663  0.411382  0.698606   \n",
            "\n",
            "   cx_most_apka  ...       psa qed_weighted       rtb  molecular_species_ACID  \\\n",
            "0      0.033934  ...  0.193694     0.771613 -0.323383                     1.0   \n",
            "1     -0.005025  ...  0.929192     0.726410 -0.323383                     1.0   \n",
            "2     -0.005025  ...  0.291389     0.816815 -0.323383                     1.0   \n",
            "3     -0.005025  ...  0.193694     0.771613 -0.323383                     1.0   \n",
            "4     -0.005025  ...  0.193694     0.771613 -0.323383                     1.0   \n",
            "\n",
            "   molecular_species_BASE  molecular_species_NEUTRAL  \\\n",
            "0                     0.0                        0.0   \n",
            "1                     0.0                        0.0   \n",
            "2                     0.0                        0.0   \n",
            "3                     0.0                        0.0   \n",
            "4                     0.0                        0.0   \n",
            "\n",
            "   molecular_species_ZWITTERION  molecular_species_None  ro3_pass_N  \\\n",
            "0                           0.0                     0.0         1.0   \n",
            "1                           0.0                     0.0         1.0   \n",
            "2                           0.0                     0.0         1.0   \n",
            "3                           0.0                     0.0         1.0   \n",
            "4                           0.0                     0.0         1.0   \n",
            "\n",
            "   ro3_pass_Y  \n",
            "0         0.0  \n",
            "1         0.0  \n",
            "2         0.0  \n",
            "3         0.0  \n",
            "4         0.0  \n",
            "\n",
            "[5 rows x 33 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from torch_geometric.utils import from_networkx\n",
        "import networkx as nx\n",
        "import torch\n",
        "\n",
        "# Function to process SMILES strings into graph data with additional features\n",
        "def process_smiles_to_graph(smiles, features):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    AllChem.Compute2DCoords(mol)\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes (atoms) with atom features and additional molecular features\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_features = [atom.GetAtomicNum(), atom.GetDegree(), atom.GetImplicitValence()]\n",
        "        additional_features = features.tolist()\n",
        "        total_features = atom_features + additional_features\n",
        "\n",
        "        graph.add_node(atom.GetIdx(), x=torch.tensor(total_features, dtype=torch.float))\n",
        "\n",
        "    # Add edges (bonds)\n",
        "    for bond in mol.GetBonds():\n",
        "        graph.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
        "\n",
        "    return from_networkx(graph)\n",
        "\n",
        "# Convert compounds DataFrame to PyG Data objects, including additional features\n",
        "data_list = []\n",
        "for idx, row in compounds_df.iterrows():\n",
        "    features = row[['alogp', 'aromatic_rings', 'cx_logd', 'cx_logp', 'cx_most_apka',\n",
        "                    'cx_most_bpka', 'full_mwt', 'hba', 'hbd', 'heavy_atoms',\n",
        "                    'mw_freebase', 'mw_monoisotopic', 'np_likeness_score',\n",
        "                    'num_lipinski_ro5_violations', 'psa', 'qed_weighted', 'rtb',\n",
        "                    'molecular_species_ACID', 'molecular_species_BASE',\n",
        "                    'molecular_species_NEUTRAL', 'molecular_species_ZWITTERION',\n",
        "                    'molecular_species_None', 'ro3_pass_N', 'ro3_pass_Y']]\n",
        "\n",
        "    graph_data = process_smiles_to_graph(row['canonical_smiles'], features)\n",
        "    if graph_data:\n",
        "        graph_data.y = torch.tensor([float(row.get('full_mwt', 0))], dtype=torch.float)  # Target is molecular weight\n",
        "        data_list.append(graph_data)\n",
        "\n",
        "# Check how many valid data points we have\n",
        "print(f\"Processed {len(data_list)} valid graph data points for regression.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsRUz7NB5ss7",
        "outputId": "805673de-3f96-4967-f060-53668cb436f1"
      },
      "id": "JsRUz7NB5ss7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 valid graph data points for regression.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Define the batch size and create DataLoader for training\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "czLmyT6B5spQ"
      },
      "id": "czLmyT6B5spQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim // 2, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.bn2(x))\n",
        "\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "RN_m_n0X5slv"
      },
      "id": "RN_m_n0X5slv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = len(data_list[0].x[0])  # Number of features per node\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCNModel(input_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = loss_fn(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(data_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJHTouf25shk",
        "outputId": "41e121a5-51b4-4fc3-b38d-2171f0b27d02"
      },
      "id": "rJHTouf25shk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: nan\n",
            "Epoch 2, Loss: nan\n",
            "Epoch 3, Loss: nan\n",
            "Epoch 4, Loss: nan\n",
            "Epoch 5, Loss: nan\n",
            "Epoch 6, Loss: nan\n",
            "Epoch 7, Loss: nan\n",
            "Epoch 8, Loss: nan\n",
            "Epoch 9, Loss: nan\n",
            "Epoch 10, Loss: nan\n",
            "Epoch 11, Loss: nan\n",
            "Epoch 12, Loss: nan\n",
            "Epoch 13, Loss: nan\n",
            "Epoch 14, Loss: nan\n",
            "Epoch 15, Loss: nan\n",
            "Epoch 16, Loss: nan\n",
            "Epoch 17, Loss: nan\n",
            "Epoch 18, Loss: nan\n",
            "Epoch 19, Loss: nan\n",
            "Epoch 20, Loss: nan\n",
            "Epoch 21, Loss: nan\n",
            "Epoch 22, Loss: nan\n",
            "Epoch 23, Loss: nan\n",
            "Epoch 24, Loss: nan\n",
            "Epoch 25, Loss: nan\n",
            "Epoch 26, Loss: nan\n",
            "Epoch 27, Loss: nan\n",
            "Epoch 28, Loss: nan\n",
            "Epoch 29, Loss: nan\n",
            "Epoch 30, Loss: nan\n",
            "Epoch 31, Loss: nan\n",
            "Epoch 32, Loss: nan\n",
            "Epoch 33, Loss: nan\n",
            "Epoch 34, Loss: nan\n",
            "Epoch 35, Loss: nan\n",
            "Epoch 36, Loss: nan\n",
            "Epoch 37, Loss: nan\n",
            "Epoch 38, Loss: nan\n",
            "Epoch 39, Loss: nan\n",
            "Epoch 40, Loss: nan\n",
            "Epoch 41, Loss: nan\n",
            "Epoch 42, Loss: nan\n",
            "Epoch 43, Loss: nan\n",
            "Epoch 44, Loss: nan\n",
            "Epoch 45, Loss: nan\n",
            "Epoch 46, Loss: nan\n",
            "Epoch 47, Loss: nan\n",
            "Epoch 48, Loss: nan\n",
            "Epoch 49, Loss: nan\n",
            "Epoch 50, Loss: nan\n",
            "Epoch 51, Loss: nan\n",
            "Epoch 52, Loss: nan\n",
            "Epoch 53, Loss: nan\n",
            "Epoch 54, Loss: nan\n",
            "Epoch 55, Loss: nan\n",
            "Epoch 56, Loss: nan\n",
            "Epoch 57, Loss: nan\n",
            "Epoch 58, Loss: nan\n",
            "Epoch 59, Loss: nan\n",
            "Epoch 60, Loss: nan\n",
            "Epoch 61, Loss: nan\n",
            "Epoch 62, Loss: nan\n",
            "Epoch 63, Loss: nan\n",
            "Epoch 64, Loss: nan\n",
            "Epoch 65, Loss: nan\n",
            "Epoch 66, Loss: nan\n",
            "Epoch 67, Loss: nan\n",
            "Epoch 68, Loss: nan\n",
            "Epoch 69, Loss: nan\n",
            "Epoch 70, Loss: nan\n",
            "Epoch 71, Loss: nan\n",
            "Epoch 72, Loss: nan\n",
            "Epoch 73, Loss: nan\n",
            "Epoch 74, Loss: nan\n",
            "Epoch 75, Loss: nan\n",
            "Epoch 76, Loss: nan\n",
            "Epoch 77, Loss: nan\n",
            "Epoch 78, Loss: nan\n",
            "Epoch 79, Loss: nan\n",
            "Epoch 80, Loss: nan\n",
            "Epoch 81, Loss: nan\n",
            "Epoch 82, Loss: nan\n",
            "Epoch 83, Loss: nan\n",
            "Epoch 84, Loss: nan\n",
            "Epoch 85, Loss: nan\n",
            "Epoch 86, Loss: nan\n",
            "Epoch 87, Loss: nan\n",
            "Epoch 88, Loss: nan\n",
            "Epoch 89, Loss: nan\n",
            "Epoch 90, Loss: nan\n",
            "Epoch 91, Loss: nan\n",
            "Epoch 92, Loss: nan\n",
            "Epoch 93, Loss: nan\n",
            "Epoch 94, Loss: nan\n",
            "Epoch 95, Loss: nan\n",
            "Epoch 96, Loss: nan\n",
            "Epoch 97, Loss: nan\n",
            "Epoch 98, Loss: nan\n",
            "Epoch 99, Loss: nan\n",
            "Epoch 100, Loss: nan\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data.y = torch.tensor([[float(row.get('full_mwt', 0))]]).float()  # Make y of shape [1]\n"
      ],
      "metadata": {
        "id": "kNqySRLx5sdA"
      },
      "id": "kNqySRLx5sdA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, GCNConv):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n"
      ],
      "metadata": {
        "id": "WYswPrUl6zMA"
      },
      "id": "WYswPrUl6zMA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight is not None:\n",
        "        if isinstance(m, nn.Linear) or isinstance(m, GCNConv):\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "# Apply initialization to the model\n",
        "model.apply(initialize_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmzjiND36zEi",
        "outputId": "d19fa76c-9532-4620-89ff-86df1cca877f"
      },
      "id": "ZmzjiND36zEi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNModel(\n",
              "  (conv1): GCNConv(27, 64)\n",
              "  (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): GCNConv(64, 64)\n",
              "  (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def initialize_weights(m):\n",
        "    # Initialize weights for linear layers\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "    elif isinstance(m, GCNConv):\n",
        "        # Initialize parameters of GCNConv\n",
        "        if hasattr(m, 'lin'):\n",
        "            init.xavier_uniform_(m.lin.weight)\n",
        "            if m.lin.bias is not None:\n",
        "                init.zeros_(m.lin.bias)\n",
        "\n",
        "# Apply initialization to the model\n",
        "model.apply(initialize_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY6HWIeu6y91",
        "outputId": "24d018f7-f8f5-40df-a79b-440ae55c9aa0"
      },
      "id": "ZY6HWIeu6y91",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNModel(\n",
              "  (conv1): GCNConv(27, 64)\n",
              "  (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): GCNConv(64, 64)\n",
              "  (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # For fully connected layers, use Xavier initialization\n",
        "        init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "    elif hasattr(m, 'weight') and hasattr(m, 'bias'):\n",
        "        # Check if the module has 'weight' and 'bias' attributes before initializing them\n",
        "        try:\n",
        "            init.ones_(m.weight)\n",
        "            init.zeros_(m.bias)\n",
        "        except AttributeError:\n",
        "            # Skip layers that don't have weight/bias attributes\n",
        "            pass\n",
        "\n",
        "# Apply initialization to the model\n",
        "model.apply(initialize_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kT0Eyi16y2h",
        "outputId": "fe63772e-1451-42bc-e685-1dc94344e095"
      },
      "id": "_kT0Eyi16y2h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCNModel(\n",
              "  (conv1): GCNConv(27, 64)\n",
              "  (bn1): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): GCNConv(64, 64)\n",
              "  (bn2): BatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Apply first GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Pooling layer to get graph-level representation\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Apply fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Ugrq3UKX6yvc"
      },
      "id": "Ugrq3UKX6yvc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, data in enumerate(data_list):\n",
        "    print(f\"Graph {idx}: x shape: {data.x.shape}, edge_index shape: {data.edge_index.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVO1njpO6yn6",
        "outputId": "a01b77e8-19b4-497f-ad9a-7b39cb4f8db1"
      },
      "id": "mVO1njpO6yn6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph 0: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 1: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 2: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 3: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 4: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 5: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 6: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 7: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 8: x shape: torch.Size([27, 27]), edge_index shape: torch.Size([2, 58])\n",
            "Graph 9: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 104])\n",
            "Graph 10: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 11: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 12: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 13: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 14: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 15: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 16: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 17: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 18: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 19: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 20: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 21: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 22: x shape: torch.Size([29, 27]), edge_index shape: torch.Size([2, 64])\n",
            "Graph 23: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 104])\n",
            "Graph 24: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 82])\n",
            "Graph 25: x shape: torch.Size([38, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 26: x shape: torch.Size([38, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 27: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 102])\n",
            "Graph 28: x shape: torch.Size([48, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 29: x shape: torch.Size([48, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 30: x shape: torch.Size([47, 27]), edge_index shape: torch.Size([2, 106])\n",
            "Graph 31: x shape: torch.Size([37, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 32: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 82])\n",
            "Graph 33: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 34: x shape: torch.Size([47, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 35: x shape: torch.Size([37, 27]), edge_index shape: torch.Size([2, 84])\n",
            "Graph 36: x shape: torch.Size([42, 27]), edge_index shape: torch.Size([2, 90])\n",
            "Graph 37: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 38: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 39: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 40: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 41: x shape: torch.Size([28, 27]), edge_index shape: torch.Size([2, 60])\n",
            "Graph 42: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 43: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 44: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 45: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 46: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 47: x shape: torch.Size([40, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 48: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 49: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 50: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 51: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 52: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 53: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 54: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 55: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 56: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 57: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 58: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 59: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 60: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 61: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 62: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 63: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 64: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 65: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 66: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 67: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 68: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 40])\n",
            "Graph 69: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 36])\n",
            "Graph 70: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 71: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 72: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 80])\n",
            "Graph 73: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 74: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 75: x shape: torch.Size([28, 27]), edge_index shape: torch.Size([2, 62])\n",
            "Graph 76: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 77: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 78: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 79: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 74])\n",
            "Graph 80: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 81: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 82: x shape: torch.Size([34, 27]), edge_index shape: torch.Size([2, 74])\n",
            "Graph 83: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 78])\n",
            "Graph 84: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 40])\n",
            "Graph 85: x shape: torch.Size([15, 27]), edge_index shape: torch.Size([2, 34])\n",
            "Graph 86: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 87: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 88: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 89: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 90: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 91: x shape: torch.Size([12, 27]), edge_index shape: torch.Size([2, 26])\n",
            "Graph 92: x shape: torch.Size([11, 27]), edge_index shape: torch.Size([2, 24])\n",
            "Graph 93: x shape: torch.Size([44, 27]), edge_index shape: torch.Size([2, 92])\n",
            "Graph 94: x shape: torch.Size([45, 27]), edge_index shape: torch.Size([2, 94])\n",
            "Graph 95: x shape: torch.Size([44, 27]), edge_index shape: torch.Size([2, 92])\n",
            "Graph 96: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 97: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 98: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 99: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Graph convolutions\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Global mean pooling to obtain graph-level representation\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers for regression\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ik-iltrw6yj4"
      },
      "id": "ik-iltrw6yj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check input data\n",
        "print(f\"x shape: {data.x.shape}, edge_index shape: {data.edge_index.shape}\")\n",
        "if data.batch is not None:\n",
        "    print(f\"batch shape: {data.batch.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNUsy9km8lyv",
        "outputId": "26449ea2-b2de-4c73-9537-099b59ee9966"
      },
      "id": "GNUsy9km8lyv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.bn1 = BatchNorm(hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.bn2 = BatchNorm(hidden_dim)\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # First GCN layer\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(self.bn1(x))\n",
        "\n",
        "        # Second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.bn2(x))\n",
        "\n",
        "        # Global mean pooling to get graph-level embeddings\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PtrI_XFL8lrL"
      },
      "id": "PtrI_XFL8lrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.fc1 = torch.nn.Linear(hidden_channels, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # GCN Layer 1\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(self.bn1(x))\n",
        "\n",
        "        # GCN Layer 2\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.bn2(x))\n",
        "\n",
        "        # Global pooling if batch is provided (for handling multiple graphs in a batch)\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "AgJJxU5h8lkQ"
      },
      "id": "AgJJxU5h8lkQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.fc1 = torch.nn.Linear(hidden_channels, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # GCN Layer 1\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(self.bn1(x))\n",
        "\n",
        "        # GCN Layer 2\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.bn2(x))\n",
        "\n",
        "        # Global pooling if batch is provided (for handling multiple graphs in a batch)\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lD-R8qAU8ldJ"
      },
      "id": "lD-R8qAU8ldJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(data_list):\n",
        "    print(f\"Graph {i}: x shape: {data.x.shape}, edge_index shape: {data.edge_index.shape}\")\n",
        "    # Optionally add more checks for data.edge_attr or any other attributes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spUQPqs_8lWX",
        "outputId": "8e377fbf-9484-413c-9ea1-e47985f7946a"
      },
      "id": "spUQPqs_8lWX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph 0: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 1: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 2: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 3: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 4: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 5: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 6: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 7: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 8: x shape: torch.Size([27, 27]), edge_index shape: torch.Size([2, 58])\n",
            "Graph 9: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 104])\n",
            "Graph 10: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 11: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 12: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 13: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 14: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 15: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 16: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 17: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 18: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 19: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 20: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 21: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 22: x shape: torch.Size([29, 27]), edge_index shape: torch.Size([2, 64])\n",
            "Graph 23: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 104])\n",
            "Graph 24: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 82])\n",
            "Graph 25: x shape: torch.Size([38, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 26: x shape: torch.Size([38, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 27: x shape: torch.Size([46, 27]), edge_index shape: torch.Size([2, 102])\n",
            "Graph 28: x shape: torch.Size([48, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 29: x shape: torch.Size([48, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 30: x shape: torch.Size([47, 27]), edge_index shape: torch.Size([2, 106])\n",
            "Graph 31: x shape: torch.Size([37, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 32: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 82])\n",
            "Graph 33: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 34: x shape: torch.Size([47, 27]), edge_index shape: torch.Size([2, 108])\n",
            "Graph 35: x shape: torch.Size([37, 27]), edge_index shape: torch.Size([2, 84])\n",
            "Graph 36: x shape: torch.Size([42, 27]), edge_index shape: torch.Size([2, 90])\n",
            "Graph 37: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 38: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 39: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 40: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 41: x shape: torch.Size([28, 27]), edge_index shape: torch.Size([2, 60])\n",
            "Graph 42: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 43: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 44: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 45: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 46: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 47: x shape: torch.Size([40, 27]), edge_index shape: torch.Size([2, 86])\n",
            "Graph 48: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 49: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 50: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 51: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 52: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 53: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 54: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 55: x shape: torch.Size([22, 27]), edge_index shape: torch.Size([2, 48])\n",
            "Graph 56: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 57: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 58: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 59: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 60: x shape: torch.Size([25, 27]), edge_index shape: torch.Size([2, 54])\n",
            "Graph 61: x shape: torch.Size([23, 27]), edge_index shape: torch.Size([2, 50])\n",
            "Graph 62: x shape: torch.Size([24, 27]), edge_index shape: torch.Size([2, 52])\n",
            "Graph 63: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 64: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 65: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 66: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 67: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 70])\n",
            "Graph 68: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 40])\n",
            "Graph 69: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 36])\n",
            "Graph 70: x shape: torch.Size([30, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 71: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 72: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 80])\n",
            "Graph 73: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 74: x shape: torch.Size([20, 27]), edge_index shape: torch.Size([2, 44])\n",
            "Graph 75: x shape: torch.Size([28, 27]), edge_index shape: torch.Size([2, 62])\n",
            "Graph 76: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 66])\n",
            "Graph 77: x shape: torch.Size([32, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 78: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 79: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 74])\n",
            "Graph 80: x shape: torch.Size([33, 27]), edge_index shape: torch.Size([2, 72])\n",
            "Graph 81: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 82: x shape: torch.Size([34, 27]), edge_index shape: torch.Size([2, 74])\n",
            "Graph 83: x shape: torch.Size([36, 27]), edge_index shape: torch.Size([2, 78])\n",
            "Graph 84: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 40])\n",
            "Graph 85: x shape: torch.Size([15, 27]), edge_index shape: torch.Size([2, 34])\n",
            "Graph 86: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 87: x shape: torch.Size([17, 27]), edge_index shape: torch.Size([2, 38])\n",
            "Graph 88: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 89: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 90: x shape: torch.Size([31, 27]), edge_index shape: torch.Size([2, 68])\n",
            "Graph 91: x shape: torch.Size([12, 27]), edge_index shape: torch.Size([2, 26])\n",
            "Graph 92: x shape: torch.Size([11, 27]), edge_index shape: torch.Size([2, 24])\n",
            "Graph 93: x shape: torch.Size([44, 27]), edge_index shape: torch.Size([2, 92])\n",
            "Graph 94: x shape: torch.Size([45, 27]), edge_index shape: torch.Size([2, 94])\n",
            "Graph 95: x shape: torch.Size([44, 27]), edge_index shape: torch.Size([2, 92])\n",
            "Graph 96: x shape: torch.Size([21, 27]), edge_index shape: torch.Size([2, 46])\n",
            "Graph 97: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 98: x shape: torch.Size([19, 27]), edge_index shape: torch.Size([2, 42])\n",
            "Graph 99: x shape: torch.Size([18, 27]), edge_index shape: torch.Size([2, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "def custom_collate(batch):\n",
        "    return Batch.from_data_list([data for data in batch if data.x is not None and len(data.x.shape) == 2])\n",
        "\n",
        "data_loader = DataLoader(data_list, batch_size=32, shuffle=True, collate_fn=custom_collate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwIJBl-P8lSs",
        "outputId": "b9d0302d-2264-48a4-f31a-cf46fc63e48e"
      },
      "id": "HwIJBl-P8lSs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import DataLoader\n"
      ],
      "metadata": {
        "id": "cW_MTkfG-Z0o"
      },
      "id": "cW_MTkfG-Z0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n"
      ],
      "metadata": {
        "id": "z6E6UUYi-Zw9"
      },
      "id": "z6E6UUYi-Zw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "g9Juskfp-ZtD"
      },
      "id": "g9Juskfp-ZtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ELVZIvfn-Zpa"
      },
      "id": "ELVZIvfn-Zpa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # Automatic Mixed Precision for A100 GPU\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # First Graph Convolution Layer + BatchNorm + Activation\n",
        "            x = self.conv1(x, edge_index)\n",
        "            x = self.bn1(x)\n",
        "            x = F.relu(x)\n",
        "\n",
        "            # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "            x = self.conv2(x, edge_index)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "\n",
        "            # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "            if batch is not None:\n",
        "                x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "            # Fully Connected Layers\n",
        "            x = self.fc1(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1hfTIc15-Zl5"
      },
      "id": "1hfTIc15-Zl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "1tr4QdfW-ZiX"
      },
      "id": "1tr4QdfW-ZiX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n"
      ],
      "metadata": {
        "id": "5j86q3fY-ZeU"
      },
      "id": "5j86q3fY-ZeU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(27, 64)  # Input feature size is 27, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "08R0hCWzC5_S",
        "outputId": "78558cd5-20a0-4fb8-8a42-aebdf141ad24"
      },
      "id": "08R0hCWzC5_S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (906x3 and 27x64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, Validation Loss: {val_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, device, lr_scheduler)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Forward pass with mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# First Graph Convolution Layer + BatchNorm + Activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (906x3 and 27x64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Feature shape for sample graph: {dataset[0].x.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "X0ArFlyrDkxv",
        "outputId": "8a2258c2-cd76-438a-a905-879b4c0abbc8"
      },
      "id": "X0ArFlyrDkxv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-caa8ed133edd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Feature shape for sample graph: {dataset[0].x.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    print(f\"Feature shape for sample graph: {dataset[0].x.shape}\")  # Print feature shape\n",
        "\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "r-81Ij96C56o",
        "outputId": "355aa68c-ae4c-484a-ba18-e6dc50de9745"
      },
      "id": "r-81Ij96C56o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape for sample graph: torch.Size([37, 3])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1016x3 and 27x64)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-7ec04bd6b041>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-92-7ec04bd6b041>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, Validation Loss: {val_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, device, lr_scheduler)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Forward pass with mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-8107a4e5c95a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# First Graph Convolution Layer + BatchNorm + Activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/dense/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1016x3 and 27x64)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 64)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "tACq2xMyC5wa",
        "outputId": "de05ae26-d1ee-41d7-c956-10f9a074d0e5"
      },
      "id": "tACq2xMyC5wa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found dtype Long but expected Float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-df4763f67a11>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-94-df4763f67a11>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}, Validation Loss: {val_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-94-df4763f67a11>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, device, lr_scheduler)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Backward pass with scaled gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 64)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y.float())\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1L8H_6xC5tP",
        "outputId": "c7e8e4b9-123c-41d2-b62d-1c6b6fc1bc36"
      },
      "id": "Y1L8H_6xC5tP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 6.9043\n",
            "Epoch 2, Validation Loss: 2.4520\n",
            "Epoch 3, Validation Loss: 0.3367\n",
            "Epoch 4, Validation Loss: 0.4508\n",
            "Epoch 5, Validation Loss: 0.4808\n",
            "Epoch 6, Validation Loss: 0.4223\n",
            "Epoch 7, Validation Loss: 0.4839\n",
            "Epoch 8, Validation Loss: 0.4861\n",
            "Epoch 9, Validation Loss: 0.3778\n",
            "Epoch 10, Validation Loss: 0.4496\n",
            "Epoch 11, Validation Loss: 0.4795\n",
            "Epoch 12, Validation Loss: 0.5788\n",
            "Epoch 13, Validation Loss: 0.5051\n",
            "Epoch 14, Validation Loss: 0.4556\n",
            "Epoch 15, Validation Loss: 0.4308\n",
            "Epoch 16, Validation Loss: 0.4802\n",
            "Epoch 17, Validation Loss: 0.4734\n",
            "Epoch 18, Validation Loss: 0.4504\n",
            "Epoch 19, Validation Loss: 0.5864\n",
            "Epoch 20, Validation Loss: 0.4899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 64)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y.float())\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHEdorTzC5p-",
        "outputId": "d92baaf3-a9b7-4706-8b96-5f99538cfcc4"
      },
      "id": "yHEdorTzC5p-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 6.8818\n",
            "Epoch 2, Validation Loss: 3.5212\n",
            "Epoch 3, Validation Loss: 1.1146\n",
            "Epoch 4, Validation Loss: 0.6033\n",
            "Epoch 5, Validation Loss: 0.5497\n",
            "Epoch 6, Validation Loss: 0.6087\n",
            "Epoch 7, Validation Loss: 0.5123\n",
            "Epoch 8, Validation Loss: 0.4825\n",
            "Epoch 9, Validation Loss: 0.5200\n",
            "Epoch 10, Validation Loss: 0.4847\n",
            "Epoch 11, Validation Loss: 0.5293\n",
            "Epoch 12, Validation Loss: 0.5167\n",
            "Epoch 13, Validation Loss: 0.5187\n",
            "Early stopping at epoch 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 64)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y.float())\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.7 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.7 * len(dataset)):int(0.85 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.85 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 20\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    # Testing the model\n",
        "    test_loss = test_model(model, test_loader, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Yn3msCC5mk",
        "outputId": "a54bccc6-bc77-463c-f5e1-c512f167b582"
      },
      "id": "E9Yn3msCC5mk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 3.7604\n",
            "Epoch 2, Validation Loss: 1.4780\n",
            "Epoch 3, Validation Loss: 0.3093\n",
            "Epoch 4, Validation Loss: 0.1909\n",
            "Epoch 5, Validation Loss: 0.3233\n",
            "Epoch 6, Validation Loss: 0.4569\n",
            "Epoch 7, Validation Loss: 0.3103\n",
            "Epoch 8, Validation Loss: 0.2876\n",
            "Epoch 9, Validation Loss: 0.2472\n",
            "Early stopping at epoch 9\n",
            "Test Loss: 0.6763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 64)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv2 = GCNConv(64, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.fc1 = torch.nn.Linear(64, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y.float())\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.7 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.7 * len(dataset)):int(0.85 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.85 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 50\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    # Testing the model\n",
        "    test_loss = test_model(model, test_loader, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc0gMO_eC5iz",
        "outputId": "d106ff2c-e8f8-4b83-a170-0084f940d7b2"
      },
      "id": "qc0gMO_eC5iz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 3.6981\n",
            "Epoch 2, Validation Loss: 2.2752\n",
            "Epoch 3, Validation Loss: 0.8434\n",
            "Epoch 4, Validation Loss: 0.2182\n",
            "Epoch 5, Validation Loss: 0.2184\n",
            "Epoch 6, Validation Loss: 0.2211\n",
            "Epoch 7, Validation Loss: 0.3181\n",
            "Epoch 8, Validation Loss: 0.3877\n",
            "Epoch 9, Validation Loss: 0.4245\n",
            "Early stopping at epoch 9\n",
            "Test Loss: 0.3887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool, GCNConv\n",
        "from torch_geometric.loader import DataLoader  # Import DataLoader for handling datasets\n",
        "\n",
        "class GCNModel(torch.nn.Module):\n",
        "    def __init__(self, input_channels=3, hidden_channels=64, fc_hidden=32):\n",
        "        super(GCNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(input_channels, hidden_channels)  # Input feature size is 3, output is 64\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.fc1 = torch.nn.Linear(hidden_channels, fc_hidden)\n",
        "        self.fc2 = torch.nn.Linear(fc_hidden, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # First Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Second Graph Convolution Layer + BatchNorm + Activation\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # If batch is provided, perform global pooling to get graph-level embeddings\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)  # Aggregate node features to graph features\n",
        "\n",
        "        # Ensure the tensor is reshaped appropriately for the fully connected layers\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Training loop with AMP integration\n",
        "def train_model(model, data_loader, optimizer, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')  # Initialize GradScaler for mixed precision\n",
        "\n",
        "    for data in data_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y.float())\n",
        "\n",
        "        # Backward pass with scaled gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update learning rate if scheduler is provided\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation loop\n",
        "def validate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Testing loop\n",
        "def test_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            output = model(data.x, data.edge_index, data.batch)\n",
        "            loss = F.mse_loss(output.view(-1), data.y)\n",
        "            total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(data_loader.dataset)\n",
        "\n",
        "# Set up data loaders, model, optimizer, and training parameters\n",
        "def main():\n",
        "    from torch_geometric.datasets import TUDataset  # Example dataset\n",
        "\n",
        "    # Load dataset and create data loaders\n",
        "    dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "    train_dataset = dataset[:int(0.7 * len(dataset))]\n",
        "    val_dataset = dataset[int(0.7 * len(dataset)):int(0.85 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.85 * len(dataset)):]\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Set up model, optimizer, and device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = GCNModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 5\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training and validation\n",
        "    num_epochs = 50\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_model(model, train_loader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f'Epoch {epoch}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    # Testing the model\n",
        "    test_loss = test_model(model, test_loader, device)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGtT7ZcyGj4L",
        "outputId": "4da017de-e7c4-4ce0-9bc1-84ec42bc04d4"
      },
      "id": "sGtT7ZcyGj4L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation Loss: 3.5982\n",
            "Epoch 2, Validation Loss: 0.6470\n",
            "Epoch 3, Validation Loss: 0.2698\n",
            "Epoch 4, Validation Loss: 0.6901\n",
            "Epoch 5, Validation Loss: 0.4808\n",
            "Epoch 6, Validation Loss: 0.3157\n",
            "Epoch 7, Validation Loss: 0.4116\n",
            "Epoch 8, Validation Loss: 0.3816\n",
            "Early stopping at epoch 8\n",
            "Test Loss: 0.4924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kDk-wuwDGjzg"
      },
      "id": "kDk-wuwDGjzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJCWl1UzGjwO"
      },
      "id": "zJCWl1UzGjwO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sankffOPGjr7"
      },
      "id": "sankffOPGjr7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5MVyYYJfGjoh"
      },
      "id": "5MVyYYJfGjoh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB7iZGN0Gjku"
      },
      "id": "OB7iZGN0Gjku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_OazkehGjg1"
      },
      "id": "I_OazkehGjg1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-chVTlXGjdl"
      },
      "id": "Q-chVTlXGjdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AyzRjTYeGjZz"
      },
      "id": "AyzRjTYeGjZz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1OdReGWBGjWI"
      },
      "id": "1OdReGWBGjWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lllaRPpYGjTO"
      },
      "id": "lllaRPpYGjTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aPNnc4CNGjPj"
      },
      "id": "aPNnc4CNGjPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgYI41POGjLv"
      },
      "id": "UgYI41POGjLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIMXYMoqGjIP"
      },
      "id": "NIMXYMoqGjIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacf09dc",
      "metadata": {
        "id": "dacf09dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(1, 64)\n",
        "        self.bn1 = BatchNorm(64)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.bn2 = BatchNorm(128)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(128, 1)  # Outputs a scalar value for each graph\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Apply first GCN layer\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Apply second GCN layer\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Pooling to aggregate node features to graph-level representation\n",
        "        x = global_mean_pool(x, batch=batch)\n",
        "\n",
        "        # Final fully connected layer to produce output\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = GNNModel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc869fd",
      "metadata": {
        "id": "2dc869fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(data_list, batch_size=16, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "true_values = []\n",
        "predicted_values = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Example: 10 epochs\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Verify the batch before proceeding\n",
        "        if batch.x.shape[0] != batch.y.shape[0]:\n",
        "            print(\"Skipping batch due to size mismatch.\")\n",
        "            continue\n",
        "\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect true and predicted values for evaluation\n",
        "        if batch.y.shape[0] == output.shape[0]:\n",
        "            true_values.extend(batch.y.cpu().numpy().flatten())\n",
        "            predicted_values.extend(output.cpu().numpy().flatten())\n",
        "\n",
        "# Calculate metrics if true_values and predicted_values have consistent lengths\n",
        "if len(true_values) > 0 and len(true_values) == len(predicted_values):\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "else:\n",
        "    print(\"Final true_values and predicted_values lengths do not match or are empty.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}